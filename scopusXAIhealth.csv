"Authors","Author full names","Author(s) ID","Title","Year","Source title","Volume","Issue","Art. No.","Page start","Page end","Page count","Cited by","DOI","Link","Abstract","Document Type","Publication Stage","Open Access","Source","EID"
"Sivaprasad A.; Reiter E.; Tintarev N.; Oren N.","Sivaprasad, Adarsa (58638668300); Reiter, Ehud (7101771343); Tintarev, Nava (23092113300); Oren, Nir (12768237300)","58638668300; 7101771343; 23092113300; 12768237300","Evaluation of Human-Understandability of Global Model Explanations Using Decision Tree","2024","Communications in Computer and Information Science","1947","","","43","65","22","0","10.1007/978-3-031-50396-2_3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184082859&doi=10.1007%2f978-3-031-50396-2_3&partnerID=40&md5=99acaf9306a17e0c9250aa489fcacefc","In explainable artificial intelligence (XAI) research, the predominant focus has been on interpreting models for experts and practitioners. Model agnostic and local explanation approaches are deemed interpretable and sufficient in many applications. However, in domains like healthcare, where end users are patients without AI or domain expertise, there is an urgent need for model explanations that are more comprehensible and instil trust in the model’s operations. We hypothesise that generating model explanations that are narrative, patient-specific and global (holistic of the model) would enable better understandability and enable decision-making. We test this using a decision tree model to generate both local and global explanations for patients identified as having a high risk of coronary heart disease. These explanations are presented to non-expert users. We find a strong individual preference for a specific type of explanation. The majority of participants prefer global explanations, while a smaller group prefers local explanations. A task based evaluation of mental models of these participants provide valuable feedback to enhance narrative global explanations. This, in turn, guides the design of health informatics systems that are both trustworthy and actionable. © The Author(s) 2024.","Conference paper","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85184082859"
"Leung C.K.","Leung, Carson K. (7402612526)","7402612526","Biomedical Informatics: State of the Art, Challenges, and Opportunities","2024","BioMedInformatics","4","1","","89","97","8","1","10.3390/biomedinformatics4010006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188796636&doi=10.3390%2fbiomedinformatics4010006&partnerID=40&md5=f2933bc5e9b155132cd469b567e578bb","Biomedical informatics can be considered as a multidisciplinary research and educational field situated at the intersection of computational sciences (including computer science, data science, mathematics, and statistics), biology, and medicine. In recent years, there have been advances in the field of biomedical informatics. The current article highlights some interesting state-of-the-art research outcomes in these fields. These include research outcomes in areas like (i) computational biology and medicine, (ii) explainable artificial intelligence (XAI) in biomedical research and clinical practice, (iii) machine learning (including deep learning) methods and application for bioinformatics and healthcare, (iv) imaging informatics, as well as (v) medical statistics and data science. Moreover, the current article also discusses some existing challenges and potential future directions for these research areas to advance the fields of biomedical informatics. © 2024 by the author.","Editorial","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85188796636"
"Ali M.S.; Hossain M.M.; Kona M.A.; Nowrin K.R.; Islam M.K.","Ali, Md Shahin (57225851312); Hossain, Md Maruf (57224000918); Kona, Moutushi Akter (58633467500); Nowrin, Kazi Rubaya (58970920800); Islam, Md Khairul (57226522198)","57225851312; 57224000918; 58633467500; 58970920800; 57226522198","An ensemble classification approach for cervical cancer prediction using behavioral risk factors","2024","Healthcare Analytics","5","","100324","","","","0","10.1016/j.health.2024.100324","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189455161&doi=10.1016%2fj.health.2024.100324&partnerID=40&md5=b7244509449e43959902e0f0bd423ee5","Cervical cancer is a significant public health concern among females worldwide. Despite being preventable, it remains a leading cause of mortality. Early detection is crucial for successful treatment and improved survival rates. This study proposes an ensemble Machine Learning (ML) classifier for efficient and accurate identification of cervical cancer using medical data. The proposed methodology involves preparing two datasets using effective preprocessing techniques, extracting essential features using the scikit-learn package, and developing an ensemble classifier based on Random Forest, Support Vector Machine, Gaussian Naïve Bayes, and Decision Tree classifier traits. Comparison with other state-of-the-art algorithms using several ML techniques, including support vector machine, decision tree, random forest, Naïve Bayes, logistic regression, CatBoost, and AdaBoost, demonstrates that the proposed ensemble classifier outperforms them significantly, achieving accuracies of 98.06% and 95.45% for Dataset 1 and Dataset 2, respectively. The proposed ensemble classifier outperforms current state-of-the-art algorithms by 1.50% and 6.67% for Dataset 1 and Dataset 2, respectively, highlighting its superior performance compared to existing methods. The study also utilizes a five-fold cross-validation technique to analyze the benefits and drawbacks of the proposed methodology for predicting cervical cancer using medical data. The Receiver Operating Characteristic (ROC) curves with corresponding Area Under the Curve (AUC) values are 0.95 for Dataset 1 and 0.97 for Dataset 2, indicating the overall performance of the classifiers in distinguishing between the classes. Additionally, we employed SHapley Additive exPlanations (SHAP) as an Explainable Artificial Intelligence (XAI) technique to visualize the classifier's performance, providing insights into the important features contributing to cervical cancer identification. The results demonstrate that the proposed ensemble classifier can efficiently and accurately identify cervical cancer and potentially improve cervical cancer diagnosis and treatment. © 2024 The Author(s)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85189455161"
"Prasad A.; Asha V.; Tressa N.; Sharma D.; Deepthi V.; Divyashree S.","Prasad, Arpana (57222391420); Asha, V. (35519321500); Tressa, Neethu (58178664600); Sharma, Divya (58727653200); Deepthi, V. (58982262100); Divyashree, S. (58982262200)","57222391420; 35519321500; 58178664600; 58727653200; 58982262100; 58982262200","NeuroNet: A Deep Learning Framework for Stroke Analysis","2024","2nd International Conference on Intelligent Data Communication Technologies and Internet of Things, IDCIoT 2024","","","","768","772","4","0","10.1109/IDCIoT59759.2024.10467659","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190125986&doi=10.1109%2fIDCIoT59759.2024.10467659&partnerID=40&md5=c45c4769aae044e904a65d86ad79712d","The revolutionary potential of a deep learning framework for stroke analysis and prediction in the healthcare industry is examined in this paper. The suggested framework uses large datasets, such as patient records, medical images, and pertinent health information, to create histicated neural networks in order to address the critical need for early detection and intervention in stroke cases. Important areas like stroke type classification, natural language processing, image analysis, treatment optimization, outcome prediction, telemedicine, and explainable artificial intelligence are highlighted in the thorough review of related work. The steps involved in data collection, preprocessing, model selection, and result interpretation are all carefully laid out in the methodology. A thorough evaluation of machine learning models, such as Random Forest, SVM, XGBoost, and Logistic Regression, is carried out; Random Forest continuously performs well. The results highlight how difficult it is to predict strokes and how crucial it is to choose models carefully. With the ultimate goal of improving stroke diagnosis, treatment, and prevention for improved patient care through ongoing research and technological advancements, ethical considerations and adherence to healthcare regulations in AI applications are emphasized.  © 2024 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85190125986"
"Cervoni L.; Sleiman R.; Jacob D.; Roudesli M.","Cervoni, Laurent (57195673163); Sleiman, Rita (58687668300); Jacob, Damien (57218348566); Roudesli, Mehdi (55326983800)","57195673163; 58687668300; 57218348566; 55326983800","Explainable Artificial Intelligence in Response to the Failures of Musculoskeletal Disorder Rehabilitation","2024","Communications in Computer and Information Science","2020 CCIS","","","14","24","10","0","10.1007/978-3-031-54303-6_2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187652681&doi=10.1007%2f978-3-031-54303-6_2&partnerID=40&md5=a67238ea9b9f5523f002f02903dde422","Osteoarticular pathologies, and particularly low back pain and ankle sprains, due to their number and recurrence, constitute a public health issue. Practitioners do not have enough data describing the impact of treatments on the evolution of pathologies, which is necessary to develop a program that can dynamically adapt to changing patient conditions. We have therefore designed an application based on a series of medical consensus rules capable of generating dynamic exercise sessions adapted to the patient’s pathology and its evolution. In the traditional care pathway, it is difficult in retrospect to understand the failures in management (which amount to 40% in ankle sprains). Our approach, which adapts to the patient’s state of health over time, allows us to better understand how exercises are generated and then to analyze the pathways in order to monitor their effectiveness. The application, resulting from this work, is available as a WebApp. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.","Conference paper","Final","","Scopus","2-s2.0-85187652681"
"Ashraf K.; Nawar S.; Hosen M.H.; Islam M.T.; Uddin M.N.","Ashraf, Kahakashan (59012127200); Nawar, Sadia (59012455100); Hosen, Md. Hamid (59011807100); Islam, Mohammad Tanvirul (58592992300); Uddin, Mohammed Nazim (57208989719)","59012127200; 59012455100; 59011807100; 58592992300; 57208989719","Beyond the Black Box: Employing LIME and SHAP for Transparent Health Predictions with Machine Learning Models","2024","2024 International Conference on Advances in Computing, Communication, Electrical, and Smart Systems: Innovation for Sustainability, iCACCESS 2024","","","","","","","0","10.1109/iCACCESS61735.2024.10499522","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191953565&doi=10.1109%2fiCACCESS61735.2024.10499522&partnerID=40&md5=d631276c6a94163da0e8cf62f8340ee6","In the vast realm of healthcare, healthcare data gathered from patients is bountiful. With the continuous evolution and expansion of artificial intelligence, these healthcare data are a vital asset for us. Under the assistance of artificial intelligence, we can efficiently diagnose and prognose diseases to combat the increase in inaccurate prognosis and delayed diagnosis. In healthcare, diagnosis refers to identifying diseases or conditions in patients, while prognosis predicts the likely course and outcome of the medical conditions. To ease the diagnosis and prognosis, we explore the implementation of Machine Learning (ML) techniques and Simple Feedforward Neural Network. The machine learning models that are evaluated include Decision Tree (DT), Random Forest (RF), Support Vector Classifier (SVC), K-Nearest Neighbors (KNN), and Gaussian Naive Bayes (GNB). After evaluation, the KNN model achieved the highest accuracy of 98.56%, along with F1-Score of 98.53%, Precision of 98.69%, and Recall Score of 98.52%. Later, we interpret the decision-making process of the machine learning algorithms by implementing Explainable Artificial Intelligence (XAI). LIME and SHAP, two types of XAI, are employed to explain and visualize the diagnosis capability and feature impact on the models. © 2024 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85191953565"
"Niu S.; Yin Q.; Ma J.; Song Y.; Xu Y.; Bai L.; Pan W.; Yang X.","Niu, Shuai (57423321100); Yin, Qing (57423341100); Ma, Jing (57142999800); Song, Yunya (55082946100); Xu, Yida (58506109800); Bai, Liang (57198546714); Pan, Wei (55604960500); Yang, Xian (55317736900)","57423321100; 57423341100; 57142999800; 55082946100; 58506109800; 57198546714; 55604960500; 55317736900","Enhancing healthcare decision support through explainable AI models for risk prediction","2024","Decision Support Systems","181","","114228","","","","0","10.1016/j.dss.2024.114228","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191022164&doi=10.1016%2fj.dss.2024.114228&partnerID=40&md5=3f8e4e31b3c74197b6c113019cce8a23","Electronic health records (EHRs) are a valuable source of information that can aid in understanding a patient's health condition and making informed healthcare decisions. However, modelling longitudinal EHRs with heterogeneous information is a challenging task. Although recurrent neural networks (RNNs) are frequently utilized in artificial intelligence (AI) models for capturing longitudinal data, their explanatory capabilities are limited. Predictive clustering stands as the most recent advancement within this domain, offering interpretable indications at the cluster level for predicting disease risk. Nonetheless, the challenge of determining the optimal number of clusters has put a brake on the widespread application of predictive clustering for disease risk prediction. In this paper, we introduce a novel non-parametric predictive clustering-based risk prediction model that integrates the Dirichlet Process Mixture Model (DPMM) with predictive clustering via neural networks. To enhance the model's interpretability, we integrate attention mechanisms that enable the capture of local-level evidence in addition to the cluster-level evidence provided by predictive clustering. The outcome of this research is the development of a multi-level explainable artificial intelligence (AI) model. We evaluated the proposed model on two real-world datasets and demonstrated its effectiveness in capturing longitudinal EHR information for disease risk prediction. Moreover, the model successfully produced interpretable evidence to bolster its predictions. © 2024 The Author(s)","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85191022164"
"Gurmessa D.K.; Jimma W.","Gurmessa, Daraje Kaba (58000827800); Jimma, Worku (57194761825)","58000827800; 57194761825","Explainable machine learning for breast cancer diagnosis from mammography and ultrasound images: a systematic review","2024","BMJ Health and Care Informatics","31","1","e100954","","","","0","10.1136/bmjhci-2023-100954","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183808257&doi=10.1136%2fbmjhci-2023-100954&partnerID=40&md5=4e77fcc6ce3620d2468c546d77e7fe55","Background Breast cancer is the most common disease in women. Recently, explainable artificial intelligence (XAI) approaches have been dedicated to investigate breast cancer. An overwhelming study has been done on XAI for breast cancer. Therefore, this study aims to review an XAI for breast cancer diagnosis from mammography and ultrasound (US) images. We investigated how XAI methods for breast cancer diagnosis have been evaluated, the existing ethical challenges, research gaps, the XAI used and the relation between the accuracy and explainability of algorithms. Methods In this work, Preferred Reporting Items for Systematic Reviews and Meta-Analyses checklist and diagram were used. Peer-reviewed articles and conference proceedings from PubMed, IEEE Explore, ScienceDirect, Scopus and Google Scholar databases were searched. There is no stated date limit to filter the papers. The papers were searched on 19 September 2023, using various combinations of the search terms € breast cancer', € explainable', € interpretable', € machine learning', € artificial intelligence' and € XAI'. Rayyan online platform detected duplicates, inclusion and exclusion of papers. Results This study identified 14 primary studies employing XAI for breast cancer diagnosis from mammography and US images. Out of the selected 14 studies, only 1 research evaluated humans' confidence in using the XAI system - additionally, 92.86% of identified papers identified dataset and dataset-related issues as research gaps and future direction. The result showed that further research and evaluation are needed to determine the most effective XAI method for breast cancer. Conclusion XAI is not conceded to increase users' and doctors' trust in the system. For the real-world application, effective and systematic evaluation of its trustworthiness in this scenario is lacking. PROSPERO registration number CRD42023458665.  © Author(s) (or their employer(s)) 2024. Re-use permitted under CC BY-NC. No commercial re-use. See rights and permissions. Published by BMJ.","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85183808257"
"Abid R.; Rizwan M.; Alabdulatif A.; Alnajim A.; Alamro M.; Azrour M.","Abid, Rabia (57207823851); Rizwan, Muhammad (57225821156); Alabdulatif, Abdulatif (55387649900); Alnajim, Abdullah (26422016400); Alamro, Meznah (57215605596); Azrour, Mourade (57193500437)","57207823851; 57225821156; 55387649900; 26422016400; 57215605596; 57193500437","Adaptation of Federated Explainable Artificial Intelligence for Efficient and Secure E-Healthcare Systems","2024","Computers, Materials and Continua","78","3","","3413","3429","16","0","10.32604/cmc.2024.046880","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189336481&doi=10.32604%2fcmc.2024.046880&partnerID=40&md5=1b82e5c5d69b3c64a76ca17702d4058b","Explainable Artificial Intelligence (XAI) has an advanced feature to enhance the decision-making feature and improve the rule-based technique by using more advanced Machine Learning (ML) and Deep Learning (DL) based algorithms. In this paper, we chose e-healthcare systems for efficient decision-making and data classification, especially in data security, data handling, diagnostics, laboratories, and decision-making. Federated Machine Learning (FML) is a new and advanced technology that helps to maintain privacy for Personal Health Records (PHR) and handle a large amount of medical data effectively. In this context, XAI, along with FML, increases efficiency and improves the security of e-healthcare systems. The experiments show efficient system performance by implementing a federated averaging algorithm on an open-source Federated Learning (FL) platform. The experimental evaluation demonstrates the accuracy rate by taking epochs size 5, batch size 16, and the number of clients 5, which shows a higher accuracy rate (19, 104). We conclude the paper by discussing the existing gaps and future work in an e-healthcare system. © 2024 Tech Science Press. All rights reserved.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85189336481"
"Arslanoglu K.; Karakose M.","Arslanoglu, Kubra (58987632600); Karakose, Mehmet (8881363800)","58987632600; 8881363800","An Overview for Trustworthy and Explainable Artificial Intelligence in Healthcare","2024","2024 28th International Conference on Information Technology, IT 2024","","","","","","","0","10.1109/IT61232.2024.10475733","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190478464&doi=10.1109%2fIT61232.2024.10475733&partnerID=40&md5=48e7d4a523a8fe80cec2eb8931bdc8b5","Recently, the increased use of artificial intelligence in healthcare has significantly changed the developments in the field of medicine. Medical centres have adopted AI applications and used it in many applications to predict disease diagnosis and reduce health risks in a predetermined way. In addition to Artificial Intelligence (AI) techniques for processing data and understanding the results of this data, Explainable Artificial Intelligence (XAI) techniques have also gained an important place in the healthcare sector. In this study, reliable and explainable artificial intelligence studies in the field of healthcare were investigated and the blockchain framework, one of the latest technologies in the field of reliability, was examined. Many researchers have used blockchain technology in the healthcare industry to exchange information between laboratories, hospitals, pharmacies, and doctors and to protect patient data. In our study, firstly, the studies whose keywords were XAI and Trustworthy Artificial Intelligence were examined, and then, among these studies, priority was given to current articles using Blockchain technology. Combining the existing methods and results of previous studies and organizing these studies, our study presented a general framework obtained from the reviewed articles. Obtaining this framework from current studies will be beneficial for future studies of both academics and scientists.  © 2024 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85190478464"
"Liu S.; McCoy A.B.; Peterson J.F.; Lasko T.A.; Sittig D.F.; Nelson S.D.; Andrews J.; Patterson L.; Cobb C.M.; Mulherin D.; Morton C.T.; Wright A.","Liu, Siru (57210804881); McCoy, Allison B (35300118400); Peterson, Josh F (7403425761); Lasko, Thomas A (6507484673); Sittig, Dean F (35254375300); Nelson, Scott D (56902730500); Andrews, Jennifer (55197273400); Patterson, Lorraine (57685615100); Cobb, Cheryl M (57314083600); Mulherin, David (55889110500); Morton, Colleen T (58329016600); Wright, Adam (16644142900)","57210804881; 35300118400; 7403425761; 6507484673; 35254375300; 56902730500; 55197273400; 57685615100; 57314083600; 55889110500; 58329016600; 16644142900","Leveraging explainable artificial intelligence to optimize clinical decision support","2024","Journal of the American Medical Informatics Association","31","4","","968","974","6","1","10.1093/jamia/ocae019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189754761&doi=10.1093%2fjamia%2focae019&partnerID=40&md5=da6a83afec0394146543c37967c4b2ff","Objective: To develop and evaluate a data-driven process to generate suggestions for improving alert criteria using explainable artificial intelligence (XAI) approaches. Methods: We extracted data on alerts generated from January 1, 2019 to December 31, 2020, at Vanderbilt University Medical Center. We developed machine learning models to predict user responses to alerts. We applied XAI techniques to generate global explanations and local explanations. We evaluated the generated suggestions by comparing with alert's historical change logs and stakeholder interviews. Suggestions that either matched (or partially matched) changes already made to the alert or were considered clinically correct were classified as helpful. Results: The final dataset included 2 991 823 firings with 2689 features. Among the 5 machine learning models, the LightGBM model achieved the highest Area under the ROC Curve: 0.919 [0.918, 0.920]. We identified 96 helpful suggestions. A total of 278 807 firings (9.3%) could have been eliminated. Some of the suggestions also revealed workflow and education issues. Conclusion: We developed a data-driven process to generate suggestions for improving alert criteria using XAI techniques. Our approach could identify improvements regarding clinical decision support (CDS) that might be overlooked or delayed in manual reviews. It also unveils a secondary purpose for the XAI: to improve quality by discovering scenarios where CDS alerts are not accepted due to workflow, education, or staffing issues. © 2024 The Author(s). Published by Oxford University Press on behalf of the American Medical Informatics Association.","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85189754761"
"Woodbright M.D.; Morshed A.; Browne M.; Ray B.; Moore S.","Woodbright, Mitchell D. (57219687051); Morshed, Ahsan (53363838700); Browne, Matthew (14826358800); Ray, Biplob (37051483800); Moore, Steven (7403537329)","57219687051; 53363838700; 14826358800; 37051483800; 7403537329","Toward Transparent AI for Neurological Disorders: A Feature Extraction and Relevance Analysis Framework","2024","IEEE Access","12","","","37731","37743","12","0","10.1109/ACCESS.2024.3375877","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188004762&doi=10.1109%2fACCESS.2024.3375877&partnerID=40&md5=79ea04c493cca527d72e8ee049437b44","The lack of interpretability and transparency in deep learning architectures has raised concerns among professionals in various industries and academia. One of the main concerns is the ability to trust these architectures' without being provided any insight into the decision-making process. Despite these concerns, researchers continue to explore new models and architectures that do not incorporate explainability into their main construct. In the medical industry, it is crucial to provide explanations of any decision, as patient health outcomes can vary according to decisions made. Furthermore, in medical research, incorrectly diagnosed neurological conditions are a high-cost error that contributes significantly to morbidity and mortality. Therefore, the development of new transparent techniques for neurological conditions is critical. This paper presents a novel Autonomous Relevance Technique for an Explainable neurological disease prediction framework called ART-Explain. The proposed technique autonomously extracts features from within the deep learning architecture to create novel visual explanations of the resulting prediction. ART-Explain is an end-to-end autonomous explainable technique designed to present an intuitive and holistic overview of a prediction made by a deep learning classifier. To evaluate the effectiveness of our approach, we benchmark it with other state-of-the-art techniques using three data sets of neurological disorders. The results demonstrate the generalisation capabilities of our technique and its suitability for real-world applications. By providing transparent insights into the decision-making process, ART-Explain can improve end-user trust and enable a better understanding of classification outcomes in the detection of neurological diseases. © 2013 IEEE.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85188004762"
"Gaspar D.; Silva P.; Silva C.","Gaspar, Diogo (58908331400); Silva, Paulo (57203164598); Silva, Catarina (8770080300)","58908331400; 57203164598; 8770080300","Explainable AI for Intrusion Detection Systems: LIME and SHAP Applicability on Multi-Layer Perceptron","2024","IEEE Access","12","","","30164","30175","11","0","10.1109/ACCESS.2024.3368377","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186107633&doi=10.1109%2fACCESS.2024.3368377&partnerID=40&md5=54c035af115ca42fcba971d6ac5a9ba0","Machine learning-based systems have presented increasing learning performance, in a wide variety of tasks. However, the problem with some state-of-the-art models is their lack of transparency, trustworthiness, and explainability. To address this problem, eXplainable Artificial Intelligence (XAI) appeared. It is a research field that aims to make black-box models more understandable to humans. The research on this topic has increased in recent years, and many methods, such as LIME (Local Interpretable Model-Agnostic Explanations) and SHAP (SHapley Additive exPlanations) have been proposed. Machine learning-based Intrusion Detection Systems (IDS) are one of the many application domains of XAI. However, most of the works about model interpretation focus on other fields, like computer vision, natural language processing, biology, healthcare, etc. This poses a challenge for cybersecurity professionals tasked with analyzing IDS results, thereby impeding their capacity to make informed decisions. In an attempt to address this problem, we have selected two XAI methods, LIME, and SHAP. Using the methods, we have retrieved explanations for the results of a black-box model, part of an IDS solution that performs intrusion detection on IoT devices, increasing its interpretability. In order to validate the explanations, we carried out a perturbation analysis where we tried to obtain a different classification based on the features present in the explanations. With the explanations and the perturbation analysis we were able to draw conclusions about the negative impact of particular features on the model results when present in the input data, making it easier for cybersecurity experts when analyzing the model results and it serves as an aid to the continuous improvement the model. The perturbations also serve as a comparison of performance between LIME and SHAP. To evaluate the degree of interpretability increase, and the explanations provided by each XAI method of the model and directly compare the XAI methods, we have performed a survey analysis.  © 2013 IEEE.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85186107633"
"de Sousa Ribeiro Filho J.; Cardoso L.F.F.; da Silva R.L.S.; Carneiro N.J.S.; Santos V.C.A.; de Oliveira Alves R.C.","de Sousa Ribeiro Filho, José (57961545900); Cardoso, Lucas Felipe Ferraro (57219592159); da Silva, Raíssa Lorena Silva (58728841500); Carneiro, Nikolas Jorge Santiago (36714978800); Santos, Vitor Cirilo Araujo (57204640524); de Oliveira Alves, Ronnie Cley (57201477238)","57961545900; 57219592159; 58728841500; 36714978800; 57204640524; 57201477238","Explanations based on Item Response Theory (eXirt): A model-specific method to explain tree-ensemble model in trust perspective","2024","Expert Systems with Applications","244","","122986","","","","0","10.1016/j.eswa.2023.122986","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180537747&doi=10.1016%2fj.eswa.2023.122986&partnerID=40&md5=c2fd20e757e262ce10964f04bef557f2","Solutions based on tree-ensemble models represent a considerable alternative to real-world prediction problems, but these models are considered black box, thus hindering their applicability in problems of sensitive contexts (such as: health and safety). Explainable Artificial Intelligence (XAI) aims to develop techniques that generate explanations of black box models, since these models are normally not self-explanatory. Methods such as Ciu, Dalex, Eli5, Lofo, Shap and Skater emerged with the proposal to explain black box models through global rankings of feature relevance, which based on different methodologies, generate global explanations that indicate how the model's inputs explain its predictions. This research aims to present an innovative XAI method, called eXirt, capable of carrying out the process of explaining tree-ensemble models, based on Item Response Theory (IRT). In this context, 41 datasets, 4 tree-ensemble algorithms (Light Gradient Boosting, CatBoost, Random Forest, and Gradient Boosting), and 7 XAI methods (including eXirt) were used to generate explanations. In the first set of analyses, the 164 ranks of global feature relevance generated by eXirt were compared with 984 ranks of the other XAI methods present in the literature, being verified that the new method generated different explanations from other existing methods. In a second analysis, exclusive local and global explanations generated by eXirt were presented that help in understanding the model trust, since in this explanation it is possible to observe particularities of the model regarding difficulty (if the model had difficulty predicting the test dataset), discrimination (if the model understands the test dataset as discriminative) and guesswork (if the model got the test dataset right by chance). Thus, it was verified that eXirt is able to generate global explanations of tree-ensemble models and also local and global explanations of models through IRT, showing how this consolidated theory can be used in machine learning in order to obtain explainable and reliable models. © 2023 Elsevier Ltd","Article","Final","","Scopus","2-s2.0-85180537747"
"El-Magd L.M.A.; Dahy G.; Farrag T.A.; Darwish A.; Hassnien A.E.","El-Magd, Lobna M. Abou (58942191600); Dahy, Ghada (57911536300); Farrag, Tamer Ahmed (16030245000); Darwish, Ashraf (56236505100); Hassnien, Aboul Ella (57222373466)","58942191600; 57911536300; 16030245000; 56236505100; 57222373466","An interpretable deep learning based approach for chronic obstructive pulmonary disease using explainable artificial intelligence","2024","International Journal of Information Technology (Singapore)","","","","","","","0","10.1007/s41870-023-01713-w","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187918812&doi=10.1007%2fs41870-023-01713-w&partnerID=40&md5=61e9e30820c8e01da7bd6fd7a3e4ea17","Artificial intelligence has become like-humans in thinking and interpretations. But its uses are still limited and are viewed as black boxes, and this is the most important factor underlying the limited applications, especially in the field of health care. Therefore, there is a need for interpretable prediction that provide better predicts and also explain their prediction. This paper proposed an approach for identification and interpretation of chronic obstructive pulmonary disease (COPD) using exhaled breath data and providing explanations of the predictions results based on explainable artificial intelligence. This paper examined a total of 78 patients, using data collected from 8 sensors that analyzed exhaled air. Diagnostic reasons often involve the utilization of transfer learning techniques based on Deep Neural Networks (DNNs). Five pre-train CNNs were tested for recognizing COPD with global average pooling and flattening layers. For interpreting the results, the given results of the DNN are explained by a twin system that uses Case Based Reasoning. The mission of the twinning system is to convert a black box into a white box to be easier to explain. Compared to the other pre-train CNNs models, GoogleNet deep learning model produces promising results, with up to 100% test accuracy with small number of parameters. © The Author(s), under exclusive licence to Bharati Vidyapeeth's Institute of Computer Applications and Management 2024.","Article","Article in press","","Scopus","2-s2.0-85187918812"
"","","","6th International Conference on Information and Knowledge Systems, ICIKS 2023","2024","Lecture Notes in Business Information Processing","486","","","","","344","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184329896&partnerID=40&md5=ddba6ef65c7dff21760e0397c0a4fbee","The proceedings contain 24 papers. The special focus in this conference is on Information and Knowledge Systems. The topics include: Exploiting Machine Learning Technique for Attack Detection in Intrusion Detection System (IDS) Based on Protocol; robust Aggregation Function in Federated Learning; should I Share or Should I Go?: A Study of Tacit Knowledge Sharing Behaviors in Extended Enterprises; designing a User Contextual Profile Ontology: A Focus on the Vehicle Sales Domain; Management of Implicit Ontology Changes Generated by Non-conservative JSON Instance Updates in the τJOWL Environment; a Model Driven Architecture Approach for Implementing Sensitive Business Processes; Extension of the Functional Dimension of BPMN Based on MDA Approach for Sensitive Business Processes Execution; inclusive Mobile Health System for Yoruba Race in Nigeria; epistemology for Cyber Security: A Controlled Natural Language Approach; decision and Information Support System for a Framework to Building Multicriteria Decision Models; Application of Fuzzy Decision Support Systems in IT Industry Functioning; moving Towards Explainable Artificial Intelligence Using Fuzzy Rule-Based Networks in Decision-Making Process; Sentiment Analysis: Effect of Combining BERT as an Embedding Technique with CNN Model for Tunisian Dialect; an Enhanced Machine Learning-Based Analysis of Teaching and Learning Process for Higher Education System; topic Modelling of Legal Texts Using Bidirectional Encoder Representations from Sentence Transformers; graph Representation Learning for Recommendation Systems: A Short Review; FITradeoff Decision Support System Applied to Solve a Supplier Selection Problem; a Step-By-Step Decision Process for Application Migration to Cloud-Native Architecture; ACTIVE SMOTE for Imbalanced Medical Data Classification; Evolutionary Graph-Clustering vs Evolutionary Cluster-Detection Approaches for Community Identification in PPI Networks; predictive Monitoring of Business Process Execution Delays.","Conference review","Final","","Scopus","2-s2.0-85184329896"
"Kharrat F.G.Z.; Gagne C.; Lesage A.; Gariépy G.; Pelletier J.-F.; Brousseau-Paradis C.; Rochette L.; Pelletier E.; Lévesque P.; Mohammed M.; Wang J.","Kharrat, Fatemeh Gholi Zadeh (57205702323); Gagne, Christian (12140356400); Lesage, Alain (26643549100); Gariépy, Geneviève (26658974500); Pelletier, Jean-François (56386738300); Brousseau-Paradis, Camille (57219980854); Rochette, Louis (7005691387); Pelletier, Eric (56998370600); Lévesque, Pascale (29767688000); Mohammed, Mada (58119192500); Wang, JianLi (9250575900)","57205702323; 12140356400; 26643549100; 26658974500; 56386738300; 57219980854; 7005691387; 56998370600; 29767688000; 58119192500; 9250575900","Explainable artificial intelligence models for predicting risk of suicide using health administrative data in Quebec","2024","PLoS ONE","19","4 April","e0301117","","","","0","10.1371/journal.pone.0301117","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189359885&doi=10.1371%2fjournal.pone.0301117&partnerID=40&md5=f5be0eee8ba80d3103395fab8d281a53","Suicide is a complex, multidimensional event, and a significant challenge for prevention globally. Artificial intelligence (AI) and machine learning (ML) have emerged to harness large-scale datasets to enhance risk detection. In order to trust and act upon the predictions made with ML, more intuitive user interfaces must be validated. Thus, Interpretable AI is one of the crucial directions which could allow policy and decision makers to make reasonable and data-driven decisions that can ultimately lead to better mental health services planning and suicide prevention. This research aimed to develop sex-specific ML models for predicting the population risk of suicide and to interpret the models. Data were from the Quebec Integrated Chronic Disease Surveillance System (QICDSS), covering up to 98% of the population in the province of Quebec and containing data for over 20,000 suicides between 2002 and 2019. We employed a case-control study design. Individuals were considered cases if they were aged 15+ and had died from suicide between January 1st, 2002, and December 31st, 2019 (n = 18339). Controls were a random sample of 1% of the Quebec population aged 15+ of each year, who were alive on December 31st of each year, from 2002 to 2019 (n = 1,307,370). We included 103 features, including individual, programmatic, systemic, and community factors, measured up to five years prior to the suicide events. We trained and then validated the sex-specific predictive risk model using supervised ML algorithms, including Logistic Regression (LR), Random Forest (RF), Extreme Gradient Boosting (XGBoost) and Multilayer perceptron (MLP). We computed operating characteristics, including sensitivity, specificity, and Positive Predictive Value (PPV). We then generated receiver operating characteristic (ROC) curves to predict suicides and calibration measures. For interpretability, Shapley Additive Explanations (SHAP) was used with the global explanation to determine how much the input features contribute to the models' output and the largest absolute coefficients. The best sensitivity was 0.38 with logistic regression for males and 0.47 with MLP for females; the XGBoost Classifier with 0.25 for males and 0.19 for females had the best precision (PPV). This study demonstrated the useful potential of explainable AI models as tools for decision-making and population-level suicide prevention actions. The ML models included individual, programmatic, systemic, and community levels variables available routinely to decision makers and planners in a public managed care system. Caution shall be exercised in the interpretation of variables associated in a predictive model since they are not causal, and other designs are required to establish the value of individual treatments. The next steps are to produce an intuitive user interface for decision makers, planners and other stakeholders like clinicians or representatives of families and people with live experience of suicidal behaviors or death by suicide. For example, how variations in the quality of local area primary care programs for depression or substance use disorders or increased in regional mental health and addiction budgets would lower suicide rates.  © 2024 Gholi Zadeh Kharrat et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85189359885"
"Uma K.; Francis S.; Sun W.; Moens M.-F.","Uma, Kanimozhi (58156005500); Francis, Sumam (57205441995); Sun, Wei (57224474157); Moens, Marie-Francine (7005471657)","58156005500; 57205441995; 57224474157; 7005471657","Towards Explainability in Automated Medical Code Prediction from Clinical Records","2024","Lecture Notes in Networks and Systems","825","","","593","637","44","0","10.1007/978-3-031-47718-8_40","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186671434&doi=10.1007%2f978-3-031-47718-8_40&partnerID=40&md5=2a0eeb1e08329434b33a0b805d01946a","The International Statistical Classification of Diseases and Related Health Problems (ICD) is a global standard, a diagnostic tool that is frequently used for endemic research, health management, and clinical diagnosis, and it plays a crucial role in providing shrewd medical treatment. Comparable statistics on the causes of mortality and morbidity across locations and throughout time have been based on the ICD. The traditional procedure of assigning codes is expensive, error-prone and time-consuming, and automated mapping of ICD codes is now a significant area of scholarly research. With the help of statistical modeling, rule-engines, conventional machine learning, and deep learning techniques like graph embedding, attention mechanisms, adversarial learning, and pre-trained language models (PLMs), this paper aims to analyze and document inferences on the evolution of clinical coding automation. We try to summarize with comparative performance analysis various approaches addressed towards codification of free-text clinical narratives on the publicly available Medical Information Mart. This study investigates whether clinicians and researchers could benefit from an adequate interpretation of model predictions from an Explainable Artificial Intelligence (XAI) perspective. Finally, the survey illustrates ICD coding and disease classification applications and its challenges, evaluation metrics, datasets, and directions towards automating explanatory medical code predictions. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.","Conference paper","Final","","Scopus","2-s2.0-85186671434"
"","","","13th International Conference on Pattern Recognition Applications and Methods, ICPRAM 2024","2024","International Conference on Pattern Recognition Applications and Methods","1","","","","","1000","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190683034&partnerID=40&md5=1fc93ef42627a00fe49a3ace9d578f39","The proceedings contain 113 papers. The special focus in this conference is on Pattern Recognition Applications and Methods. The topics include: TenebrioVision: A Fully Annotated Dataset of Tenebrio Molitor Larvae Worms in a Controlled Environment for Accurate Small Object Detection and Segmentation; offline Text-Independent Arabic and Chinese Writer Identification Using a Multi-Segmentation Codebook-Based Strategy; Tab-VAE: A Novel VAE for Generating Synthetic Tabular Data; Group Importance Estimation Method Based on Group LASSO Regression; Impact of Using GAN Generated Synthetic Data for the Classification of Chemical Foam in Low Data Availability Environments; FaceVision-GAN: A 3D Model Face Reconstruction Method from a Single Image Using GANs; anomaly Detection Methods for Finding Technosignatures; a Branch-and-Bound Approach to Efficient Classification and Retrieval of Documents; benchmarking a Wide Range of Unsupervised Learning Methods for Detecting Anomaly in Blast Furnace; learned Fusion: 3D Object Detection Using Calibration-Free Transformer Feature Fusion; content Rating Classification in Fan Fiction Using Active Learning and Explainable Artificial Intelligence; improvement of Tensor Representation Label in Image Recognition: Evaluation on Selection, Complexity and Size; Leveraging VR and Force-Haptic Feedback for an Effective Training with Robots; GENUINE: Genomic and Nucleus Information Embedding for Single Cell Genetic Alteration Classification in Microscopic Images; practical Deep Feature-Based Visual-Inertial Odometry; detection of Energy Drifts in Waste Water Treatment Plants Using Dynamic Clustering; fetal Health Classification Using One-Dimensional Convolutional Neural Network; fast Filtering for Similarity Search Using Conjunctive Enumeration of Sketches in Order of Hamming Distance; deep Learning, Feature Selection and Model Bias with Home Mortgage Loan Classification.","Conference review","Final","","Scopus","2-s2.0-85190683034"
"Malhotra A.; Jindal R.","Malhotra, Anshu (57216819512); Jindal, Rajni (8698929800)","57216819512; 8698929800","XAI Transformer based Approach for Interpreting Depressed and Suicidal User Behavior on Online Social Networks","2024","Cognitive Systems Research","84","","101186","","","","5","10.1016/j.cogsys.2023.101186","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179619406&doi=10.1016%2fj.cogsys.2023.101186&partnerID=40&md5=4428caf7e4eac554a4e44722ee0bb26f","Online social networks can be used for mental healthcare monitoring using Artificial Intelligence and Machine Learning techniques for detecting various mental health disorders and corresponding risk assessment. Recent research in this domain has primarily been focused on leveraging deep neural networks and various Transformer based Large Language Models, which have now become state-of-the-art for most natural language processing and computational linguistic tasks due to their unmatched prediction accuracy. Unlike conventional machine learning algorithms, these deep neural networks are black box architectures, where it is difficult to interpret and explain their predicted outcome. However, a black box classification outcome is insufficient for healthcare applications. Such systems will not be widely adopted and trusted by healthcare practitioners if they are not able to understand and explain the reasoning behind the predicted decisions made by an AI and ML based healthcare diagnostic system. The key objective of our research is to demonstrate the applications of model agnostic, post hoc surrogate XAI techniques for providing explainability to classification decisions of pretrained LLMs (Transformers) based mental healthcare diagnostic systems fine-tuned (or trained) to detect depressive and suicidal behavior using UGC from online social networks. For this, we have used the two most recent and popular techniques, SHAP and LIME. We have conducted extensive and in-depth experiments with four datasets and six pretrained LLMs, three of which have already been domain-adapted using mental health related datasets. We have also performed Few Shot Learning experiments with these three pretrained mental health domain-adapted LLMs. The results of qualitative and descriptive data analysis in this paper demonstrate that in order to build a comprehensive understanding of a person's psychological state, emotion, and behavior and to discover the causes, symptoms, and triggers of mental health issues, it is essential to utilize eXplAInable (XAI) techniques with Transformer based LLMs (supervised). Alternatively, Transformer based unsupervised topic modeling technique BERTopic may be used for mental health risk monitoring and cause or symptom extraction when supervised training of LLMs is not feasible due to dataset annotation or availability challenges. © 2023 Elsevier B.V.","Article","Final","","Scopus","2-s2.0-85179619406"
"Nasarian E.; Alizadehsani R.; Acharya U.R.; Tsui K.-L.","Nasarian, Elham (57211201933); Alizadehsani, Roohallah (55328861400); Acharya, U.Rajendra (58896843200); Tsui, Kwok-Leung (7101671584)","57211201933; 55328861400; 58896843200; 7101671584","Designing interpretable ML system to enhance trust in healthcare: A systematic review to proposed responsible clinician-AI-collaboration framework","2024","Information Fusion","108","","102412","","","","0","10.1016/j.inffus.2024.102412","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189938333&doi=10.1016%2fj.inffus.2024.102412&partnerID=40&md5=c350f539774b809b5e2c2fbe18bd9d05","Background: Artificial intelligence (AI)-based medical devices and digital health technologies, including medical sensors, wearable health trackers, telemedicine, mobile health (mHealth), large language models (LLMs), and digital care twins (DCTs), significantly influence the process of clinical decision support systems (CDSS) in healthcare and medical applications. However, given the complexity of medical decisions, it is crucial that results generated by AI tools not only be correct but also carefully evaluated, understandable, and explainable to end-users, especially clinicians. The lack of interpretability in communicating AI clinical decisions can lead to mistrust among decision-makers and a reluctance to use these technologies. Objective: This paper systematically reviews the processes and challenges associated with interpretable machine learning (IML) and explainable artificial intelligence (XAI) within the healthcare and medical domains. Its main goals are to examine the processes of IML and XAI, their related methods, applications, and the implementation challenges they pose in digital health interventions (DHIs), particularly from a quality control perspective, to help understand and improve communication between AI systems and clinicians. The IML process is categorized into pre-processing interpretability, interpretable modeling, and post-processing interpretability. This paper aims to foster a comprehensive understanding of the significance of a robust interpretability approach in clinical decision support systems (CDSS) by reviewing related experimental results. The goal is to provide future researchers with insights for creating clinician-AI tools that are more communicable in healthcare decision support systems and offer a deeper understanding of their challenges. Methods: Our research questions, eligibility criteria, and primary goals were proved using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guideline and the PICO (population, intervention, control, and outcomes) method. We systematically searched PubMed, Scopus, and Web of Science databases using sensitive and specific search strings. Subsequently, duplicate papers were removed using EndNote and Covidence. A two-phase selection process was then carried out on Covidence, starting with screening by title and abstract, followed by a full-text appraisal. The Meta Quality Appraisal Tool (MetaQAT) was used to assess the quality and risk of bias. Finally, a standardized data extraction tool was employed for reliable data mining. Results: The searches yielded 2,241 records, from which 555 duplicate papers were removed. During the title and abstract screening step, 958 papers were excluded, and the full-text review step excluded 482 studies. Subsequently, in quality and risk of bias assessment, 172 papers were removed. 74 publications were selected for data extraction, which formed 10 insightful reviews and 64 related experimental studies. Conclusion: The paper provides general definitions of explainable artificial intelligence (XAI) in the medical domain and introduces a framework for interpretability in clinical decision support systems structured across three levels. It explores XAI-related health applications within each tier of this framework, underpinned by a review of related experimental findings. Furthermore, the paper engages in a detailed discussion of quality assessment tools for evaluating XAI in intelligent health systems. It also presents a step-by-step roadmap for implementing XAI in clinical settings. To direct future research toward bridging current gaps, the paper examines the importance of XAI models from various angles and acknowledges their limitations. © 2024 Elsevier B.V.","Article","Final","","Scopus","2-s2.0-85189938333"
"Chadaga K.; Prabhu S.; Sampathila N.; Chadaga R.; Bhat D.; Sharma A.K.; Swathi K.S.","Chadaga, Krishnaraj (57226665664); Prabhu, Srikanth (57197645702); Sampathila, Niranjana (56584740000); Chadaga, Rajagopala (57393370000); Bhat, Devadas (57941511900); Sharma, Akhilesh Kumar (57555735800); Swathi, K.S. (58986232200)","57226665664; 57197645702; 56584740000; 57393370000; 57941511900; 57555735800; 58986232200","SADXAI: Predicting social anxiety disorder using multiple interpretable artificial intelligence techniques","2024","SLAS technology","29","2","","100129","","","0","10.1016/j.slast.2024.100129","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190398111&doi=10.1016%2fj.slast.2024.100129&partnerID=40&md5=f91b41a284dd457dbff39fffe4de3aa0","Social anxiety disorder (SAD), also known as social phobia, is a psychological condition in which a person has a persistent and overwhelming fear of being negatively judged or observed by other individuals. This fear can affect them at work, in relationships and other social activities. The intricate combination of several environmental and biological factors is the reason for the onset of this mental condition. SAD is diagnosed using a test called the ""Diagnostic and Statistical Manual of Mental Health Disorders (DSM-5), which is based on several physical, emotional and demographic symptoms. Artificial Intelligence has been a boon for medicine and is regularly used to diagnose various health conditions and diseases. Hence, this study used demographic, emotional, and physical symptoms and multiple machine learning (ML) techniques to diagnose SAD. A thorough descriptive and statistical analysis has been conducted before using the classifiers. Among all the models, the AdaBoost and logistic regression obtained the highest accuracy of 88 % each. Four eXplainable artificial techniques (XAI) techniques are utilized to make the predictions interpretable, transparent and understandable. According to XAI, the ""Liebowitz Social Anxiety Scale questionnaire"" and ""The fear of speaking in public"" are the most critical attributes in the diagnosis of SAD. This clinical decision support system framework could be utilized in various suitable locations such as schools, hospitals and workplaces to identify SAD in people. Copyright © 2024. Published by Elsevier Inc.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85190398111"
"Fania A.; Monaco A.; Amoroso N.; Bellantuono L.; Cazzolla Gatti R.; Firza N.; Lacalamita A.; Pantaleo E.; Tangaro S.; Velichevskaya A.; Bellotti R.","Fania, Alessandro (58549028700); Monaco, Alfonso (7201639219); Amoroso, Nicola (55419832300); Bellantuono, Loredana (56166549700); Cazzolla Gatti, Roberto (56022184900); Firza, Najada (57929348200); Lacalamita, Antonio (57226689619); Pantaleo, Ester (16245945500); Tangaro, Sabina (8712490600); Velichevskaya, Alena (57089057400); Bellotti, Roberto (8419904800)","58549028700; 7201639219; 55419832300; 56166549700; 56022184900; 57929348200; 57226689619; 16245945500; 8712490600; 57089057400; 8419904800","Machine learning and XAI approaches highlight the strong connection between O3 and NO2 pollutants and Alzheimer’s disease","2024","Scientific Reports","14","1","5385","","","","0","10.1038/s41598-024-55439-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186864583&doi=10.1038%2fs41598-024-55439-1&partnerID=40&md5=84f44e641356f2e0e7e9648cc2aa8a15","Alzheimer’s disease (AD) is the most common type of dementia with millions of affected patients worldwide. Currently, there is still no cure and AD is often diagnosed long time after onset because there is no clear diagnosis. Thus, it is essential to study the physiology and pathogenesis of AD, investigating the risk factors that could be strongly connected to the disease onset. Despite AD, like other complex diseases, is the result of the combination of several factors, there is emerging agreement that environmental pollution should play a pivotal role in the causes of disease. In this work, we implemented an Artificial Intelligence model to predict AD mortality, expressed as Standardized Mortality Ratio, at Italian provincial level over 5 years. We employed a set of publicly available variables concerning pollution, health, society and economy to feed a Random Forest algorithm. Using methods based on eXplainable Artificial Intelligence (XAI) we found that air pollution (mainly O3 and NO2) contribute the most to AD mortality prediction. These results could help to shed light on the etiology of Alzheimer’s disease and to confirm the urgent need to further investigate the relationship between the environment and the disease. © The Author(s) 2024.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85186864583"
"Hu M.; Yue N.; Groves R.M.","Hu, Muping (57204030877); Yue, Nan (57191439685); Groves, Roger M. (57055203600)","57204030877; 57191439685; 57055203600","Loose bolt localization and torque prediction in a bolted joint using lamb waves and explainable artificial intelligence","2024","Structural Health Monitoring","","","","","","","0","10.1177/14759217241241976","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191290484&doi=10.1177%2f14759217241241976&partnerID=40&md5=c2a883f811ce0a70bcd0a9a9c84d9d39","With the increasing application of artificial intelligence (AI) techniques in the field of structural health monitoring (SHM), there is a growing interest in explaining the decision-making of the black-box models in deep learning-based SHM methods. In this work, we take explainability a step further by using it to improve the performance of AI models. In this work, the results of explainable artificial intelligence (XAI) algorithms are used to reduce the input size of a one-dimensional convolutional neural network (1D-CNN), hence simplifying the CNN structure. To select the most accurate XAI algorithm for this purpose, we propose a new evaluation method, feature sensitivity (FS). Utilizing XAI and FS, a reduced dimension 1D-CNN regression model (FS-X1D-CNN) is proposed to locate and predict the torque of loose bolts in a 16-bolt connected aluminum plate under varying temperature conditions. The results were compared with 1D CNN with raw input vector (RI-1D-CNN) and deep autoencoders-1D-CNN (DAE-1D-CNN). It is shown that FS-X1D-CNN achieves the highest prediction accuracy with 5.95 mm in localization and 0.54 Nm in torque prediction, and converges 10 times faster than RI-1D-CNN and 15 times faster than DAE-1D-CNN, while only using a single lamb wave signal path. © The Author(s) 2024.","Article","Article in press","","Scopus","2-s2.0-85191290484"
"Iqbal T.; Khalid A.; Ullah I.","Iqbal, Talha (57204137944); Khalid, Aaleen (58883899100); Ullah, Ihsan (57204470575)","57204137944; 58883899100; 57204470575","Explaining decisions of a light-weight deep neural network for real-time coronary artery disease classification in magnetic resonance imaging","2024","Journal of Real-Time Image Processing","21","2","31","","","","0","10.1007/s11554-023-01411-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184854096&doi=10.1007%2fs11554-023-01411-7&partnerID=40&md5=ebaefe4adcb6d749dc40d0d0b882e4ef","In certain healthcare settings, such as emergency or critical care units, where quick and accurate real-time analysis and decision-making are required, the healthcare system can leverage the power of artificial intelligence (AI) models to support decision-making and prevent complications. This paper investigates the optimization of healthcare AI models based on time complexity, hyper-parameter tuning, and XAI for a classification task. The paper highlights the significance of a lightweight convolutional neural network (CNN) for analysing and classifying Magnetic Resonance Imaging (MRI) in real-time and is compared with CNN-RandomForest (CNN-RF). The role of hyper-parameter is also examined in finding optimal configurations that enhance the model’s performance while efficiently utilizing the limited computational resources. Finally, the benefits of incorporating the XAI technique (e.g. GradCAM and Layer-wise Relevance Propagation) in providing transparency and interpretable explanations of AI model predictions, fostering trust, and error/bias detection are explored. Our inference time on a MacBook laptop for 323 test images of size 100x100 is only 2.6 sec, which is merely 8 milliseconds per image while providing comparable classification accuracy with the ensemble model of CNN-RF classifiers. Using the proposed model, clinicians/cardiologists can achieve accurate and reliable results while ensuring patients’ safety and answering questions imposed by the General Data Protection Regulation (GDPR). The proposed investigative study will advance the understanding and acceptance of AI systems in connected healthcare settings. © The Author(s) 2024.","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85184854096"
"Lamba K.; Rani S.","Lamba, Kamini (57703679800); Rani, Shalli (56024601000)","57703679800; 56024601000","Explainable Artificial Intelligence for Deep Learning Models in Diagnosing Brain Tumor Disorder","2024","Lecture Notes in Networks and Systems","894","","","149","159","10","0","10.1007/978-981-99-9562-2_13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189547382&doi=10.1007%2f978-981-99-9562-2_13&partnerID=40&md5=5a2a88ff5a072e13b80bebd40afc99f9","Deep neural networks (DNNs) have shown great potential in diagnosing brain tumor disorder, but their decision-making processes can be difficult to interpret, leading to concerns about their reliability and safety. This paper presents overview of explainable artificial intelligence techniques which have been developed to improve the interpretability and transparency of DNNs and have been applied to diagnostic systems for such disorders. Based on the utilized framework of explainable artificial intelligence (XAI) in collaboration with deep learning models, authors diagnosed brain tumor with the help of convolutional neural network and interpreted its outcomes with the help of numerical gradient-weighted class activation mapping (numGrad-CAM-CNN), therefore achieved highest accuracy of 97.11%. Thus, XAI can help healthcare professionals in understanding how a DNN arrived at a diagnosis, providing insights into the reasoning and decision-making processes of the model. XAI techniques can also help to identify biases in the data used to train the model and address potential ethical concerns. However, challenges remain in implementing XAI techniques in diagnostic systems, including the need for large, diverse datasets, and the development of user-friendly interfaces. Despite these challenges, the potential benefits for improving patient outcomes and increasing trust in AI-based medical systems make it a promising area of research. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024.","Conference paper","Final","","Scopus","2-s2.0-85189547382"
"Liu F.; Zhou J.; Zuo M.; Li Y.","Liu, Fei (57222252256); Zhou, Jilei (57200581256); Zuo, Meiyun (23391425500); Li, Yibo (59070307800)","57222252256; 57200581256; 23391425500; 59070307800","Dual-process theory-driven transparent approach for seniors to accept health misinformation detection results","2024","Information Processing and Management","61","4","103751","","","","0","10.1016/j.ipm.2024.103751","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192104264&doi=10.1016%2fj.ipm.2024.103751&partnerID=40&md5=92f200d1ad23149cccf4447c4a940ae0","Given seniors’ concerns about the reliability of black-box models in health misinformation detection (HMID), there is a pressing need for explainable HMID methods that provide transparency and instill trust. Explainable artificial intelligence (AI) aims to foster understanding and trust in AI models by implementing transparency that embodies explainability and interpretability. However, most existing explainable HMID methods solely provide post-hoc explanations and neglect the interpretation of the models’ internal logic, which prevents seniors from satisfactorily accepting the HMID results. Therefore, this study proposes a transparent Knowledge Graph-aware Two-Stage approach (KG2S) driven by the dual-process theory. KG2S combines explainability and interpretability by utilizing knowledge graphs (KGs) in two stages: Knowledge Breadth Retrieval (KBR) and Knowledge Depth Reasoning (KDR). These stages correspond with the heuristic and analytic processes of the dual-process theory, which encompasses human information processing. In the KBR stage, we leverage rich facts in KGs to replicate heuristic distillation behaviors of humans through a novel similarity-diversity twofold filter. In the KDR stage, we employ a hierarchical attention network to emulate humans’ coarse-to-fine knowledge analyses during decision-making. Extensive experiments were conducted on two real-world datasets, along with user testing, to assess the effectiveness of the proposed model. Results demonstrated that the proposed model not only outperformed competing methods in terms of HMID accuracy but also provided persuasive detection processes and reasons. Moreover, the model showed high adaptability to new topics in HMID. Our research offers valuable insights into integrating humanistic into AI algorithms and promoting the trustworthiness of AI systems. © 2024 Elsevier Ltd","Article","Final","","Scopus","2-s2.0-85192104264"
"Pietilä E.; Moreno-Sánchez P.A.","Pietilä, Essi (58799943300); Moreno-Sánchez, Pedro A. (57219909354)","58799943300; 57219909354","When an Explanation is not Enough: An Overview of Evaluation Metrics of Explainable AI Systems in the Healthcare Domain","2024","IFMBE Proceedings","93","","","573","584","11","1","10.1007/978-3-031-49062-0_60","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181775359&doi=10.1007%2f978-3-031-49062-0_60&partnerID=40&md5=fb084cfbb53c329ae4cd75d37f73e8fe","Despite the promising advantages in diagnostics and treatment that Artificial Intelligence (AI) and Machine Learning (ML) can bring to the healthcare domain, the complexity and black-box behavior of the AI/ML algorithms hinder the adoption by healthcare professionals and patients due to issues regarding explainability and trustworthiness of the results. Explainable AI (XAI) has emerged to support the need for understanding the AI/ML models’ outputs and is expected to have a substantial relevance in the success of these models within the healthcare domain. Nevertheless, the information provided by XAI systems might be not enough to generate the required trustworthiness in the models. Thus, the existence of tools and metrics that allow domain experts and stakeholders to evaluate the explanations arises as needed solution. At the moment, there is an obvious lack of standardization and validation of metrics, and researchers require studies that compile the metrics together to know what, how, and why should be measured. This paper aims to provide an overview of the current metrics to evaluate XAI systems with a particular view on the healthcare domain. From the metrics identified and reviewed by following the PRISMA methodology, we present a taxonomy in which certain aspects are considered, such as the domain (general or healthcare) of the metric, as well as whether the expert is included in the validation process (human-in-the-loop). From our results, we observed many metrics developed in the general domain are being used for clinical XAI models. Nevertheless, it is essential to evaluate the XAI models in a more domain-specific manner, particularly because medical experts have valuable specialist information about the use cases that computer scientists might lack. © 2024, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Conference paper","Final","","Scopus","2-s2.0-85181775359"
"Lo Z.J.; Mak M.H.W.; Liang S.; Chan Y.M.; Goh C.C.; Lai T.; Tan A.; Thng P.; Rodriguez J.; Weyde T.; Smit S.","Lo, Zhiwen Joseph (56562854800); Mak, Malcolm Han Wen (57211554432); Liang, Shanying (57223167483); Chan, Yam Meng (57215589485); Goh, Cheng Cheng (57200267953); Lai, Tina (57224528690); Tan, Audrey (57223152209); Thng, Patrick (58785649500); Rodriguez, Jorge (58830410900); Weyde, Tillman (24476899500); Smit, Sylvia (58785649600)","56562854800; 57211554432; 57223167483; 57215589485; 57200267953; 57224528690; 57223152209; 58785649500; 58830410900; 24476899500; 58785649600","Development of an explainable artificial intelligence model for Asian vascular wound images","2024","International Wound Journal","21","4","e14565","","","","0","10.1111/iwj.14565","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180921659&doi=10.1111%2fiwj.14565&partnerID=40&md5=9dfff6a708df9300b7ba37ae5edc6cc9","Chronic wounds contribute to significant healthcare and economic burden worldwide. Wound assessment remains challenging given its complex and dynamic nature. The use of artificial intelligence (AI) and machine learning methods in wound analysis is promising. Explainable modelling can help its integration and acceptance in healthcare systems. We aim to develop an explainable AI model for analysing vascular wound images among an Asian population. Two thousand nine hundred and fifty-seven wound images from a vascular wound image registry from a tertiary institution in Singapore were utilized. The dataset was split into training, validation and test sets. Wound images were classified into four types (neuroischaemic ulcer [NIU], surgical site infections [SSI], venous leg ulcers [VLU], pressure ulcer [PU]), measured with automatic estimation of width, length and depth and segmented into 18 wound and peri-wound features. Data pre-processing was performed using oversampling and augmentation techniques. Convolutional and deep learning models were utilized for model development. The model was evaluated with accuracy, F1 score and receiver operating characteristic (ROC) curves. Explainability methods were used to interpret AI decision reasoning. A web browser application was developed to demonstrate results of the wound AI model with explainability. After development, the model was tested on additional 15 476 unlabelled images to evaluate effectiveness. After the development on the training and validation dataset, the model performance on unseen labelled images in the test set achieved an AUROC of 0.99 for wound classification with mean accuracy of 95.9%. For wound measurements, the model achieved AUROC of 0.97 with mean accuracy of 85.0% for depth classification, and AUROC of 0.92 with mean accuracy of 87.1% for width and length determination. For wound segmentation, an AUROC of 0.95 and mean accuracy of 87.8% was achieved. Testing on unlabelled images, the model confidence score for wound classification was 82.8% with an explainability score of 60.6%. Confidence score was 87.6% for depth classification with 68.0% explainability score, while width and length measurement obtained 93.0% accuracy score with 76.6% explainability. Confidence score for wound segmentation was 83.9%, while explainability was 72.1%. Using explainable AI models, we have developed an algorithm and application for analysis of vascular wound images from an Asian population with accuracy and explainability. With further development, it can be utilized as a clinical decision support system and integrated into existing healthcare electronic systems. © 2023 The Authors. International Wound Journal published by Medicalhelplines.com Inc and John Wiley & Sons Ltd.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85180921659"
"Roy S.; Pal D.; Meena T.","Roy, Sudipta (56406670700); Pal, Debojyoti (57222132945); Meena, Tanushree (57221382965)","56406670700; 57222132945; 57221382965","Explainable artificial intelligence to increase transparency for revolutionizing healthcare ecosystem and the road ahead","2024","Network Modeling Analysis in Health Informatics and Bioinformatics","13","1","4","","","","2","10.1007/s13721-023-00437-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180367177&doi=10.1007%2fs13721-023-00437-y&partnerID=40&md5=3b063297ebcd91bc096668e4bfac7c62","The integration of deep learning (DL) into co-clinical applications has generated substantial interest among researchers aiming to enhance clinical decision support systems for various aspects of disease management, including detection, prediction, diagnosis, treatment, and therapy. However, the inherent opacity of DL methods has raised concerns within the healthcare community, particularly in high-risk or complex medical domains. There exists a significant gap in research and understanding when it comes to elucidating and rendering transparent the inner workings of DL models applied to the analysis of medical images. While explainable artificial intelligence (XAI) has gained ground in diverse fields, including healthcare, numerous unexplored facets remain within the realm of medical imaging. To better understand the complexities of DL techniques, there is an urgent need for rapid advancement in the field of eXplainable DL (XDL) or eXplainable Artificial Intelligence (XAI). This would empower healthcare professionals to comprehend, assess, and contribute to decision-making processes before taking any actions. This viewpoint article conducts an extensive review of XAI and XDL, shedding light on methods for unveiling the “black-box” nature of DL. Additionally, it explores the adaptability of techniques originally designed for solving problems across diverse domains for addressing healthcare challenges. The article also delves into how physicians can interpret and comprehend data-driven technologies effectively. This comprehensive literature review serves as a valuable resource for scientists and medical practitioners, offering insights into both technical and clinical aspects. It assists in identifying methods to make XAI and XDL models more comprehensible, enabling wise model choices based on particular requirements and goals. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Austria, part of Springer Nature.","Review","Final","","Scopus","2-s2.0-85180367177"
"Lalk C.; Steinbrenner T.; Kania W.; Popko A.; Wester R.; Schaffrath J.; Eberhardt S.; Schwartz B.; Lutz W.; Rubel J.","Lalk, Christopher (57216675894); Steinbrenner, Tobias (58959926400); Kania, Weronika (58960555300); Popko, Alexander (58959926500); Wester, Robin (57487571500); Schaffrath, Jana (57651807200); Eberhardt, Steffen (57705026600); Schwartz, Brian (57212013199); Lutz, Wolfgang (7202713172); Rubel, Julian (55462606800)","57216675894; 58959926400; 58960555300; 58959926500; 57487571500; 57651807200; 57705026600; 57212013199; 7202713172; 55462606800","Measuring Alliance and Symptom Severity in Psychotherapy Transcripts Using Bert Topic Modeling","2024","Administration and Policy in Mental Health and Mental Health Services Research","","","","","","","0","10.1007/s10488-024-01356-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188964678&doi=10.1007%2fs10488-024-01356-4&partnerID=40&md5=2cba2eada2c0e71310093f1b330b2e4e","We aim to use topic modeling, an approach for discovering clusters of related words (“topics”), to predict symptom severity and therapeutic alliance in psychotherapy transcripts, while also identifying the most important topics and overarching themes for prediction. We analyzed 552 psychotherapy transcripts from 124 patients. Using BERTopic (Grootendorst, 2022), we extracted 250 topics each for patient and therapist speech. These topics were used to predict symptom severity and alliance with various competing machine-learning methods. Sensitivity analyses were calculated for a model based on 50 topics, LDA-based topic modeling, and a bigram model. Additionally, we grouped topics into themes using qualitative analysis and identified key topics and themes with eXplainable Artificial Intelligence (XAI). Symptom severity could be predicted with highest accuracy by patient topics (r=0.45, 95%-CI 0.40, 0.51), whereas alliance was better predicted by therapist topics (r=0.20, 95%-CI 0.16, 0.24). Drivers for symptom severity were themes related to health and negative experiences. Lower alliance was correlated with various themes, especially psychotherapy framework, income, and everyday life. This analysis shows the potential of using topic modeling in psychotherapy research allowing to predict several treatment-relevant metrics with reasonable accuracy. Further, the use of XAI allows for an analysis of the individual predictive value of topics and themes. Limitations entail heterogeneity across different topic modeling hyperparameters and a relatively small sample size. © The Author(s) 2024.","Article","Article in press","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85188964678"
"Sun Z.","Sun, Zhuanlan (57208837572)","57208837572","Textual features of peer review predict top-cited papers: An interpretable machine learning perspective","2024","Journal of Informetrics","18","2","101501","","","","1","10.1016/j.joi.2024.101501","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183453057&doi=10.1016%2fj.joi.2024.101501&partnerID=40&md5=6ef6194b53a423fd25d696b76c79f79a","Peer review is crucial in improving the quality and reliability of scientific research. However, the mechanisms through which peer review practices ensure papers become top-cited papers (TCPs) after publication are not well understood. In this study, by collecting a data set containing 13, 066 papers published between 2016 and 2020 from Nature communications with open peer review reports, we aim to examine how textual features embedded within the peer review reports of papers that reflect the reviewers’ emotions may predict the papers to be TCPs. We compiled a list of 15 textual features and classified them into three categories: peer review features, linguistic features, and sentiment features. We then chose the XGBoost machine learning model with the best performance in predicting TCPs, and utilized the explainable artificial intelligence techniques SHAP to interpret the role of feature importance on the prediction results. The distribution of feature importance ranking results demonstrates that sentiment features play a crucial role in determining papers’ potential to be highly cited. This conclusion still holds, even when the ranking of the feature importance changes in the subgroup analysis of dividing the samples into four disciplines (biological sciences, health sciences, physical sciences, and earth and environmental sciences), as well as two groups based on whether reviewers’ identities were revealed. This research emphasizes the textual features retrieved from peer review reports that play role in improving manuscript quality can predict the post-publication research impact. © 2024","Article","Final","","Scopus","2-s2.0-85183453057"
"Choi I.; Kim J.; Kim W.C.","Choi, Insu (57224825321); Kim, Jihye (57203325064); Kim, Woo Chang (55479371400)","57224825321; 57203325064; 55479371400","An Explainable Prediction for Dietary-Related Diseases via Language Models","2024","Nutrients","16","5","686","","","","0","10.3390/nu16050686","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187464386&doi=10.3390%2fnu16050686&partnerID=40&md5=86cd178474ac3e5b827b04780cafba55","Our study harnesses the power of natural language processing (NLP) to explore the relationship between dietary patterns and metabolic health outcomes among Korean adults using data from the Seventh Korea National Health and Nutrition Examination Survey (KNHANES VII). Using Latent Dirichlet Allocation (LDA) analysis, we identified three distinct dietary patterns: “Traditional and Staple”, “Communal and Festive”, and “Westernized and Convenience-Oriented”. These patterns reflect the diversity of dietary preferences in Korea and reveal the cultural and social dimensions influencing eating habits and their potential implications for public health, particularly concerning obesity and metabolic disorders. Integrating NLP-based indices, including sentiment scores and the identified dietary patterns, into our predictive models significantly enhanced the accuracy of obesity and dyslipidemia predictions. This improvement was consistent across various machine learning techniques—XGBoost, LightGBM, and CatBoost—demonstrating the efficacy of NLP methodologies in refining disease prediction models. Our findings underscore the critical role of dietary patterns as indicators of metabolic diseases. The successful application of NLP techniques offers a novel approach to public health and nutritional epidemiology, providing a deeper understanding of the diet–disease nexus. This study contributes to the evolving field of personalized nutrition and emphasizes the potential of leveraging advanced computational tools to inform targeted nutritional interventions and public health strategies aimed at mitigating the prevalence of metabolic disorders in the Korean population. © 2024 by the authors.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85187464386"
"Domínguez J.; Prociuk D.; Marović B.; Čyras K.; Cocarascu O.; Ruiz F.; Mi E.; Mi E.; Ramtale C.; Rago A.; Darzi A.; Toni F.; Curcin V.; Delaney B.","Domínguez, Jesús (57226244834); Prociuk, Denys (57207300258); Marović, Branko (6506266913); Čyras, Kristijonas (57074054700); Cocarascu, Oana (57191373347); Ruiz, Francis (8508856100); Mi, Ella (57163595100); Mi, Emma (57208184596); Ramtale, Christian (57212173858); Rago, Antonio (6602273542); Darzi, Ara (57540830600); Toni, Francesca (6603756423); Curcin, Vasa (8510022100); Delaney, Brendan (7005430757)","57226244834; 57207300258; 6506266913; 57074054700; 57191373347; 8508856100; 57163595100; 57208184596; 57212173858; 6602273542; 57540830600; 6603756423; 8510022100; 7005430757","ROAD2H: Development and evaluation of an open-source explainable artificial intelligence approach for managing co-morbidity and clinical guidelines","2024","Learning Health Systems","8","2","e10391","","","","0","10.1002/lrh2.10391","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170654120&doi=10.1002%2flrh2.10391&partnerID=40&md5=17e1324664a5121b9cc02a59ea26b2a3","Introduction: Clinical decision support (CDS) systems (CDSSs) that integrate clinical guidelines need to reflect real-world co-morbidity. In patient-specific clinical contexts, transparent recommendations that allow for contraindications and other conflicts arising from co-morbidity are a requirement. In this work, we develop and evaluate a non-proprietary, standards-based approach to the deployment of computable guidelines with explainable argumentation, integrated with a commercial electronic health record (EHR) system in Serbia, a middle-income country in West Balkans. Methods: We used an ontological framework, the Transition-based Medical Recommendation (TMR) model, to represent, and reason about, guideline concepts, and chose the 2017 International global initiative for chronic obstructive lung disease (GOLD) guideline and a Serbian hospital as the deployment and evaluation site, respectively. To mitigate potential guideline conflicts, we used a TMR-based implementation of the Assumptions-Based Argumentation framework extended with preferences and Goals (ABA+G). Remote EHR integration of computable guidelines was via a microservice architecture based on HL7 FHIR and CDS Hooks. A prototype integration was developed to manage chronic obstructive pulmonary disease (COPD) with comorbid cardiovascular or chronic kidney diseases, and a mixed-methods evaluation was conducted with 20 simulated cases and five pulmonologists. Results: Pulmonologists agreed 97% of the time with the GOLD-based COPD symptom severity assessment assigned to each patient by the CDSS, and 98% of the time with one of the proposed COPD care plans. Comments were favourable on the principles of explainable argumentation; inclusion of additional co-morbidities was suggested in the future along with customisation of the level of explanation with expertise. Conclusion: An ontological model provided a flexible means of providing argumentation and explainable artificial intelligence for a long-term condition. Extension to other guidelines and multiple co-morbidities is needed to test the approach further. © 2023 The Authors. Learning Health Systems published by Wiley Periodicals LLC on behalf of University of Michigan.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85170654120"
"Syed M.J.; Goggins J.","Syed, Muslim Jameel (58221543800); Goggins, Jamie (16506997000)","58221543800; 16506997000","Explanation artificial intelligence-based condition monitoring for tidal energy turbines","2024","AIP Conference Proceedings","2919","1","140001","","","","0","10.1063/5.0184400","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190802498&doi=10.1063%2f5.0184400&partnerID=40&md5=bc9209d194ac01fa4cf413170361d914","Our reliance on fossil fuels primarily contributes to global warming and threatens our survival. Renewable energy is currently considered the leading solution to reduce greenhouse gas emissions. Energy extraction from the ocean's tides can help fulfill the global renewable energy demand and combat world climate crises. With 45% of Europe's citizens living in coastal regions, Ocean Energy Europe predicts that ocean energy can meet 10% of EU electricity demands by 2050. Installations at this scale will have the associated benefit of reducing the levelised cost of tidal energy (LCOE) towards a target of €100/MWh, which will make ocean energy a viable compatriot to offshore wind. To achieve this, increased performance and reliability of tidal energy devices is required. In particular, there is a need for further technology investigation and demonstration for improved reliability and efficiency of tidal turbine rotor and blades, including control and condition monitoring systems. Failure in a blade can create long downtimes. The research project will develop and demonstrate an innovative condition monitoring system for rotors and blades of tidal energy devices that provides invaluable learnings regarding performance, reliability, availability, maintainability and survivability. Machine learning and data analytics methods will be employed utilising measured data for an existing operating tidal energy device, which will improve the seaworthiness of rotor and blades, reducing the likelihood of failure, as well as reducing operating costs. Due to harsh marine environments, achieving reliable operational health and performance for rotor and blade components is crucial. Fault diagnoses and maintenance operations become more challenging in the sea, leading to performance degradation, failure, or breakdown of the entire tidal energy system if unattended. Therefore, the evidence-based structure health monitoring (SHM) system that will be developed in the research project could effectively avoid unplanned shutdowns or catastrophic failure. For example, by diagnosing early signs of faults/damages in the tidal turbines allows pre-emptive maintenance scheduling. For the development of the SHM system, along with the statistical approaches, the explainable artificial intelligence (XAI) based methods will be helpful to detect the deterioration of the tidal components and allow the observer to comprehend and trust the decision. The evidence based SHM system will increase reliability and improve performance over the entire tidal turbine life in complex environmental tidal conditions.  © 2024 Author(s).","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85190802498"
"Doan Q.C.; Chen C.; He S.; Zhang X.","Doan, Quang Cuong (57219473900); Chen, Chen (57213375212); He, Shenjing (8621446800); Zhang, Xiaohu (57202025299)","57219473900; 57213375212; 8621446800; 57202025299","How urban air quality affects land values: Exploring non-linear and threshold mechanism using explainable artificial intelligence","2024","Journal of Cleaner Production","434","","140340","","","","1","10.1016/j.jclepro.2023.140340","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181584583&doi=10.1016%2fj.jclepro.2023.140340&partnerID=40&md5=8b35bb9b3c6d19a8bcb763102ef018f4","Air pollution has a significant impact on human health and influences housing choices. Existing studies on determinant factors affecting property prices, including air pollution, mainly employed hedonic and spatial regression models that have limitations in capturing non-linear relationships and local interactions. Neglecting the non-linear relationships can lead to incomplete estimation impacts between explainable features and prices. To fill these gaps, this study used machine learning algorithms and SHAP (SHapley Additive exPlanations) to identify the relative importance of air pollutant variables on land values and their non-linear mechanism. The results showed that the Extreme Gradient Boosting (XGBoost) outperformed Ordinary Least Squares (OLS), Gradient Boosting Decision Tree (GBDT), Random Forest (RF), K-Nearest Neighbors (K-NN), and Extra Tree Regression (ETR) models in price prediction and capturing the non-linear relationship between air pollution and land values. Notably, Ozone (O3) and Nitric Oxide (NO) concentrations contribute 1.64% and 1.47% to land value variation, respectively. The results confirmed the non-linear relationship and identified that the threshold effect between air pollution and land values is at mean concentration values. Furthermore, we observed that the negative influence of O3 and NO on land values appears as their concentration level is higher than 29.3 and 13 ppb, respectively. This paper contributes valuable insights by advancing our understanding of the non-linear mechanism impact of air pollution on house and land values, informing environmental policymaking, and shedding light on housing decisions. © 2023 Elsevier Ltd","Article","Final","","Scopus","2-s2.0-85181584583"
"Goswami N.G.; Goswami A.; Sampathila N.; Bairy M.G.; Chadaga K.; Belurkar S.","Goswami, Neelankit Gautam (57942590900); Goswami, Anushree (58564297000); Sampathila, Niranjana (56584740000); Bairy, Muralidhar G. (57942168900); Chadaga, Krishnaraj (57226665664); Belurkar, Sushma (55681709200)","57942590900; 58564297000; 56584740000; 57942168900; 57226665664; 55681709200","Detection of sickle cell disease using deep neural networks and explainable artificial intelligence","2024","Journal of Intelligent Systems","33","1","20230179","","","","0","10.1515/jisys-2023-0179","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190561252&doi=10.1515%2fjisys-2023-0179&partnerID=40&md5=c9ac7d70d5ccf163918001618777bed9","Sickle cell disease (SCD), a blood disorder that transforms the shape of red blood cells into a distinctive sickle form, is a major concern as it not only compromises the blood’s oxygen-carrying capacity but also poses significant health risks, ranging from weakness to paralysis and, in severe cases, even fatality. This condition not only underscores the pressing need for innovative solutions but also encapsulates the broader challenges faced by medical professionals, including delayed treatment, protracted processes, and the potential for subjective errors in diagnosis and classification. Consequently, the application of artificial intelligence (AI) in healthcare has emerged as a transformative force, inspiring multidisciplinary efforts to overcome the complexities associated with SCD and enhance diagnostic accuracy and treatment outcomes. The use of transfer learning helps to extract features from the input dataset and give an accurate prediction. We analyse and compare the performance parameters of three distinct models for this purpose: GoogLeNet, ResNet18, and ResNet50. The best results were shown by the ResNet50 model, with an accuracy of 94.90%. Explainable AI is the best approach for transparency and confirmation of the predictions made by the classifiers. This research utilizes Grad-CAM to interpret and make the models more reliable. Therefore, this specific approach benefits pathologists through its speed, precision, and accuracy of classification of sickle cells. © 2024 the author(s),","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85190561252"
"Novielli P.; Romano D.; Magarelli M.; Bitonto P.D.; Diacono D.; Chiatante A.; Lopalco G.; Sabella D.; Venerito V.; Filannino P.; Bellotti R.; De Angelis M.; Iannone F.; Tangaro S.","Novielli, Pierfrancesco (58172791300); Romano, Donato (58174292500); Magarelli, Michele (58764191600); Bitonto, Pierpaolo Di (58861780400); Diacono, Domenico (6603113523); Chiatante, Annalisa (58910439700); Lopalco, Giuseppe (56054615700); Sabella, Daniele (57763750000); Venerito, Vincenzo (57189029364); Filannino, Pasquale (55648428600); Bellotti, Roberto (8419904800); De Angelis, Maria (7102614438); Iannone, Florenzo (7003309859); Tangaro, Sabina (8712490600)","58172791300; 58174292500; 58764191600; 58861780400; 6603113523; 58910439700; 56054615700; 57763750000; 57189029364; 55648428600; 8419904800; 7102614438; 7003309859; 8712490600","Explainable artificial intelligence for microbiome data analysis in colorectal cancer biomarker identification","2024","Frontiers in Microbiology","15","","1348974","","","","2","10.3389/fmicb.2024.1348974","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186209289&doi=10.3389%2ffmicb.2024.1348974&partnerID=40&md5=743483e420ad6c30780209f7e92f6312","Background: Colorectal cancer (CRC) is a type of tumor caused by the uncontrolled growth of cells in the mucosa lining the last part of the intestine. Emerging evidence underscores an association between CRC and gut microbiome dysbiosis. The high mortality rate of this cancer has made it necessary to develop new early diagnostic methods. Machine learning (ML) techniques can represent a solution to evaluate the interaction between intestinal microbiota and host physiology. Through explained artificial intelligence (XAI) it is possible to evaluate the individual contributions of microbial taxonomic markers for each subject. Our work also implements the Shapley Method Additive Explanations (SHAP) algorithm to identify for each subject which parameters are important in the context of CRC. Results: The proposed study aimed to implement an explainable artificial intelligence framework using both gut microbiota data and demographic information from subjects to classify a cohort of control subjects from those with CRC. Our analysis revealed an association between gut microbiota and this disease. We compared three machine learning algorithms, and the Random Forest (RF) algorithm emerged as the best classifier, with a precision of 0.729 ± 0.038 and an area under the Precision-Recall curve of 0.668 ± 0.016. Additionally, SHAP analysis highlighted the most crucial variables in the model's decision-making, facilitating the identification of specific bacteria linked to CRC. Our results confirmed the role of certain bacteria, such as Fusobacterium, Peptostreptococcus, and Parvimonas, whose abundance appears notably associated with the disease, as well as bacteria whose presence is linked to a non-diseased state. Discussion: These findings emphasizes the potential of leveraging gut microbiota data within an explainable AI framework for CRC classification. The significant association observed aligns with existing knowledge. The precision exhibited by the RF algorithm reinforces its suitability for such classification tasks. The SHAP analysis not only enhanced interpretability but identified specific bacteria crucial in CRC determination. This approach opens avenues for targeted interventions based on microbial signatures. Further exploration is warranted to deepen our understanding of the intricate interplay between microbiota and health, providing insights for refined diagnostic and therapeutic strategies. Copyright © 2024 Novielli, Romano, Magarelli, Bitonto, Diacono, Chiatante, Lopalco, Sabella, Venerito, Filannino, Bellotti, De Angelis, Iannone and Tangaro.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85186209289"
"Doulani K.; Rajput A.; Hazra A.; Adhikari M.; Singh A.K.","Doulani, Khushbu (57828775200); Rajput, Amrita (58664076100); Hazra, Abhishek (57203914210); Adhikari, Mainak (56340886300); Singh, Amit Kumar (55726466900)","57828775200; 58664076100; 57203914210; 56340886300; 55726466900","Explainable AI for Communicable Disease Prediction and Sustainable Living: Implications for Consumer Electronics","2024","IEEE Transactions on Consumer Electronics","70","1","","2460","2467","7","4","10.1109/TCE.2023.3325155","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174820979&doi=10.1109%2fTCE.2023.3325155&partnerID=40&md5=0fb74159c381b77eb0f2b44122cdcf0d","Communicable diseases are transmitted through water, food, contaminated surfaces, bodily fluids, air. In such a situation, staying in home isolation for fewer chronic health problems and monitoring health status frequently through Medical Sensors (MSs) is recommended. The use of Artificial Intelligence (AI) in smart consumer electronics and sustainable healthcare has recently demonstrated remarkable results. However, the healthcare domain requires high levels of accountability and transparency for communicable disease prediction and sustainable life in edge networks. This paper aims to present an intelligent healthcare prototype that can identify risk factors according to monitoring parameters by analyzing the Explainable XGBoost (XXGB) model. Using edge networks for sustainable living, we explore the intersection between healthcare and consumer electronics. Initially, the prototype has been trained using the XXGB model over one publicly available dataset related to communicable diseases. Next, the prototype identifies patient risk factors by analyzing real-time monitoring parameters. Simulation results illustrate the efficiency of the proposed XXGB model up to 84.2% accuracy, which is higher than existing models. © 1975-2011 IEEE.","Article","Final","","Scopus","2-s2.0-85174820979"
"Lavanya J M S.; P S.","Lavanya J M, Sheela (58999473600); P, Subbulakshmi (58999028800)","58999473600; 58999028800","Innovative approach towards early prediction of ovarian cancer: Machine learning- enabled XAI techniques","2024","Heliyon","10","9","e29197","","","","0","10.1016/j.heliyon.2024.e29197","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191185593&doi=10.1016%2fj.heliyon.2024.e29197&partnerID=40&md5=36dfef989be63382f56a48e7abf0b954","Globally, ovarian cancer affects women disproportionately, causing significant morbidity and mortality rates. The early diagnosis of ovarian cancer is necessary for enhancing patient health and survival rates. This research article explores the utilization of Machine Learning (ML) techniques alongside eXplainable Artificial Intelligence (XAI) methodologies to aid in the early detection of ovarian cancer. ML techniques have recently gained popularity in developing predictive models to detect early-stage ovarian cancer. These predictions are made using XAI in a transparent and understandable way for healthcare professionals and patients. The primary aim of this study is to evaluate the effectiveness of various ovarian cancer prediction methodologies. This includes assessing K Nearest Neighbors, Support Vector Machines, Decision trees, and ensemble learning techniques such as Max Voting, Boosting, Bagging, and Stacking. A dataset of 349 patients with known ovarian cancer status was collected from Kaggle. The dataset included a comprehensive range of clinical features such as age, family history, tumor markers, and imaging characteristics. Preprocessing techniques were applied to enhance input data, including feature scaling and dimensionality reduction. A Minimum Redundancy Maximum Relevance (MRMR) algorithm was used to select the features in the model. Our experimental results demonstrate that in Support Vector Machines, we found 85 % base model accuracy and 89 % accuracy after stacking several ensemble learning techniques. With the help of XAI, complex ML algorithms can be given more profound insights into their decision-making, improving their applicability. This paper aims to introduce the best practices for integrating ML and artificial intelligence in biomarker evaluation. Building and evaluating Shapley values-based classifiers and visualizing results were the focus of our investigation. The study contributes to the field of oncology and women's health by offering a promising approach to the early diagnosis of ovarian cancer. © 2024 The Authors","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85191185593"
"Romano D.; Novielli P.; Diacono D.; Cilli R.; Pantaleo E.; Amoroso N.; Bellantuono L.; Monaco A.; Bellotti R.; Tangaro S.","Romano, Donato (58174292500); Novielli, Pierfrancesco (58172791300); Diacono, Domenico (6603113523); Cilli, Roberto (57218566813); Pantaleo, Ester (16245945500); Amoroso, Nicola (55419832300); Bellantuono, Loredana (56166549700); Monaco, Alfonso (7201639219); Bellotti, Roberto (8419904800); Tangaro, Sabina (8712490600)","58174292500; 58172791300; 6603113523; 57218566813; 16245945500; 55419832300; 56166549700; 7201639219; 8419904800; 8712490600","Insights from Explainable Artificial Intelligence of Pollution and Socioeconomic Influences for Respiratory Cancer Mortality in Italy","2024","Journal of Personalized Medicine","14","4","430","","","","0","10.3390/jpm14040430","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191690673&doi=10.3390%2fjpm14040430&partnerID=40&md5=234669448e0aae9df7855cf347743bf4","Respiratory malignancies, encompassing cancers affecting the lungs, the trachea, and the bronchi, pose a significant and dynamic public health challenge. Given that air pollution stands as a significant contributor to the onset of these ailments, discerning the most detrimental agents becomes imperative for crafting policies aimed at mitigating exposure. This study advocates for the utilization of explainable artificial intelligence (XAI) methodologies, leveraging remote sensing data, to ascertain the primary influencers on the prediction of standard mortality rates (SMRs) attributable to respiratory cancer across Italian provinces, utilizing both environmental and socioeconomic data. By scrutinizing thirteen distinct machine learning algorithms, we endeavor to pinpoint the most accurate model for categorizing Italian provinces as either above or below the national average SMR value for respiratory cancer. Furthermore, employing XAI techniques, we delineate the salient factors crucial in predicting the two classes of SMR. Through our machine learning scrutiny, we illuminate the environmental and socioeconomic factors pertinent to mortality in this disease category, thereby offering a roadmap for prioritizing interventions aimed at mitigating risk factors. © 2024 by the authors.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85191690673"
"Shi Z.; Chang F.; Jia Y.; Li J.; Qiu Y.; Miao J.; Jiang W.; Guo X.; Han X.; Tang W.","Shi, Zhonghao (58745228500); Chang, Fangyuan (57203525153); Jia, Yuanzheng (57376362900); Li, Jun (57201796989); Qiu, Yawei (57201357158); Miao, Jinfeng (55272328400); Jiang, Wei (56931737900); Guo, Xiaojun (35791104200); Han, Xiangan (57711843100); Tang, Wei (57220768948)","58745228500; 57203525153; 57376362900; 57201796989; 57201357158; 55272328400; 56931737900; 35791104200; 57711843100; 57220768948","Classifying and Understanding of Dairy Cattle Health Using Wearable Inertial Sensors with Random Forest and Explainable Artificial Intelligence","2024","IEEE Sensors Letters","8","3","6001804","1","4","3","0","10.1109/LSENS.2024.3358619","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183941947&doi=10.1109%2fLSENS.2024.3358619&partnerID=40&md5=a9ca968246cf73d6ae3811e8ff0ef989","Recent developments in the field of machine learning andwearable sensor technology have led to a renewed interest in the efficient monitoring of cattle health conditions. Wearable inertial sensors are particularly suited for a noninvasive solution to assess cattle health by continuously providing their daily behavior information. However, current classification methods have limitations in terms of the lack of interpretability analysis of the resulting models for the combined behaviors. In this study, a data-driven assessment approach was proposed to classify and understand cattle health through behavior recognition and health classification algorithms. Data were collected through wearable sensors attached to the cattle's collar and leg, providing acceleration, angular velocity, and Euler angles. Random forest was used to implement behavior recognition and health classification of dairy cattle, which was incorporated with explainable artificial intelligence (XAI) for result interpretation. The behavior recognition model achieves high accuracy in distinguishing between behaviors. The health classification model exhibits a strong discriminative ability, and the receiver operating characteristic curve confirms its effectiveness in identifying cattle health status using behavior data. The results showcase the crucial role of XAI analysis in establishing a correlational description between the combined behavioral characteristics of cows and their health classification. This work not only enhances the credibility of the development model but also offers valuable guidance for the predictions of cattle health by motion and feeding behaviors.  © 2017 IEEE.","Article","Final","","Scopus","2-s2.0-85183941947"
"Hu M.; Yue N.; Groves R.M.","Hu, Muping (57204030877); Yue, Nan (57191439685); Groves, Roger M. (57055203600)","57204030877; 57191439685; 57055203600","Damage Classification of a Bolted Connection using Guided Waves and Explainable Artificial Intelligence","2024","Procedia Structural Integrity","52","","","224","233","9","0","10.1016/j.prostr.2023.12.023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186668093&doi=10.1016%2fj.prostr.2023.12.023&partnerID=40&md5=06f8f01162940cbc2764480185844891","With the improvements in computational power and advances in chip and sensor technology, the applications of machine learning (ML) technologies in structural health monitoring (SHM) are increasing rapidly. Compared with traditional methods, deep learning based SHM (Deep SHM) methods are more efficient and have a higher accuracy. However, due to the black box nature of deep learning, the trained models are usually difficult to interpret, which blocks their practical application. Therefore, it is of great importance to develop explainable artificial intelligence (XAI) methods to understand the internal decision-making mechanisms of damage classification in Deep SHM. In this paper, a novel XAI algorithm named Deep Gradient-weighted Class Activation Mapping (Deep Grad CAM) is proposed by combining the existing method Grad CAM with the convolutional neural network (CNN) deconvolution mechanism. In this paper, Deep Grad CAM is used to interpret a one-dimensional convolutional neural network trained to detect bolt loosening based on guided wave propagation. The interpretation performance of Deep Grad CAM is compared with Grad CAM, and their performances are quantified using Infidelity. The results show that the Infidelity of Deep Grad CAM is much smaller than that of Grad CAM, indicating significant improvements in explanation accuracy and reliability. © 2023 The Authors. Published by Elsevier B.V.","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85186668093"
"Kundu A.; Nguyen U.T.","Kundu, Arghya (58979112500); Nguyen, Uyen Trang (55664212400)","58979112500; 55664212400","Automated Fact Checking Using A Knowledge Graph-based Model","2024","6th International Conference on Artificial Intelligence in Information and Communication, ICAIIC 2024","","","","709","716","7","0","10.1109/ICAIIC60209.2024.10463196","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189943978&doi=10.1109%2fICAIIC60209.2024.10463196&partnerID=40&md5=2d0e81929a311f8b3ebcb78c13b5e697","Misinformation is a growing threat to the economy, social stability, public health, democracy, and national security. One of the most effective methods to combat misinformation is fact checking. Fact checking is the process of verifying the factual accuracy of a statement or claim. Fact checkers employ rigorous methodologies to scrutinize claims, verify sources, and expose falsehoods. However, the huge volume of content circulating online makes it challenging for humans to identify misinformation manually. Automated tools can analyze large datasets to detect patterns in misinformation content, scaling up fact checking efforts. This paper proposes a knowledge graph-based fact checking model that uses two separate knowledge graphs, one containing true claims and the other, false claims. The model uses knowledge graph embeddings which are based on convolutional neural networks. The deep learning model is trained on the above two knowledge graphs to learn distinguishing patterns between true and false claims. Additionally, we employ explainable artificial intelligence (XAI) techniques to provide explanations for the model's classification, reducing cost of errors and increasing transparency and user trust in the system. © 2024 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85189943978"
"Aversano L.; Bernardi M.L.; Cimitile M.; Montano D.; Pecori R.; Veltri L.","Aversano, Lerina (6701736448); Bernardi, Mario Luca (57195515766); Cimitile, Marta (23392132800); Montano, Debora (57751325700); Pecori, Riccardo (35186369500); Veltri, Luca (6603599631)","6701736448; 57195515766; 23392132800; 57751325700; 35186369500; 6603599631","Explainable Anomaly Detection of Synthetic Medical IoT Traffic Using Machine Learning","2024","SN Computer Science","5","5","488","","","","0","10.1007/s42979-024-02830-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191298105&doi=10.1007%2fs42979-024-02830-4&partnerID=40&md5=1a217f23e6bac42f82850314b9ab9c73","In the context of the Internet of Things (IoT), particularly within medical facilities, the detection and categorization of Internet traffic remain significant challenges. While conventional methods for IoT traffic analysis can be applied, obtaining suitable medical traffic data is challenging due to the stringent privacy constraints associated with the health domain. To address this, this study proposes a network traffic simulation approach using an open-source tool called IoT Flock, which supports both CoAP and MQTT protocols. The tool is used to create a synthetic dataset, to simulate IoT traffic originating from various smart devices in different hospital rooms. The study shows a complete anomaly detection analysis of IoT-Flock-generated traffic, both normal and malicious, by leveraging and comparing traditional machine learning techniques, deep learning models with multiple hidden layers, and explainable artificial intelligence techniques. The results are very promising. For the binary classification, for example, the obtained accuracy is close to 100% in the case of the CoAP protocol. Good results are also obtained when the multinomial classification is performed, observing that CoAP packets are classified better than MQTT packets, even if the identification of the different MQTT packets reaches very high metrics for the most of the considered algorithms. Moreover, the obtained classification rules are also meaningful in the considered IoT context. The results indicate that IoT-Flock synthetic data can effectively be used to train and test machine and deep learning models for detecting abnormal IoT traffic in medical scenarios. This research attempts also to bridge the gap between IoT security and healthcare, providing useful insights into securing medical IoT networks in general. © The Author(s) 2024.","Article","Final","","Scopus","2-s2.0-85191298105"
"Tang H.; Miri Rekavandi A.; Rooprai D.; Dwivedi G.; Sanfilippo F.M.; Boussaid F.; Bennamoun M.","Tang, Hao (58158397800); Miri Rekavandi, Aref (58016959600); Rooprai, Dharjinder (25629906900); Dwivedi, Girish (10340469500); Sanfilippo, Frank M. (9736813100); Boussaid, Farid (6602749360); Bennamoun, Mohammed (7004376121)","58158397800; 58016959600; 25629906900; 10340469500; 9736813100; 6602749360; 7004376121","Analysis and evaluation of explainable artificial intelligence on suicide risk assessment","2024","Scientific Reports","14","1","6163","","","","0","10.1038/s41598-024-53426-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187910106&doi=10.1038%2fs41598-024-53426-0&partnerID=40&md5=420035e4b1c6796bbda1878a0ffae172","This study explores the effectiveness of Explainable Artificial Intelligence (XAI) for predicting suicide risk from medical tabular data. Given the common challenge of limited datasets in health-related Machine Learning (ML) applications, we use data augmentation in tandem with ML to enhance the identification of individuals at high risk of suicide. We use SHapley Additive exPlanations (SHAP) for XAI and traditional correlation analysis to rank feature importance, pinpointing primary factors influencing suicide risk and preventive measures. Experimental results show the Random Forest (RF) model is excelling in accuracy, F1 score, and AUC (>97% across metrics). According to SHAP, anger issues, depression, and social isolation emerge as top predictors of suicide risk, while individuals with high incomes, esteemed professions, and higher education present the lowest risk. Our findings underscore the effectiveness of ML and XAI in suicide risk assessment, offering valuable insights for psychiatrists and facilitating informed clinical decisions. © Crown 2024.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85187910106"
"Khan U.A.; Kauttonen J.; Henttonen P.; Määttänen I.","Khan, Umair Ali (56653146300); Kauttonen, Janne (24449222400); Henttonen, Pentti (56180528600); Määttänen, Ilmari (37091039200)","56653146300; 24449222400; 56180528600; 37091039200","Understanding the impact of sisu on workforce and well-being: A machine learning-based analysis","2024","Heliyon","10","2","e24148","","","","0","10.1016/j.heliyon.2024.e24148","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182550173&doi=10.1016%2fj.heliyon.2024.e24148&partnerID=40&md5=1478ab797c3bef907514f7fb320f8ec3","This study investigates the construct of sisu, a Finnish attribute representing mental resilience and fortitude when confronted with difficult situations. By leveraging advanced analytical methods and explainable Artificial Intelligence, we gain insights into how sisu factors influence well-being, work efficiency, and overall health. We investigate how the beneficial aspects of sisu contribute significantly to mental and physical health, satisfaction, and professional accomplishments. Conversely, we analyze the harmful sisu and its adverse impacts on the same domains. Our findings, including intriguing trends related to age, educational level, emotional states, and gender, pave the way for developing tailored solutions and initiatives to nurture the beneficial aspects of sisu and curtail the damaging consequences of sisu within professional settings and personal welfare. © 2024 The Authors","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85182550173"
"","","","7th International Conference on Microelectronics and Telecommunication Engineering, ICMETE 2023","2024","Lecture Notes in Networks and Systems","894","","","","","812","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189559818&partnerID=40&md5=8f565fe27ad4497ad454e5d1b6d4bf4a","The proceedings contain 66 papers. The special focus in this conference is on Microelectronics and Telecommunication Engineering. The topics include: OpenFace Tracker and GoogleNet: To Track and Detect Emotional States for People with Asperger Syndrome; vehicle Classification and License Number Plate Detection Using Deep Learning; Car Price Prediction Model Using ML; effects of Material Deformation on U-shaped Optical Fiber Sensor; Classification of DNA Sequence for Diabetes Mellitus Type Using Machine Learning Methods; unveiling the Future: A Review of Financial Fraud Detection Using Artificial Intelligence Techniques; remodeling E-Commerce Through Decentralization: A Study of Trust, Security and Efficiency; estimation of Wildfire Conditions via Perimeter and Surface Area Optimization Using Convolutional Neural Network; a Framework Provides Authorized Personnel with Secure Access to Their Electronic Health Records; explainable Artificial Intelligence for Deep Learning Models in Diagnosing Brain Tumor Disorder; pioneering a New Era of Global Transactions: Decentralized Overseas Transactions on the Blockchain; a Perspective Review of Generative Adversarial Network in Medical Image Denoising; osteoporosis Detection Based on X-Ray Using Deep Convolutional Neural Network; fault Prediction and Diagnosis of Bearing Assembly; bearing Fault Diagnosis Using Machine Learning Models; a High-Payload Image Steganography Based on Shamir’s Secret Sharing Scheme; Design and Comparison of Various Parameters of T-Shaped TFET of Variable Gate Lengths and Materials; experiment to Find Out Suitable Machine Learning Algorithm for Enzyme Subclass Classification; iris Recognition Method for Non-cooperative Images; an Exploration: Deep Learning-Based Hybrid Model for Automated Diagnosis and Classification of Brain Tumor Disorder; recognition of Apple Leaves Infection Using DenseNet121 with Additional Layers.","Conference review","Final","","Scopus","2-s2.0-85189559818"
"Aikodon N.; Ortega-Martorell S.; Olier I.","Aikodon, Nosa (58844338700); Ortega-Martorell, Sandra (36090096200); Olier, Ivan (8914762100)","58844338700; 36090096200; 8914762100","Predicting Decompensation Risk in Intensive Care Unit Patients Using Machine Learning","2024","Algorithms","17","1","6","","","","0","10.3390/a17010006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183184053&doi=10.3390%2fa17010006&partnerID=40&md5=c82e48882c832947a11d777f07ebf33c","Patients in Intensive Care Units (ICU) face the threat of decompensation, a rapid decline in health associated with a high risk of death. This study focuses on creating and evaluating machine learning (ML) models to predict decompensation risk in ICU patients. It proposes a novel approach using patient vitals and clinical data within a specified timeframe to forecast decompensation risk sequences. The study implemented and assessed long short-term memory (LSTM) and hybrid convolutional neural network (CNN)-LSTM architectures, along with traditional ML algorithms as baselines. Additionally, it introduced a novel decompensation score based on the predicted risk, validated through principal component analysis (PCA) and k-means analysis for risk stratification. The results showed that, with PPV = 0.80, NPV = 0.96 and AUC-ROC = 0.90, CNN-LSTM had the best performance when predicting decompensation risk sequences. The decompensation score’s effectiveness was also confirmed (PPV = 0.83 and NPV = 0.96). SHAP plots were generated for the overall model and two risk strata, illustrating variations in feature importance and their associations with the predicted risk. Notably, this study represents the first attempt to predict a sequence of decompensation risks rather than single events, a critical advancement given the challenge of early decompensation detection. Predicting a sequence facilitates early detection of increased decompensation risk and pace, potentially leading to saving more lives. © 2023 by the authors.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85183184053"
"Brankovic A.; Huang W.; Cook D.; Khanna S.; Bialkowski K.","Brankovic, Aida (48860972300); Huang, Wenjie (58482861700); Cook, David (15764622300); Khanna, Sankalp (54583756600); Bialkowski, Konstanty (6603836683)","48860972300; 58482861700; 15764622300; 54583756600; 6603836683","Elucidating Discrepancy in Explanations of Predictive Models Developed Using EMR","2024","Studies in Health Technology and Informatics","310","","","865","869","4","0","10.3233/SHTI231088","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183587096&doi=10.3233%2fSHTI231088&partnerID=40&md5=6a910ea82578a74b60db206ffa1e6272","The lack of transparency and explainability hinders the clinical adoption of Machine learning (ML) algorithms. While explainable artificial intelligence (XAI) methods have been proposed, little research has focused on the agreement between these methods and expert clinical knowledge. This study applies current state-of-the-art explainability methods to clinical decision support algorithms developed for Electronic Medical Records (EMR) data to analyse the concordance between these factors and discusses causes for identified discrepancies from a clinical and technical perspective. Important factors for achieving trustworthy XAI solutions for clinical decision support are also discussed.  © 2024 International Medical Informatics Association (IMIA) and IOS Press.","Conference paper","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85183587096"
"Kose U.; Sengoz N.; Chen X.; Saucedo J.A.M.","Kose, Utku (36544118500); Sengoz, Nilgun (57219639156); Chen, Xi (59092083700); Saucedo, Jose Antonio Marmolejo (59094324400)","36544118500; 57219639156; 59092083700; 59094324400","Explainable Artificial Intelligence (XAI) in healthcare","2024","Explainable Artificial Intelligence (XAI) in Healthcare","","","","1","208","207","0","10.1201/9781003426073","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192553949&doi=10.1201%2f9781003426073&partnerID=40&md5=d108d5a39de13be44cebcfbd910f18e4","This book highlights the use of explainable artificial intelligence (XAI) for healthcare problems, in order to improve trustworthiness, performance and sustainability levels in the context of applications. Explainable Artificial Intelligence (XAI) in Healthcare adopts the understanding that AI solutions should not only have high accuracy performance, but also be transparent, understandable and reliable from the end user's perspective. The book discusses the techniques, frameworks, and tools to effectively implement XAI methodologies in critical problems of healthcare field. The authors offer different types of solutions, evaluation methods and metrics for XAI and reveal how the concept of explainability finds a response in target problem coverage. The authors examine the use of XAI in disease diagnosis, medical imaging, health tourism, precision medicine and even drug discovery. They also point out the importance of user perspectives and value of the data used in target problems. Finally, the authors also ensure a well-defined future perspective for advancing XAI in terms of healthcare. This book will offer great benefits to students at the undergraduate and graduate levels and researchers. The book will also be useful for industry professionals and clinicians who perform critical decision-making tasks. © © 2024 selection and editorial matter, Utku Kose, Nilgun Sengoz, Xi Chen and Jose Antonio Marmolejo Saucedo. All rights reserved.","Book","Final","","Scopus","2-s2.0-85192553949"
"Ahmad I.; Zhu M.; Li G.; Javeed D.; Kumar P.; Chen S.","Ahmad, Ijaz (58280612400); Zhu, Mingxing (55903611500); Li, Guanglin (7407054228); Javeed, Danish (57216501078); Kumar, Prabhat (57441546500); Chen, Shixiong (35236039500)","58280612400; 55903611500; 7407054228; 57216501078; 57441546500; 35236039500","A Secure and Interpretable AI for Smart Healthcare System: A Case Study on Epilepsy Diagnosis Using EEG Signals","2024","IEEE Journal of Biomedical and Health Informatics","","","","1","12","11","0","10.1109/JBHI.2024.3366341","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188673392&doi=10.1109%2fJBHI.2024.3366341&partnerID=40&md5=b0775300f2d7b72471e9f06c1ebfd99f","The efficient patient-independent and interpretable framework for electroencephalogram (EEG) epileptic seizure detection (ESD) has informative challenges due to the complex pattern of EEG nature. Automated detection of ES is crucial, and Explainable Artificial Intelligence (XAI) is urgently needed to justify algorithmic predictions in clinical settings. Therefore, this study implements an XAI-based computer-aided ES detection system (XAI-CAESDs), comprising three major modules including of feature engineering module, a seizure detection module, and an explainable decision-making process module in a smart healthcare system. To ensure the privacy and security of biomedical EEG data, the blockchain is employed. Initially, the Butterworth filter eliminates various artifacts, and the Dual-Tree Complex Wavelet Transform (DTCWT) decomposes EEG signals, extracting real and imaginary eigenvalue features using frequency domain (FD), time domain (TD), and Fractal Dimension (FD) of linear and non-linear features. The best features are selected by using Correlation Coefficients (CC) and Distance Correlation (DC). The selected features are fed into the Stacking Ensemble Classifiers (SEC) for EEG ES detection. Further, the Shapley Additive Explanations (SHAP) method of XAI is implemented to facilitate the interpretation of predictions made by the proposed approach, enabling medical experts to make accurate and understandable decisions. The proposed ensemble-based stacking classifiers in XAI-CAESDs have demonstrated 2&#x0025; best average accuracy, Recall, specificity, and F1-score using the University of California, Irvine, Bonn University, and Boston Children&#x0027;s Hospital-MIT EEG data sets. The proposed framework enhances decision-making and the diagnosis process using biomedical EEG signals and ensures data security in smart healthcare systems. IEEE","Article","Article in press","","Scopus","2-s2.0-85188673392"
"Xiao Y.; Huo Y.; Cai J.; Gong Y.; Liang W.; Kolodziej J.","Xiao, Yufeng (58539658100); Huo, Yingzi (58184381900); Cai, Jiahong (57226277744); Gong, Yinyan (58183529700); Liang, Wei (58843978600); Kolodziej, Joanna (26435260800)","58539658100; 58184381900; 57226277744; 58183529700; 58843978600; 26435260800","ERF-XGB: An Edge-IoT-Based Explainable Model for Predictive Maintenance","2024","IEEE Transactions on Consumer Electronics","70","1","","4016","4025","9","0","10.1109/TCE.2024.3371440","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187984779&doi=10.1109%2fTCE.2024.3371440&partnerID=40&md5=8b1db9baede3680e2e5f94100bf9a52d","As the number of Internet of Things edge devices in smart factories increasing, it is crucial to predict the lifetime of the equipment to keep the production running normally. Although predictive maintenance based on machine learning achieve a better performance, they still face the challenge of black box and time-efficient. This paper propose a stacking-based model called ERF-XGB for predictive maintenance in the edge computing environment. The model first performs initial prediction by integrating the Random Forest model and Extreme Gradient Boosting model, followed by further processing of the initial prediction results using the linear regression model to obtain the final prediction results. In addition, the model incorporates the Shapley Additive exPlanations method, which can enhance the interpretability of the model when performing predictive maintenance. An experimental evaluation of the Predictive and Health Management dataset shows that the ERF-XGB model has an RMSE of 18.271 and an MAE of 13.454, which are the two metrics that perform the best when compared to other comparison models, suggesting that the model has a better predictive performance. Meanwhile, the Shapley Additive exPlanations method visualizes the impact of each edge device on normal operation in the production process, facilitating precise equipment management and maintenance.  © 1975-2011 IEEE.","Article","Final","","Scopus","2-s2.0-85187984779"
"Naser M.A.; Majeed A.A.; Alsabah M.; Al-Shaikhli T.R.; Kaky K.M.","Naser, Marwah Abdulrazzaq (57220935279); Majeed, Aso Ahmed (56385161000); Alsabah, Muntadher (57217525813); Al-Shaikhli, Taha Raad (57203802841); Kaky, Kawa M. (57190001890)","57220935279; 56385161000; 57217525813; 57203802841; 57190001890","A Review of Machine Learning’s Role in Cardiovascular Disease Prediction: Recent Advances and Future Challenges","2024","Algorithms","17","2","78","","","","1","10.3390/a17020078","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185845417&doi=10.3390%2fa17020078&partnerID=40&md5=dffdf9d506c439c8d28ed8e1682a7855","Cardiovascular disease is the leading cause of global mortality and responsible for millions of deaths annually. The mortality rate and overall consequences of cardiac disease can be reduced with early disease detection. However, conventional diagnostic methods encounter various challenges, including delayed treatment and misdiagnoses, which can impede the course of treatment and raise healthcare costs. The application of artificial intelligence (AI) techniques, especially machine learning (ML) algorithms, offers a promising pathway to address these challenges. This paper emphasizes the central role of machine learning in cardiac health and focuses on precise cardiovascular disease prediction. In particular, this paper is driven by the urgent need to fully utilize the potential of machine learning to enhance cardiovascular disease prediction. In light of the continued progress in machine learning and the growing public health implications of cardiovascular disease, this paper aims to offer a comprehensive analysis of the topic. This review paper encompasses a wide range of topics, including the types of cardiovascular disease, the significance of machine learning, feature selection, the evaluation of machine learning models, data collection & preprocessing, evaluation metrics for cardiovascular disease prediction, and the recent trends & suggestion for future works. In addition, this paper offers a holistic view of machine learning’s role in cardiovascular disease prediction and public health. We believe that our comprehensive review will contribute significantly to the existing body of knowledge in this essential area. © 2024 by the authors.","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85185845417"
"Ghosh S.K.; Khandoker A.H.","Ghosh, Samit Kumar (57197818632); Khandoker, Ahsan H. (23091108200)","57197818632; 23091108200","Investigation on explainable machine learning models to predict chronic kidney diseases","2024","Scientific Reports","14","1","3687","","","","0","10.1038/s41598-024-54375-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185094698&doi=10.1038%2fs41598-024-54375-4&partnerID=40&md5=21610ef596af0413b8b01d4711105dc8","Chronic kidney disease (CKD) is a major worldwide health problem, affecting a large proportion of the world’s population and leading to higher morbidity and death rates. The early stages of CKD sometimes present without visible symptoms, causing patients to be unaware. Early detection and treatments are critical in reducing complications and improving the overall quality of life for people afflicted. In this work, we investigate the use of an explainable artificial intelligence (XAI)-based strategy, leveraging clinical characteristics, to predict CKD. This study collected clinical data from 491 patients, comprising 56 with CKD and 435 without CKD, encompassing clinical, laboratory, and demographic variables. To develop the predictive model, five machine learning (ML) methods, namely logistic regression (LR), random forest (RF), decision tree (DT), Naïve Bayes (NB), and extreme gradient boosting (XGBoost), were employed. The optimal model was selected based on accuracy and area under the curve (AUC). Additionally, the SHAP (SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic Explanations) algorithms were utilized to demonstrate the influence of the features on the optimal model. Among the five models developed, the XGBoost model achieved the best performance with an AUC of 0.9689 and an accuracy of 93.29%. The analysis of feature importance revealed that creatinine, glycosylated hemoglobin type A1C (HgbA1C), and age were the three most influential features in the XGBoost model. The SHAP force analysis further illustrated the model’s visualization of individualized CKD predictions. For further insights into individual predictions, we also utilized the LIME algorithm. This study presents an interpretable ML-based approach for the early prediction of CKD. The SHAP and LIME methods enhance the interpretability of ML models and help clinicians better understand the rationale behind the predicted outcomes more effectively. © The Author(s) 2024.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85185094698"
"Seong H.; Lee K.-S.; Choi Y.; Na D.; Kim J.; Shin H.J.; Ahn K.H.","Seong, Hyunyoung (57216892298); Lee, Kwang-Sig (57221177656); Choi, Yumin (58904087200); Na, Donghyun (58617316000); Kim, Jaewoo (58904034600); Shin, Hyeon Ju (57415569000); Ahn, Ki Hoon (26031248300)","57216892298; 57221177656; 58904087200; 58617316000; 58904034600; 57415569000; 26031248300","Explainable artificial intelligence for predicting red blood cell transfusion in geriatric patients undergoing hip arthroplasty: Machine learning analysis using national health insurance data","2024","Medicine (United States)","103","8","","E36909","","","0","10.1097/MD.0000000000036909","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185858732&doi=10.1097%2fMD.0000000000036909&partnerID=40&md5=5ab606468df64c910b4453c474142be5","This study uses machine learning and population data to analyze major determinants of blood transfusion among patients with hip arthroplasty. Retrospective cohort data came from Korea National Health Insurance Service claims data for 19,110 patients aged 65 years or more with hip arthroplasty in 2019. The dependent variable was blood transfusion (yes vs no) in 2019 and its 31 predictors were included. Random forest variable importance and Shapley Additive Explanations were used for identifying major predictors and the directions of their associations with blood transfusion. The random forest registered the area under the curve of 73.6%. Based on random forest variable importance, the top-10 predictors were anemia (0.25), tranexamic acid (0.17), age (0.16), socioeconomic status (0.05), spinal anesthesia (0.05), general anesthesia (0.04), sex (female) (0.04), dementia (0.03), iron (0.02), and congestive heart failure (0.02). These predictors were followed by their top-20 counterparts including cardiovascular disease, statin, chronic obstructive pulmonary disease, diabetes mellitus, chronic kidney disease, peripheral vascular disease, liver disease, solid tumor, myocardial infarction and hypertension. In terms of max Shapley Additive Explanations values, these associations were positive, e.g., anemia (0.09), tranexamic acid (0.07), age (0.09), socioeconomic status (0.05), spinal anesthesia (0.05), general anesthesia (0.04), sex (female) (0.02), dementia (0.03), iron (0.04), and congestive heart failure (0.03). For example, the inclusion of anemia, age, tranexamic acid or spinal anesthesia into the random forest will increase the probability of blood transfusion among patients with hip arthroplasty by 9%, 7%, 9% or 5%. Machine learning is an effective prediction model for blood transfusion among patients with hip arthroplasty. The high-risk group with anemia, age and comorbid conditions need to be treated with tranexamic acid, iron and/or other appropriate interventions. © 2024 Lippincott Williams and Wilkins. All rights reserved.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85185858732"
"Romanić S.H.; Mendaš G.; Fingler S.; Drevenkar V.; Mustać B.; Jovanović G.","Romanić, Snježana Herceg (12778504400); Mendaš, Gordana (6506705756); Fingler, Sanja (56633633900); Drevenkar, Vlasta (6701645108); Mustać, Bosiljka (34977142800); Jovanović, Gordana (57225371381)","12778504400; 6506705756; 56633633900; 6701645108; 34977142800; 57225371381","Polychlorinated biphenyls in mussels, small pelagic fish, tuna, turtles, and dolphins from the Croatian Adriatic Sea waters: an overview of the last two decades of monitoring; [Poliklorirani bifenili u dagnjama, malim plavim ribama, tunama, kornjačama i dupinima iz voda hrvatskoga Jadrana - pregled monitoringa u protekla dva desetljeća]","2024","Arhiv za Higijenu Rada i Toksikologiju","75","1","","15","23","8","0","10.2478/aiht-2024-75-3814","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188994271&doi=10.2478%2faiht-2024-75-3814&partnerID=40&md5=a12196c416b6f9e28e69054d2f6392b2","This review summarises our two decades of polychlorinated biphenyl (PCB) monitoring in different marine organisms along the eastern Adriatic Sea. The aim was to gain an insight into the trends of PCB distribution in order to evaluate the effectiveness of past and current legislation and suggest further action. Here we mainly focus on PCB levels in wild and farmed Mediterranean mussels, wild and farmed bluefin tuna, loggerhead sea turtles, common bottlenose dolphins, and small pelagic fish. The use of artificial intelligence and advanced statistics enabled an insight into the influence of various variables on the uptake of PCBs in the investigated organisms as well as into their mutual dependence. Our findings suggest that PCBs in small pelagic fish and mussels reflect global pollution and that high levels in dolphins and wild tuna tissues raise particular concern, as they confirm their biomagnification up the food chain. Therefore, the ongoing PCB monitoring should focus on predatory species in particular to help us better understand PCB contamination in marine ecosystems in our efforts to protect the environment and human health. © 2024 Sciendo. All rights reserved.","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85188994271"
"Cummins L.; Sommers A.; Ramezani S.B.; Mittal S.; Jabour J.; Seale M.; Rahimi S.","Cummins, Logan (57477813800); Sommers, Alexander (57219549633); Ramezani, Somayeh Bakhtiari (55956312600); Mittal, Sudip (57188835015); Jabour, Joseph (57224516925); Seale, Maria (57222466119); Rahimi, Shahram (7004058733)","57477813800; 57219549633; 55956312600; 57188835015; 57224516925; 57222466119; 7004058733","Explainable Predictive Maintenance: A Survey of Current Methods, Challenges and Opportunities","2024","IEEE Access","12","","","57574","57602","28","0","10.1109/ACCESS.2024.3391130","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190784162&doi=10.1109%2fACCESS.2024.3391130&partnerID=40&md5=03722eb7c193f02d2d92b979b5316660","Predictive maintenance is a well studied collection of techniques that aims to prolong the life of a mechanical system by using artificial intelligence and machine learning to predict the optimal time to perform maintenance. The methods allow maintainers of systems and hardware to reduce financial and time costs of upkeep. As these methods are adopted for more serious and potentially life-threatening applications, the human operators need trust the predictive system. This attracts the field of Explainable AI (XAI) to introduce explainability and interpretability into the predictive system. XAI brings methods to the field of predictive maintenance that can amplify trust in the users while maintaining well-performing systems. This survey on explainable predictive maintenance (XPM) discusses and presents the current methods of XAI as applied to predictive maintenance while following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) 2020 guidelines. We categorize the different XPM methods into groups that follow the XAI literature. Additionally, we include current challenges and a discussion on future research directions in XPM.  © 2013 IEEE.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85190784162"
"Gnanavel N.; Inparaj P.; Sritharan N.; Meedeniya D.; Yogarajah P.","Gnanavel, Nishaanthini (58893777300); Inparaj, Prathushan (59032227800); Sritharan, Niruthikka (59031540200); Meedeniya, Dulani (24779848000); Yogarajah, Pratheepan (36844934000)","58893777300; 59032227800; 59031540200; 24779848000; 36844934000","Interpretable Cervical Cell Classification: A Comparative Analysis","2024","ICARC 2024 - 4th International Conference on Advanced Research in Computing: Smart and Innovative Trends in Next Generation Computing Technologies","","","","7","12","5","0","10.1109/ICARC61713.2024.10499737","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192218089&doi=10.1109%2fICARC61713.2024.10499737&partnerID=40&md5=1ea66d9a4fa863afc52271b35079a65d","Cervical cancer is a significant global health issue, and traditional screening methods like Pap smears are labor-intensive and may miss some cases. Automation is needed, but it faces challenges in terms of interpretability and data availability. To address this, the paper proposes using Explainable Artificial Intelligence (XAI) techniques like GradCAM, GradCAM++, and LRP to improve the transparency and interpretability of a cervical cell classification model, making it a novel contribution to enhancing the trustworthiness of automated cervical cancer detection. Using the Herlev Dataset, we employ data pre-processing, data augmentation techniques and develop a binary classification model, achieving a 91.94% accuracy with VGG16. The qualitative analysis of XAI methods confirmed that the model relied on nucleus and cytoplasm features, key indicators of malignancy. The least mean image entropy of 2.4849 and steep prediction confidence drop with perturbations quantitatively proved Layer-wise Relevance Propagation (LRP) to be the most effective XAI technique for cervical cell classification.  © 2024 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85192218089"
"Kolozali S.; White S.L.; Norris S.; Fasli M.; Van Heerden A.","Kolozali, Sefki (49961785700); White, Sara L. (57051934800); Norris, Shane (7103213613); Fasli, Maria (8933789200); Van Heerden, Alastair (35243623700)","49961785700; 57051934800; 7103213613; 8933789200; 35243623700","Explainable Early Prediction of Gestational Diabetes Biomarkers by Combining Medical Background and Wearable Devices: A Pilot Study With a Cohort Group in South Africa","2024","IEEE Journal of Biomedical and Health Informatics","28","4","","1860","1871","11","1","10.1109/JBHI.2024.3361505","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187312153&doi=10.1109%2fJBHI.2024.3361505&partnerID=40&md5=8f836ab59a6218dd278c2638d80c597c","This study aims to explore the potential of Internet of Things (IoT) devices and explainable Artificial Intelligence (AI) techniques in predicting biomarker values associated with GDM when measured 13-16 weeks prior to diagnosis. We developed a system that forecasts biomarkers such as LDL, HDL, triglycerides, cholesterol, HbA1c, and results from the Oral Glucose Tolerance Test (OGTT) including fasting glucose, 1-hour, and 2-hour post-load glucose values. These biomarker values are predicted based on sensory measurements collected around week 12 of pregnancy, including continuous glucose levels, short physical movement recordings, and medical background information. To the best of our knowledge, this is the first study to forecast GDM-associated biomarker values 13 to 16 weeks prior to the GDM screening test, using continuous glucose monitoring devices, a wristband for activity detection, and medical background data. We applied machine learning models, specifically Decision Tree and Random Forest Regressors, along with Coupled-Matrix Tensor Factorisation (CMTF) and Elastic Net techniques, examining all possible combinations of these methods across different data modalities. The results demonstrated good performance for most biomarkers. On average, the models achieved Mean Squared Error (MSE) between 0.29 and 0.42 and Mean Absolute Error (MAE) between 0.23 and 0.45 for biomarkers like HDL, LDL, cholesterol, and HbA1c. For the OGTT glucose values, the average MSE ranged from 0.95 to 2.44, and the average MAE ranged from 0.72 to 0.91. Additionally, the utilisation of CMTF with Alternating Least Squares technique yielded slightly better results (0.16 MSE and 0.07 MAE on average) compared to the well-known Elastic Net feature selection technique. While our study was conducted with a limited cohort in South Africa, our findings offer promising indications regarding the potential for predicting biomarker values in pregnant women through the integration of wearable devices and medical background data in the analysis. Nevertheless, further validation on a larger, more diverse cohort is imperative to substantiate these encouraging results.  © 2013 IEEE.","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85187312153"
"Mayrose H.; Sampathila N.; Muralidhar Bairy G.; Nayak T.; Belurkar S.; Saravu K.","Mayrose, Hilda (57356755200); Sampathila, Niranjana (56584740000); Muralidhar Bairy, G. (57197855850); Nayak, Tushar (58307214400); Belurkar, Sushma (55681709200); Saravu, Kavitha (16834027500)","57356755200; 56584740000; 57197855850; 58307214400; 55681709200; 16834027500","An Explainable Artificial Intelligence Integrated System for Automatic Detection of Dengue From Images of Blood Smears Using Transfer Learning","2024","IEEE Access","12","","","41750","41762","12","0","10.1109/ACCESS.2024.3378516","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188468334&doi=10.1109%2fACCESS.2024.3378516&partnerID=40&md5=73e88c4e4b2ff663128738a6dce3fb73","Dengue fever is a rapidly increasing mosquito-borne ailment spread by the virus DENV in the tropics and subtropics worldwide. It is a significant public health problem and accounts for many deaths globally. Implementing more effective methods that can more accurately detect dengue cases is challenging. The theme of this digital pathology-associated research is automatic dengue detection from peripheral blood smears (PBS) employing deep learning (DL) techniques. In recent years, DL has been significantly employed for automated computer-assisted diagnosis of various diseases from medical images. This paper explores pre-trained convolution neural networks (CNNs) for automatic dengue fever detection. Transfer learning (TL) is executed on three state-of-the-art CNNs - ResNet50, MobileNetV3Small, and MobileNetV3Large, to customize the models for differentiating the dengue-infected blood smears from the healthy ones. The dataset used to design and test the models contains 100x magnified dengue-infected and healthy control digital microscopic PBS images. The models are validated with a 5-fold cross-validation framework and tested on unseen data. An explainable artificial intelligence (XAI) approach, Gradient-weighted Class Activation Mapping (GradCAM), is eventually applied to the models to allow visualization of the precise regions on the smears most instrumental in making the predictions. While all three transferred pre-trained CNN models performed well (above 98% overall classification accuracy), MobileNetV3Small is the recommended model for this classification problem due to its significantly less computationally demanding characteristics. Transferred pre-trained CNN based on MobileNetV3Small yielded Accuracy, Recall, Specificity, Precision, F1 Score, and Area Under the ROC Curve (AUC) of 0.982 ± 0.011, 0.973 ± 0.027, 0.99 ± 0.013, 0.989 ± 0.015, 0.981 ± 0.012 and 0.982 ± 0.012 respectively, averaged over the five folds on the unseen dataset. Promising results show that the developed models have the potential to provide high-quality support to haematologists by expertly performing tedious, repetitive, and time-consuming tasks in hospitals and remote/low-resource settings. © 2013 IEEE.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85188468334"
"Shoeibi A.; Khodatars M.; Jafari M.; Ghassemi N.; Sadeghi D.; Moridian P.; Khadem A.; Alizadehsani R.; Hussain S.; Zare A.; Sani Z.A.; Khozeimeh F.; Nahavandi S.; Acharya U.R.; Gorriz J.M.","Shoeibi, Afshin (57193554372); Khodatars, Marjane (57219759554); Jafari, Mahboobeh (57219767874); Ghassemi, Navid (57211581674); Sadeghi, Delaram (57219763771); Moridian, Parisa (57219761470); Khadem, Ali (57027659100); Alizadehsani, Roohallah (55328861400); Hussain, Sadiq (57223086827); Zare, Assef (34881223700); Sani, Zahra Alizadeh (36060136500); Khozeimeh, Fahime (57191575180); Nahavandi, Saeid (55992860000); Acharya, U. Rajendra (7004510847); Gorriz, Juan M. (7004736801)","57193554372; 57219759554; 57219767874; 57211581674; 57219763771; 57219761470; 57027659100; 55328861400; 57223086827; 34881223700; 36060136500; 57191575180; 55992860000; 7004510847; 7004736801","Automated detection and forecasting of COVID-19 using deep learning techniques: A review","2024","Neurocomputing","577","","127317","","","","2","10.1016/j.neucom.2024.127317","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184668022&doi=10.1016%2fj.neucom.2024.127317&partnerID=40&md5=5c3334a9dc9ed4d213cad90860fe4cfb","In March 2020, the World Health Organization (WHO) declared COVID-19 a global epidemic, caused by the SARS-CoV-2 virus. Initially, COVID-19 was diagnosed using real-time reverse transcription–polymerase chain reaction (RT-PCR) tests with a turnaround time of 2–3 days. To enhance diagnostic accuracy, medical professionals use medical imaging alongside RT-PCR. A positive result on both RT-PCR and medical imaging confirms a COVID-19 diagnosis. Imaging modalities like chest X-ray (CXR), computed tomography (CT) scans, and ultrasound are widely utilized for rapid and precise COVID-19 diagnoses. However, interpreting COVID-19 from these images is time-consuming and susceptible to human error. Therefore, leveraging artificial intelligence (AI) methods, particularly deep learning (DL) models, can deliver consistent, high-performance results. Unlike conventional machine learning (ML), DL models automate all stages of feature extraction, selection, and classification. This paper presents a comprehensive review of using DL techniques for diagnosing COVID-19 from medical imaging. The introduction provides an overview of diagnosing the coronavirus using medical imaging, highlighting associated challenges. Subsequently, the paper delves into key aspects of Computer-Aided Diagnosis Systems (CADS) based on DL methods for diagnosing COVID-19, covering segmentation, classification, explainable AI (XAI), and predictive research. Additionally, it reviews the rehabilitation systems such as the Internet of Medical Things (IoMT) in the context of COVID-19. In another section, uncertainty quantification (UQ) research is showcased, focusing on DL models for the diagnosis of Covid-19. Crucial challenges and future research directions are outlined in another section. Finally, discussion and conclusion sections are also provided at the end of the paper. © 2024 Elsevier B.V.","Short survey","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85184668022"
"Nguyen V.G.; Sharma P.; Ağbulut Ü.; Le H.S.; Cao D.N.; Dzida M.; Osman S.M.; Le H.C.; Tran V.D.","Nguyen, Van Giao (57790254900); Sharma, Prabhakar (58961316700); Ağbulut, Ümit (57202959651); Le, Huu Son (57283939900); Cao, Dao Nam (57210821909); Dzida, Marek (6603168119); Osman, Sameh M. (55327003700); Le, Huu Cuong (57220183570); Tran, Viet Dung (57207245944)","57790254900; 58961316700; 57202959651; 57283939900; 57210821909; 6603168119; 55327003700; 57220183570; 57207245944","Improving the prediction of biochar production from various biomass sources through the implementation of eXplainable machine learning approaches","2024","International Journal of Green Energy","","","","","","","1","10.1080/15435075.2024.2326076","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188314940&doi=10.1080%2f15435075.2024.2326076&partnerID=40&md5=935da8238e5bac388cd0b8b97b242b66","Examining the game-changing possibilities of explainable machine learning techniques, this study explores the fast-growing area of biochar production prediction. The paper demonstrates how recent advances in sensitivity analysis methodology, optimization of training hyperparameters, and state-of-the-art ensemble techniques have greatly simplified and enhanced the forecasting of biochar output and composition from various biomass sources. The study argues that white-box models, which are more open and comprehensible, are crucial for biochar prediction in light of the increasing suspicion of black-box models. Accurate forecasts are guaranteed by these explainable AI systems, which also give detailed explanations of the mechanisms generating the outcomes. For prediction models to gain confidence and for biochar production processes to enable informed decision-making, there must be an emphasis on interpretability and openness. The paper comprehensively synthesizes the most critical features of biochar prediction by a rigorous assessment of current literature and relies on the authors’ own experience. Explainable machine learning techniques encourage ecologically responsible decision-making by improving forecast accuracy and transparency. Biochar is positioned as a crucial participant in solving global concerns connected to soil health and climate change, and this ultimately contributes to the wider aims of environmental sustainability and renewable energy consumption. © 2024 Taylor & Francis Group, LLC.","Review","Article in press","","Scopus","2-s2.0-85188314940"
"Liu Z.; Liu L.; Heidel R.E.; Zhao X.","Liu, Ziming (57222116271); Liu, Longjian (7405257119); Heidel, Robert E. (36967449200); Zhao, Xiaopeng (55705069200)","57222116271; 7405257119; 36967449200; 55705069200","Explainable AI and transformer models: Unraveling the nutritional influences on Alzheimer's disease mortality","2024","Smart Health","32","","100478","","","","0","10.1016/j.smhl.2024.100478","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189005864&doi=10.1016%2fj.smhl.2024.100478&partnerID=40&md5=2a6b51b6acb613736deeec2ded1b8b51","This pioneering study introduces the use of transformer-based machine learning models and explainable AI approaches to explore the impact of nutrition on Alzheimer's disease (AD) mortality. Using data from the Third National Health and Nutrition Examination Survey (Nhanes iii 1988 to 1994) and the NHANES III Mortality-Linked File (2019) databases, we investigate the intricate relationship between various nutritional factors and AD mortality. Our approach features a novel application of transformer models, which are then benchmarked against established methods like random forests and support vector machines. This comparison not only underscores the strengths of transformer models in handling complex medical datasets but also highlights their potential for providing deeper insights into disease progression. Key findings, such as the significant roles of Platelet distribution width in AD mortality in transformer and Serum Vitamin B12 in random forest, are enhanced by the use of Explainable Artificial Intelligence (XAI), particularly the Shapley Additive Explanations (SHAP) and the integrated gradient methods. This study serves as a vital step forward in applying advanced AI techniques to medical research, offering new perspectives in understanding and combating Alzheimer's Disease. © 2024","Article","Final","","Scopus","2-s2.0-85189005864"
"Muhire J.; Badru S.; Nakatumba-Nabende J.; Marvin G.","Muhire, Jacob (58900533400); Badru, Ssenoga (58900627900); Nakatumba-Nabende, Joyce (57197759951); Marvin, Ggaliwango (57302525500)","58900533400; 58900627900; 57197759951; 57302525500","Interpretable Drug Resistance Prediction for Patients on Anti-Retroviral Therapies (ART)","2024","Communications in Computer and Information Science","1973 CCIS","","","43","53","10","0","10.1007/978-3-031-50993-3_4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185710435&doi=10.1007%2f978-3-031-50993-3_4&partnerID=40&md5=b1e0d908fcc24df25f8b1783bdec7d9e","The challenge of eliminating HIV transmission is a critical and complex under taking, particularly in Africa, where countries like Uganda are grappling with a staggering 1.6 million people living with the disease. The virus’s fast pace of mutation is one of the main challenges in this battle, which often leads to the development of drug resistance and makes it difficult to provide effective treatment through AntiRetroviral Therapies (ART). By leveraging the latest innovations in Smart Technologies and Systems, such as Machine Learning, Artificial Intelligence, and Deep Learning, we can create novel approaches to tackle this issue. We presented a model that predicts which HIV patients are likely to develop drug resistance using viral load laboratory test data and machine learning algorithms. On the remaining 30% of the data, we tested our algorithms after painstakingly training and validating them on the previous 70%. Our findings were remarkable: the Decision Tree algorithm outperformed four other comparative algorithms with an f1 scoring mean of 0.9949, greatly improving our ability to identify drug resistance in HIV patients. Our research highlights the potential of combining data from viral load tests with machine learning techniques to identify patients who are likely to develop treatment resistance. These findings are a significant step forward in our ongoing fight against HIV, and we are confident that they will pave the way for new, innovative solutions to address this global health crisis. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.","Conference paper","Final","","Scopus","2-s2.0-85185710435"
"Njei B.; Osta E.; Njei N.; Al-Ajlouni Y.A.; Lim J.K.","Njei, Basile (55615163600); Osta, Eri (58764286200); Njei, Nelvis (58986016500); Al-Ajlouni, Yazan A. (57192869693); Lim, Joseph K. (7403453977)","55615163600; 58764286200; 58986016500; 57192869693; 7403453977","An explainable machine learning model for prediction of high-risk nonalcoholic steatohepatitis","2024","Scientific Reports","14","1","8589","","","","0","10.1038/s41598-024-59183-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190393377&doi=10.1038%2fs41598-024-59183-4&partnerID=40&md5=ead05e2c44a9dcd46cec1751f944130d","Early identification of high-risk metabolic dysfunction-associated steatohepatitis (MASH) can offer patients access to novel therapeutic options and potentially decrease the risk of progression to cirrhosis. This study aimed to develop an explainable machine learning model for high-risk MASH prediction and compare its performance with well-established biomarkers. Data were derived from the National Health and Nutrition Examination Surveys (NHANES) 2017-March 2020, which included a total of 5281 adults with valid elastography measurements. We used a FAST score ≥ 0.35, calculated using liver stiffness measurement and controlled attenuation parameter values and aspartate aminotransferase levels, to identify individuals with high-risk MASH. We developed an ensemble-based machine learning XGBoost model to detect high-risk MASH and explored the model’s interpretability using an explainable artificial intelligence SHAP method. The prevalence of high-risk MASH was 6.9%. Our XGBoost model achieved a high level of sensitivity (0.82), specificity (0.91), accuracy (0.90), and AUC (0.95) for identifying high-risk MASH. Our model demonstrated a superior ability to predict high-risk MASH vs. FIB-4, APRI, BARD, and MASLD fibrosis scores (AUC of 0.95 vs. 0.50, 0.50, 0.49 and 0.50, respectively). To explain the high performance of our model, we found that the top 5 predictors of high-risk MASH were ALT, GGT, platelet count, waist circumference, and age. We used an explainable ML approach to develop a clinically applicable model that outperforms commonly used clinical risk indices and could increase the identification of high-risk MASH patients in resource-limited settings. © This is a U.S. Government work and not under copyright protection in the US; foreign copyright protection may apply 2024.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85190393377"
"Shakhgeldyan K.J.; Gribova V.V.; Geltser B.I.; Shalfeyeva E.A.; Potapenko B.V.","Shakhgeldyan, Karina J. (41961853900); Gribova, Valeriya V. (7801667631); Geltser, Boris I. (6602487479); Shalfeyeva, Elena A. (57940272600); Potapenko, Bogdan V. (58527803700)","41961853900; 7801667631; 6602487479; 57940272600; 58527803700","Hybrid Clinical Decision Support for Cardiology: Architectural Foundations for Integrations","2024","Proceedings of SPIE - The International Society for Optical Engineering","13074","","130740M","","","","0","10.1117/12.3023751","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191416893&doi=10.1117%2f12.3023751&partnerID=40&md5=d803fb128af1c1f3c08672fcc1890091","Cardiovascular diseases (CVD) are the leading cause of death in most countries around the world, making the accurate assessment of risks and the selection of individual preventive strategies a current focus in healthcare. In this article, the authors presented a prototype of a Clinical Decision Support System (CDSS) for predicting and preventing cardiovascular risks based on a hybrid architecture that integrates machine learning models and ontological knowledge bases. A microservice architecture based on the Cloud-Edge approach is proposed for optimizing computational resources when processing tabular data, signals, video, and images, as well as for enhancing the effectiveness of integration with various Healthcare Information Systems (HIS). The CDSS supports the formalization not only of medical history data and results of studies but also the rules for interpreting the results of predictions based on machine learning models and methods of explainable artificial intelligence (XAI). The developed CDSS includes widely used tools in clinical cardiology and cardiothoracic surgery for risk assessment, as well as proprietary machine learning models for predicting in-hospital mortality, and others. These models contribute to making informed medical decisions for the diagnosis, prevention, and treatment of CVD. The prototype was implemented at the Medical Center of the Far Eastern Federal University and integrated with the ""1C"" HIS. The experience of implementing the prototype demonstrated the high potential of hybrid CDSS based on microservice architecture for use in clinical practice. © 2024 SPIE. All rights reserved.","Conference paper","Final","","Scopus","2-s2.0-85191416893"
"Robinsha S.D.; Amutha B.","Robinsha, S. Delsi (59012001400); Amutha, B. (26639006000)","59012001400; 26639006000","Velocious: A Resilient IoT Architecture for 6G Based Intelligent Transportation System with Expeditious Movement Mechanism","2024","Wireless Personal Communications","","","","","","","0","10.1007/s11277-024-11072-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191956905&doi=10.1007%2fs11277-024-11072-9&partnerID=40&md5=2dbbaeb31b81e121237eadb33d2e9295","The Internet of Things [IoT] has provided fascinating solutions in various fields. As the significant usages of IoT in smart cities incorporate almost all the domains such as healthcare, Vehicle to Vehicle (V2V), Energy sector, home automation, pollution monitoring, garbage collection, gardening, environment, road and rail transportation, education, public and private sector organization, etc., the need for developing an intelligent IoT architecture for every domain is a paramount criterion. In this research, we are interested in designing and developing, an IoT architecture, Velocious, which will provide a fast and speedy transportation system without delay and with a greater obstacle avoidance mechanism. The integration of sensors and actuators with a cognitive level of thinking based on circumstances leads to decisions with reasoning using the explainable Artificial Intelligence (AI) concept. The procedure of data acquisition, data transferring, and data interpretation requires a federated learning concept with context awareness where the location, time, and situation act as the prime context in decision-making. It can be purported that the development of a smart city relies on different sectors, and consequently the development of an Intelligent Transport System (ITS), highly relies on expansion. Development, penetration, and growth of IoT technologies with seamless integration with other domain architectures too. To lead smart and safe travel with fuel consumption, obstacle avoidance, and traffic management decisions are mandated with explainable Artificial Intelligence (AI) which could be built in Velocious. This research provides Velocious to be applicable to incorporate integrating all the necessary information required for setting up decisions and making rules. These types of rules can also be viewed as a rule set with proper explanations in decision-making in critical circumstances. Meanwhile, the developed Velocious IoT architecture can also be integrated seamlessly with other IoT architectures for smart Energy, smart health care, etc., for the development of smart cities. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.","Article","Article in press","","Scopus","2-s2.0-85191956905"
"Strobl E.V.","Strobl, Eric V. (57205091692)","57205091692","Counterfactual formulation of patient-specific root causes of disease","2024","Journal of Biomedical Informatics","150","","104585","","","","1","10.1016/j.jbi.2024.104585","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183626678&doi=10.1016%2fj.jbi.2024.104585&partnerID=40&md5=43b45a4d60719b60be703e44c4193556","Objective: Root causes of disease intuitively correspond to root vertices of a causal model that increase the likelihood of a diagnosis. This description of a root cause nevertheless lacks the rigorous mathematical formulation needed for the development of computer algorithms designed to automatically detect root causes from data. We seek a definition of patient-specific root causes of disease that models the intuitive procedure routinely utilized by physicians to uncover root causes in the clinic. Methods: We use structural equation models, interventional counterfactuals and the recently developed mathematical formalization of backtracking counterfactuals to propose a counterfactual formulation of patient-specific root causes of disease matching clinical intuition. Results: We introduce a definition of patient-specific root causes of disease that climbs to the third rung of Pearl's Ladder of Causation and matches clinical intuition given factual patient data and a working causal model. We then show how to assign a root causal contribution score to each variable using Shapley values from explainable artificial intelligence. Conclusion: The proposed counterfactual formulation of patient-specific root causes of disease accounts for noisy labels, adapts to disease prevalence and admits fast computation without the need for counterfactual simulation. © 2024 Elsevier Inc.","Article","Final","","Scopus","2-s2.0-85183626678"
"Ahmed F.; Abbas S.; Athar A.; Shahzad T.; Khan W.A.; Alharbi M.; Khan M.A.; Ahmed A.","Ahmed, Fahad (57208179916); Abbas, Sagheer (57202833192); Athar, Atifa (57205156055); Shahzad, Tariq (57211482169); Khan, Wasim Ahmad (57221311084); Alharbi, Meshal (57483830000); Khan, Muhammad Adnan (57226797214); Ahmed, Arfan (57224998577)","57208179916; 57202833192; 57205156055; 57211482169; 57221311084; 57483830000; 57226797214; 57224998577","Correction to: Identification of kidney stones in KUB X-ray images using VGG16 empowered with explainable artificial intelligence (Scientific Reports, (2024), 14, 1, (6173), 10.1038/s41598-024-56478-4)","2024","Scientific Reports","14","1","10366","","","","0","10.1038/s41598-024-60554-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192217424&doi=10.1038%2fs41598-024-60554-0&partnerID=40&md5=79d092f7c01f8be1db7f29e57174ee03","Correction to: Scientific Reportshttps://doi.org/10.1038/s41598-024-56478-4, published online 14 March 2024 The original version of this Article contained an error in the Funding section. ""This research work is supported by Qatar National Library."" now reads: ""The publication of this article was funded by the Weill Cornell Medicine – Qatar Health Sciences Library."" The original Article has been corrected. © The Author(s) 2024.","Erratum","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85192217424"
"Fuster-Barceló C.; Guerrero-López A.; Camara C.; Peris-Lopez P.","Fuster-Barceló, Caterina (57818766700); Guerrero-López, Alejandro (57219693178); Camara, Carmen (56414371100); Peris-Lopez, Pedro (15020997800)","57818766700; 57219693178; 56414371100; 15020997800","Exploring the power of photoplethysmogram matrix for atrial fibrillation detection with integrated explainability","2024","Engineering Applications of Artificial Intelligence","133","","108325","","","","0","10.1016/j.engappai.2024.108325","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188805111&doi=10.1016%2fj.engappai.2024.108325&partnerID=40&md5=ea53e64cdd52d0d5d4eb71a211b37d22","Atrial Fibrillation (AF) detection is paramount for cardiovascular health due to its potential complications. In this study, we investigate the utility of Photoplethysmogram (PPG) for continuous heart rate monitoring and detection of patients with AF. Our approach centres on creating PhotoplethysmoMatrices (PPMs) and leverages Explainable Artificial Intelligence (XAI) techniques to enable accurate AF patient classification with interpretability. Our method involves transforming PPG data into multiple PPM images, which represent aligned peaks within the PPG signal, presented as heatmaps. The diagnostic architecture is a lightweight and efficient Convolutional Neural Network (CNN) combined with attention mechanisms for model transparency. Remarkably, our approach achieves a 100% classification accuracy across multiple experiments, even with variations in the number of users and training images. Furthermore, the attention module underscores the significance of peak positioning and shifting in AF patient detection. Overall, our research makes a substantial contribution to the field of AF patient classification using PPG signals. The combination of image-based preprocessing techniques and explainable architectures enhances accuracy and interpretability, promising improved diagnostic capabilities in clinical settings. © 2024 Elsevier Ltd","Article","Final","","Scopus","2-s2.0-85188805111"
"Allen B.","Allen, Ben (57853800500)","57853800500","The Promise of Explainable AI in Digital Health for Precision Medicine: A Systematic Review","2024","Journal of Personalized Medicine","14","3","277","","","","1","10.3390/jpm14030277","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188991567&doi=10.3390%2fjpm14030277&partnerID=40&md5=4c97b49e71620cb5ebd2ccb8fcb2995a","This review synthesizes the literature on explaining machine-learning models for digital health data in precision medicine. As healthcare increasingly tailors treatments to individual characteristics, the integration of artificial intelligence with digital health data becomes crucial. Leveraging a topic-modeling approach, this paper distills the key themes of 27 journal articles. We included peer-reviewed journal articles written in English, with no time constraints on the search. A Google Scholar search, conducted up to 19 September 2023, yielded 27 journal articles. Through a topic-modeling approach, the identified topics encompassed optimizing patient healthcare through data-driven medicine, predictive modeling with data and algorithms, predicting diseases with deep learning of biomedical data, and machine learning in medicine. This review delves into specific applications of explainable artificial intelligence, emphasizing its role in fostering transparency, accountability, and trust within the healthcare domain. Our review highlights the necessity for further development and validation of explanation methods to advance precision healthcare delivery. © 2024 by the author.","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85188991567"
"Islam M.T.; Ashraf K.; Hosen M.H.; Nawar S.; Asgar S.","Islam, Mohammad Tanvirul (58592992300); Ashraf, Kahakashan (59012127200); Hosen, Md.Hamid (59011807100); Nawar, Sadia (59012455100); Asgar, Safa (58930833300)","58592992300; 59012127200; 59011807100; 59012455100; 58930833300","Predictive Modeling of Anxiety Levels in Bangladeshi University Students: A Voting-Based Approach with LIME and SHAP Explanations","2024","2024 International Conference on Advances in Computing, Communication, Electrical, and Smart Systems: Innovation for Sustainability, iCACCESS 2024","","","","","","","0","10.1109/iCACCESS61735.2024.10499576","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191976131&doi=10.1109%2fiCACCESS61735.2024.10499576&partnerID=40&md5=7edffb2a6b48566406b94e95f9a07fbf","In today's developing world, anxiety is a common mental disorder among university students. In this work, we predict anxiety in university students using a voting classifier. We have applied explainable artificial intelligence (XAI), to gain a better understanding of the machine learning model, using a Google Form, the dataset was gathered from several public, private, and national universities in Bangladesh. We have compared machine learning algorithms using 20 selected features and without feature selection. By using the concept of voting, we have created a new model. In order to create our final model, we selected the best three ML algorithms based on their accuracy. The voting classifier has the highest accuracy of 96%, while F1 and Recall scores are 96%, and Precision is 97%. The LIME and SHAP models are used to explain model predictions instead of a black-box machine learning model. The study determines anxiety levels by particular observations, allowing for transparency and comprehension. The goal is to create prediction models to detect at-risk individuals and root causes of anxiety among university students, consequently reducing detrimental consequences for academic performance and well-being. © 2024 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85191976131"
"Deshmukh A.; Kallivalappil N.; D'Souza K.; Kadam C.","Deshmukh, Afif (58765296700); Kallivalappil, Neave (58765685300); D'Souza, Kyle (58765363000); Kadam, Chinmay (58765750200)","58765296700; 58765685300; 58765363000; 58765750200","AL-XAI-MERS: Unveiling Alzheimer's Mysteries with Explainable AI","2024","2nd International Conference on Emerging Trends in Information Technology and Engineering, ic-ETITE 2024","","","","","","","0","10.1109/ic-ETITE58242.2024.10493489","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192553286&doi=10.1109%2fic-ETITE58242.2024.10493489&partnerID=40&md5=5d88b3cda4427e4df8453f01bcda67b6","Alzheimer's disease poses an escalating global health challenge, necessitating accurate and timely diagnosis for effective intervention. This study presents a novel approach to Alzheimer's detection utilising advanced machine learning techniques applied to brain MRI scans. Leveraging Explainable Artificial Intelligence (XAI) methods, the developed model not only detects Alzheimer's disease but also offers transparent insights into the intricate patterns within the MRI data. In an era where Alzheimer's prevalence is rising, our methodology provides a valuable tool for clinicians and patients. By employing XAI, individuals can gain a comprehensive understanding of their MRI results, enabling them to seek second opinions and fostering a deeper comprehension of their condition. This research marks a significant step towards democratising medical diagnostics, empowering individuals with knowledge and promoting informed decision-making in Alzheimer's diagnosis and management.  © 2024 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85192553286"
"Narteni S.; Baiardini I.; Braido F.; Mongelli M.","Narteni, Sara (57220574684); Baiardini, Ilaria (6603202824); Braido, Fulvio (8314050300); Mongelli, Maurizio (7005882346)","57220574684; 6603202824; 8314050300; 7005882346","Explainable artificial intelligence for cough-related quality of life impairment prediction in asthmatic patients","2024","PLoS ONE","19","3 MARCH","e0292980","","","","0","10.1371/journal.pone.0292980","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188248564&doi=10.1371%2fjournal.pone.0292980&partnerID=40&md5=c6a2b9764198a728fd3cb5eeffa78ae2","Explainable Artificial Intelligence (XAI) is becoming a disruptive trend in healthcare, allowing for transparency and interpretability of autonomous decision-making. In this study, we present an innovative application of a rule-based classification model to identify the main causes of chronic cough-related quality of life (QoL) impairment in a cohort of asthmatic patients. The proposed approach first involves the design of a suitable symptoms questionnaire and the subsequent analyses via XAI. Specifically, feature ranking, derived from statistically validated decision rules, helped in automatically identifying the main factors influencing an impaired QoL: pharynx/larynx and upper airways when asthma is under control, and asthma itself and digestive trait when asthma is not controlled. Moreover, the obtained if-then rules identified specific thresholds on the symptoms associated to the impaired QoL. These results, by finding priorities among symptoms, may prove helpful in supporting physicians in the choice of the most adequate diagnostic/therapeutic plan. Copyright: © 2024 Narteni et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85188248564"
"Bhushan M.; Kukreti A.; Negi A.","Bhushan, Megha (57191519623); Kukreti, Abhishek (58998699500); Negi, Arun (57196403246)","57191519623; 58998699500; 57196403246","Explainable artificial intelligence for diagnosis of cardiovascular disease","2024","Improving Security, Privacy, and Connectivity Among Telemedicine Platforms","","","","152","161","9","0","10.4018/979-8-3693-2141-6.ch007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191138096&doi=10.4018%2f979-8-3693-2141-6.ch007&partnerID=40&md5=4c795c3037c37baf313b65e2e70eeb57","Cardiovascular disease (CVD) is among the top causes of mortality in today's world; according to the World Health Organisation (WHO), 17.9 million individuals worldwide have died from this illness, leading to 31% of all fatalities. Through early detection and alteration in lifestyle, more than 80% of deaths due to CVD can be avoided. The majority of CVD cases are identified in adults; however, the risk factors for its beginning develops at a younger age. Various machine learning and deep learning algorithms have been utilized to diagnose and predict different types of CVDs, resulting in the development of sophisticated and efficient risk classification algorithms for every patient with CVD. These models incorporate explainability modalities which can improve people's comprehension of how reasoning works, increase transparency, and boost confidence in the usage of models in medical practice. It can help in optimising the frequency of doctor visits and carrying out prompt therapeutic along with preventative interventions against CVD occurrences. © 2024, IGI Global. All rights reserved.","Book chapter","Final","","Scopus","2-s2.0-85191138096"
"Tangaro S.; Lopalco G.; Sabella D.; Venerito V.; Novielli P.; Romano D.; Di Gilio A.; Palmisani J.; de Gennaro G.; Filannino P.; Latronico R.; Bellotti R.; De Angelis M.; Iannone F.","Tangaro, Sabina (8712490600); Lopalco, Giuseppe (56054615700); Sabella, Daniele (57763750000); Venerito, Vincenzo (57189029364); Novielli, Pierfrancesco (58172791300); Romano, Donato (58174292500); Di Gilio, Alessia (57217631443); Palmisani, Jolanda (46461925300); de Gennaro, Gianluigi (6701545353); Filannino, Pasquale (55648428600); Latronico, Rosanna (58914828700); Bellotti, Roberto (8419904800); De Angelis, Maria (7102614438); Iannone, Florenzo (7003309859)","8712490600; 56054615700; 57763750000; 57189029364; 58172791300; 58174292500; 57217631443; 46461925300; 6701545353; 55648428600; 58914828700; 8419904800; 7102614438; 7003309859","Unraveling the microbiome-metabolome nexus: a comprehensive study protocol for personalized management of Behçet’s disease using explainable artificial intelligence","2024","Frontiers in Microbiology","15","","1341152","","","","0","10.3389/fmicb.2024.1341152","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186451301&doi=10.3389%2ffmicb.2024.1341152&partnerID=40&md5=5955743a15202f9fcc579d410761394f","The presented study protocol outlines a comprehensive investigation into the interplay among the human microbiota, volatilome, and disease biomarkers, with a specific focus on Behçet’s disease (BD) using methods based on explainable artificial intelligence. The protocol is structured in three phases. During the initial three-month clinical study, participants will be divided into control and experimental groups. The experimental groups will receive a soluble fiber-based dietary supplement alongside standard therapy. Data collection will encompass oral and fecal microbiota, breath samples, clinical characteristics, laboratory parameters, and dietary habits. The subsequent biological data analysis will involve gas chromatography, mass spectrometry, and metagenetic analysis to examine the volatilome and microbiota composition of salivary and fecal samples. Additionally, chemical characterization of breath samples will be performed. The third phase introduces Explainable Artificial Intelligence (XAI) for the analysis of the collected data. This novel approach aims to evaluate eubiosis and dysbiosis conditions, identify markers associated with BD, dietary habits, and the supplement. Primary objectives include establishing correlations between microbiota, volatilome, phenotypic BD characteristics, and identifying patient groups with shared features. The study aims to identify taxonomic units and metabolic markers predicting clinical outcomes, assess the supplement’s impact, and investigate the relationship between dietary habits and patient outcomes. This protocol contributes to understanding the microbiome’s role in health and disease and pioneers an XAI-driven approach for personalized BD management. With 70 recruited BD patients, XAI algorithms will analyze multi-modal clinical data, potentially revolutionizing BD management and paving the way for improved patient outcomes. Copyright © 2024 Tangaro, Lopalco, Sabella, Venerito, Novielli, Romano, Di Gilio, Palmisani, de Gennaro, Filannino, Latronico, Bellotti, De Angelis and Iannone.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85186451301"
"Dalla Vecchia A.; Oliboni B.; Quintarelli E.","Dalla Vecchia, Anna (58552951100); Oliboni, Barbara (6507489888); Quintarelli, Elisa (6602528586)","58552951100; 6507489888; 6602528586","ICARE: the principles of Explainable AI in a Context-aware Recommendation APP","2024","CEUR Workshop Proceedings","3651","","","","","","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188525242&partnerID=40&md5=ea2973f6517339b2adff2ca2e6acdc23","In this paper, we summarize our previous contribution to the research area of Explainable Recommender Systems in the healthcare domain, called ICARE (Intuitive Context-Aware Recommender with Explanations), which is a framework based on data-mining algorithms that can provide personalized recommendations with contextual and intuitive explanations. In particular, we consider the scenario related to physical activities to improve sleep quality, and we now describe how the system satisfies the four principles of Explainable Artificial Intelligence. © 2024 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).","Conference paper","Final","","Scopus","2-s2.0-85188525242"
"Aliyeva K.; Mehdiyev N.","Aliyeva, Kamala (57192182953); Mehdiyev, Nijat (35782063300)","57192182953; 35782063300","Uncertainty-aware multi-criteria decision analysis for evaluation of explainable artificial intelligence methods: A use case from the healthcare domain","2024","Information Sciences","657","","119987","","","","0","10.1016/j.ins.2023.119987","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179889343&doi=10.1016%2fj.ins.2023.119987&partnerID=40&md5=cbd44af36b673b1937080933d40236db","This study introduces a Z-numbers-based Weighted Sum Model (WSM) tailored to evaluate user satisfaction with explanations provided by Explainable Artificial Intelligence (XAI) methods in the healthcare domain. Focusing on the interpretability of XAI, we measure how users perceive the adequacy of explanations through the lens of SHapley Additive exPlanations (SHAP), Individual Conditional Expectation (ICE) plots, and Counterfactual Explanations (CFE). By conducting interviews with healthcare professionals, we integrate their qualitative feedback with quantitative analysis to assess the effectiveness of these methods. The results present a user-centric perspective on the clarity, relevance, and trustworthiness of the generated post-hoc explanations. This study advances the fields of information sciences and healthcare by offering a systematic approach for evaluating XAI, enhancing the transparency and reliability of AI in critical decision-making processes. © 2023 Elsevier Inc.","Article","Final","","Scopus","2-s2.0-85179889343"
"Lai T.","Lai, Tin (57210806067)","57210806067","Interpretable Medical Imagery Diagnosis with Self-Attentive Transformers: A Review of Explainable AI for Health Care","2024","BioMedInformatics","4","1","","113","126","13","1","10.3390/biomedinformatics4010008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187234485&doi=10.3390%2fbiomedinformatics4010008&partnerID=40&md5=3d6b98606f8c2968a1068b0a8584eaa0","Recent advancements in artificial intelligence (AI) have facilitated its widespread adoption in primary medical services, addressing the demand–supply imbalance in healthcare. Vision Transformers (ViT) have emerged as state-of-the-art computer vision models, benefiting from self-attention modules. However, compared to traditional machine learning approaches, deep learning models are complex and are often treated as a “black box” that can cause uncertainty regarding how they operate. Explainable artificial intelligence (XAI) refers to methods that explain and interpret machine learning models’ inner workings and how they come to decisions, which is especially important in the medical domain to guide healthcare decision-making processes. This review summarizes recent ViT advancements and interpretative approaches to understanding the decision-making process of ViT, enabling transparency in medical diagnosis applications. © 2024 by the author.","Review","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85187234485"
"Nallakaruppan M.K.; Gangadevi E.; Shri M.L.; Balusamy B.; Bhattacharya S.; Selvarajan S.","Nallakaruppan, M.K. (55808557100); Gangadevi, E. (57269268800); Shri, M. Lawanya (55808602800); Balusamy, Balamurugan (58949619100); Bhattacharya, Sweta (55808089600); Selvarajan, Shitharth (58960769800)","55808557100; 57269268800; 55808602800; 58949619100; 55808089600; 58960769800","Reliable water quality prediction and parametric analysis using explainable AI models","2024","Scientific Reports","14","1","7520","","","","0","10.1038/s41598-024-56775-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188921030&doi=10.1038%2fs41598-024-56775-y&partnerID=40&md5=4d1578fdfc284f3fe7e2c0149c054d87","The consumption of water constitutes the physical health of most of the living species and hence management of its purity and quality is extremely essential as contaminated water has to potential to create adverse health and environmental consequences. This creates the dire necessity to measure, control and monitor the quality of water. The primary contaminant present in water is Total Dissolved Solids (TDS), which is hard to filter out. There are various substances apart from mere solids such as potassium, sodium, chlorides, lead, nitrate, cadmium, arsenic and other pollutants. The proposed work aims to provide the automation of water quality estimation through Artificial Intelligence and uses Explainable Artificial Intelligence (XAI) for the explanation of the most significant parameters contributing towards the potability of water and the estimation of the impurities. XAI has the transparency and justifiability as a white-box model since the Machine Learning (ML) model is black-box and unable to describe the reasoning behind the ML classification. The proposed work uses various ML models such as Logistic Regression, Support Vector Machine (SVM), Gaussian Naive Bayes, Decision Tree (DT) and Random Forest (RF) to classify whether the water is drinkable. The various representations of XAI such as force plot, test patch, summary plot, dependency plot and decision plot generated in SHAPELY explainer explain the significant features, prediction score, feature importance and justification behind the water quality estimation. The RF classifier is selected for the explanation and yields optimum Accuracy and F1-Score of 0.9999, with Precision and Re-call of 0.9997 and 0.998 respectively. Thus, the work is an exploratory analysis of the estimation and management of water quality with indicators associated with their significance. This work is an emerging research at present with a vision of addressing the water quality for the future as well. © The Author(s) 2024.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85188921030"
"Kothari S.; Sharma S.; Shejwal S.; Kazi A.; D'Silva M.; Karthikeyan M.","Kothari, Sonali (57210409873); Sharma, Shivanandana (58662833600); Shejwal, Sanskruti (58663980900); Kazi, Aqsa (58663600500); D'Silva, Michela (58663216900); Karthikeyan, M. (59076711700)","57210409873; 58662833600; 58663980900; 58663600500; 58663216900; 59076711700","An explainable AI-assisted web application in cancer drug value prediction","2024","MethodsX","12","","102696","","","","0","10.1016/j.mex.2024.102696","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189985546&doi=10.1016%2fj.mex.2024.102696&partnerID=40&md5=16bd4ae9432810a59901d27c76a90b0f","In recent years, there has been an increase in the interest in adopting Explainable Artificial Intelligence (XAI) for healthcare. The proposed system includes • An XAI model for cancer drug value prediction. The model provides data that is easy to understand and explain, which is critical for medical decision-making. It also produces accurate projections. • A model outperformed existing models due to extensive training and evaluation on a large cancer medication chemical compounds dataset. • Insights into the causation and correlation between the dependent and independent actors in the chemical composition of the cancer cell. While the model is evaluated on Lung Cancer data, the architecture offered in the proposed solution is cancer agnostic. It may be scaled out to other cancer cell data if the properties are similar. The work presents a viable route for customizing treatments and improving patient outcomes in oncology by combining XAI with a large dataset. This research attempts to create a framework where a user can upload a test case and receive forecasts with explanations, all in a portable PDF report. © 2024","Article","Final","","Scopus","2-s2.0-85189985546"
"Mastrolorito F.; Togo M.V.; Gambacorta N.; Trisciuzzi D.; Giannuzzi V.; Bonifazi F.; Liantonio A.; Imbrici P.; De Luca A.; Altomare C.D.; Ciriaco F.; Amoroso N.; Nicolotti O.","Mastrolorito, Fabrizio (58023576900); Togo, Maria Vittoria (58024578700); Gambacorta, Nicola (57211413049); Trisciuzzi, Daniela (56925495300); Giannuzzi, Viviana (56403131200); Bonifazi, Fedele (15047380400); Liantonio, Antonella (6602736095); Imbrici, Paola (6508131152); De Luca, Annamaria (57829701700); Altomare, Cosimo Damiano (7005856522); Ciriaco, Fulvio (6507486891); Amoroso, Nicola (55419832300); Nicolotti, Orazio (6603050960)","58023576900; 58024578700; 57211413049; 56925495300; 56403131200; 15047380400; 6602736095; 6508131152; 57829701700; 7005856522; 6507486891; 55419832300; 6603050960","TISBE: A Public Web Platform for the Consensus-Based Explainable Prediction of Developmental Toxicity","2024","Chemical Research in Toxicology","37","2","","323","339","16","1","10.1021/acs.chemrestox.3c00310","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182584621&doi=10.1021%2facs.chemrestox.3c00310&partnerID=40&md5=da5d347dcf83c28f66ef01d1012d5d46","Despite being extremely relevant for the protection of prenatal and neonatal health, the developmental toxicity (Dev Tox) is a highly complex endpoint whose molecular rationale is still largely unknown. The lack of availability of high-quality data as well as robust nontesting methods makes its understanding even more difficult. Thus, the application of new explainable alternative methods is of utmost importance, with Dev Tox being one of the most animal-intensive research themes of regulatory toxicology. Descending from TIRESIA (Toxicology Intelligence and Regulatory Evaluations for Scientific and Industry Applications), the present work describes TISBE (TIRESIA Improved on Structure-Based Explainability), a new public web platform implementing four fundamental advancements for in silico analyses: a three times larger dataset, a transparent XAI (explainable artificial intelligence) framework employing a fragment-based fingerprint coding, a novel consensus classifier based on five independent machine learning models, and a new applicability domain (AD) method based on a double top-down approach for better estimating the prediction reliability. The training set (TS) includes as many as 1008 chemicals annotated with experimental toxicity values. Based on a 5-fold cross-validation, a median value of 0.410 for the Matthews correlation coefficient was calculated; TISBE was very effective, with a median value of sensitivity and specificity equal to 0.984 and 0.274, respectively. TISBE was applied on two external pools made of 1484 bioactive compounds and 85 pediatric drugs taken from ChEMBL (Chemical European Molecular Biology Laboratory) and TEDDY (Task-Force in Europe for Drug Development in the Young) repositories, respectively. Notably, TISBE gives users the option to clearly spot the molecular fragments responsible for the toxicity or the safety of a given chemical query and is available for free at https://prometheus.farmacia.uniba.it/tisbe. © 2024 American Chemical Society.","Article","Final","","Scopus","2-s2.0-85182584621"
"Chatterjee S.; Mishra J.; Sundram F.; Roop P.","Chatterjee, Sobhan (58486687400); Mishra, Jyoti (9273896600); Sundram, Frederick (35727070700); Roop, Partha (6602395877)","58486687400; 9273896600; 35727070700; 6602395877","Towards Personalised Mood Prediction and Explanation for Depression from Biophysical Data","2024","Sensors","24","1","164","","","","0","10.3390/s24010164","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181890761&doi=10.3390%2fs24010164&partnerID=40&md5=9fc77df27d497bc74aabca76265896b1","Digital health applications using Artificial Intelligence (AI) are a promising opportunity to address the widening gap between available resources and mental health needs globally. Increasingly, passively acquired data from wearables are augmented with carefully selected active data from depressed individuals to develop Machine Learning (ML) models of depression based on mood scores. However, most ML models are black box in nature, and hence the outputs are not explainable. Depression is also multimodal, and the reasons for depression may vary significantly between individuals. Explainable and personalised models will thus be beneficial to clinicians to determine the main features that lead to a decline in the mood state of a depressed individual, thus enabling suitable personalised therapy. This is currently lacking. Therefore, this study presents a methodology for developing personalised and accurate Deep Learning (DL)-based predictive mood models for depression, along with novel methods for identifying the key facets that lead to the exacerbation of depressive symptoms. We illustrate our approach by using an existing multimodal dataset containing longitudinal Ecological Momentary Assessments of depression, lifestyle data from wearables and neurocognitive assessments for 14 mild to moderately depressed participants over one month. We develop classification- and regression-based DL models to predict participants’ current mood scores—a discrete score given to a participant based on the severity of their depressive symptoms. The models are trained inside eight different evolutionary-algorithm-based optimisation schemes that optimise the model parameters for a maximum predictive performance. A five-fold cross-validation scheme is used to verify the DL model’s predictive performance against 10 classical ML-based models, with a model error as low as 6% for some participants. We use the best model from the optimisation process to extract indicators, using SHAP, ALE and Anchors from explainable AI literature to explain why certain predictions are made and how they affect mood. These feature insights can assist health professionals in incorporating personalised interventions into a depressed individual’s treatment regimen. © 2023 by the authors.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85181890761"
"Akour I.; Nuseir M.T.; Alshurideh M.T.; Alzoubi H.M.; Al Kurdi B.; AlHamad A.Q.M.","Akour, Iman (6504754448); Nuseir, Mohammed T. (26638922500); Alshurideh, Muhammad Turki (55241771400); Alzoubi, Haitham M. (57195353015); Al Kurdi, Barween (57326040900); AlHamad, Ahmad Qasim Mohammad (55576292700)","6504754448; 26638922500; 55241771400; 57195353015; 57326040900; 55576292700","Explainable Artificial Intelligence (EAI) Based Disease Prediction Model","2024","Studies in Big Data","117","","","207","221","14","0","10.1007/978-3-031-31801-6_12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182860313&doi=10.1007%2f978-3-031-31801-6_12&partnerID=40&md5=e8fabf35153baf6b5e92b3db6f969046","Early disease prediction is a critical area of focus in healthcare. Identifying diseases at an early stage can increase the chances of successful treatment and reduce healthcare costs. Artificial Intelligence (AI) techniques like NLP, speech recognition, and machine vision can be used to predict and diagnose diseases. However, traditional AI methods can be error-prone. Explainable AI (EAI) techniques can reduce detection errors and improve prediction accuracy. This study proposes an EAI model for disease prediction using eSHAP. ESHAP can explain how a model arrives at a prediction, making it easier to understand and validate. The proposed model may provide better performance in accurate disease prediction. AI and EAI techniques have significant potential to revolutionize disease prediction, early detection, and treatment, ultimately leading to improved health outcomes for patients. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.","Book chapter","Final","","Scopus","2-s2.0-85182860313"
"Lee S.; Lee J.; Park J.; Park J.; Kim D.; Lee J.; Oh J.","Lee, Siryeol (57448873100); Lee, Juncheol (57093128600); Park, Juntae (58777620700); Park, Jiwoo (57225103837); Kim, Dohoon (58777028000); Lee, Joohyun (55689930700); Oh, Jaehoon (53878311700)","57448873100; 57093128600; 58777620700; 57225103837; 58777028000; 55689930700; 53878311700","Deep learning-based natural language processing for detecting medical symptoms and histories in emergency patient triage","2024","American Journal of Emergency Medicine","77","","","29","38","9","1","10.1016/j.ajem.2023.11.063","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180481051&doi=10.1016%2fj.ajem.2023.11.063&partnerID=40&md5=5f00ad12e9f3abe8a0f4bf52e63de15f","Objective: The manual recording of electronic health records (EHRs) by clinicians in the emergency department (ED) is time-consuming and challenging. In light of recent advancements in large language models (LLMs) such as GPT and BERT, this study aimed to design and validate LLMs for automatic clinical diagnoses. The models were designed to identify 12 medical symptoms and 2 patient histories from simulated clinician–patient conversations within 6 primary symptom scenarios in emergency triage rooms. Materials and method: We developed classification models by fine-tuning BERT, a transformer-based pre-trained model. We subsequently analyzed these models using eXplainable artificial intelligence (XAI) and the Shapley additive explanation (SHAP) method. A Turing test was conducted to ascertain the reliability of the XAI results by comparing them to the outcomes of tasks performed and explained by medical workers. An emergency medicine specialist assessed the results of both XAI and the medical workers. Results: We fine-tuned four pre-trained LLMs and compared their classification performance. The KLUE-RoBERTa-based model demonstrated the highest performance (F1-score: 0.965, AUROC: 0.893) on human-transcribed script data. The XAI results using SHAP showed an average Jaccard similarity of 0.722 when compared with explanations of medical workers for 15 samples. The Turing test results revealed a small 6% gap, with XAI and medical workers receiving the mean scores of 3.327 and 3.52, respectively. Conclusion: This paper highlights the potential of LLMs for automatic EHR recording in Korean EDs. The KLUE-RoBERTa-based model demonstrated superior classification performance. Furthermore, XAI using SHAP provided reliable explanations for model outputs. The reliability of these explanations was confirmed by a Turing test. © 2023","Article","Final","","Scopus","2-s2.0-85180481051"
"Wang S.; Zhang T.; Li Z.; Hong J.","Wang, Shuo (57700156900); Zhang, Tianzhuo (58881071900); Li, Ziheng (57226662656); Hong, Jinglan (9044251300)","57700156900; 58881071900; 57226662656; 9044251300","Exploring pollutant joint effects in disease through interpretable machine learning","2024","Journal of Hazardous Materials","467","","133707","","","","1","10.1016/j.jhazmat.2024.133707","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184771986&doi=10.1016%2fj.jhazmat.2024.133707&partnerID=40&md5=0ee6e388aef55776d07a3bf19de017b2","Identifying the impact of pollutants on diseases is crucial. However, assessing the health risks posed by the interplay of multiple pollutants is challenging. This study introduces the concept of Pollutants Outcome Disease, integrating multidisciplinary knowledge and employing explainable artificial intelligence (AI) to explore the joint effects of industrial pollutants on diseases. Using lung cancer as a representative case study, an extreme gradient boosting predictive model that integrates meteorological, socio-economic, pollutants, and lung cancer statistical data is developed. The joint effects of industrial pollutants on lung cancer are identified and analyzed by employing the SHAP (Shapley Additive exPlanations) interpretable machine learning technique. Results reveal substantial spatial heterogeneity in emissions from CPG and ILC, highlighting pronounced nonlinear relationships among variables. The model yielded strong predictions (an R of 0.954, an RMSE of 4283, and an R2 of 0.911) and emphasized the impact of pollutant emission amounts on lung cancer responses. Diverse joint effects patterns were observed, varying in terms of patterns, regions (frequency), and the extent of antagonistic and synergistic effects among pollutants. The study provides a new perspective for exploring the joint effects of pollutants on diseases and demonstrates the potential of AI technology to assist scientific discovery. © 2024 Elsevier B.V.","Article","Final","","Scopus","2-s2.0-85184771986"
"Islam M.R.; Bataineh A.S.; Zulkernine M.","Islam, Mohammad Rafsun (58973588500); Bataineh, Ahmed Saleh (57189468771); Zulkernine, Mohammad (6506457317)","58973588500; 57189468771; 6506457317","Detection of Cyberbullying in Social Media Texts Using Explainable Artificial Intelligence","2024","Communications in Computer and Information Science","2034 CCIS","","","319","334","15","0","10.1007/978-981-97-1274-8_21","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189620841&doi=10.1007%2f978-981-97-1274-8_21&partnerID=40&md5=240c4ba2f4a0716139819e5dd2738fe4","The widespread use of social media has opened the door to new forms of harassment and abuse, such as cyberbullying, that have a serious impact on individuals’ psychological health. Therefore, research communities have recently developed detection approaches using Natural Language Processing (NLP) combined with machine learning algorithms to identify instances of cyberbullying in social media texts. However, they are unable to determine the type of cyberbullying and the reasons why victims may be targeted. This paper develops a novel detection approach that can identify the type of cyberbullying based on characteristics such as gender, religion, age, and ethnicity, even if the original records in the training dataset do not include such information or features. This paper has accomplished this objective by utilizing Explainable Artificial Intelligence (XAI) technology alongside machine learning models to justify and explain the classification of text as cyberbullying. Technically speaking, XAI technology enables machine learning models to capture and highlight the most influential words that affect the decision to classify a text as cyberbullying. Those influential words are utilized to re-label and update the training data. The machine learning models are then re-trained using the updated data. To evaluate the performance of the proposed approach, a simulation experiment has been conducted on a large dataset containing texts from Twitter. Simulation results show that XAI technology provides convincing explanations for classifying a text as cyberbullying. It also enables machine learning models to identify various types of cyberbullying and enhances their performance in terms of classification accuracy. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024.","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85189620841"
"Kaur I.; Kamini; Kaur J.; Gagandeep; Singh S.P.; Gupta U.","Kaur, Inderpreet (57202624501); Kamini (55561775100); Kaur, Jaskirat (57207333853); Gagandeep (57226786061); Singh, Simar Preet (56647808100); Gupta, Umesh (56973565300)","57202624501; 55561775100; 57207333853; 57226786061; 56647808100; 56973565300","Enhancing explainability in predicting mental health disorders using human–machine interaction","2024","Multimedia Tools and Applications","","","","","","","0","10.1007/s11042-024-18346-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185906248&doi=10.1007%2fs11042-024-18346-1&partnerID=40&md5=524341776ae140a1da3a39387c96c562","Mental health measures an individual's emotional, psychological, and social well-being. It influences how a person thinks, feels, and responds to events. Mental illness has wreaked havoc on society in today's globe and has come to the forefront as a serious concern. People with mental disorders, including bipolar disorder, schizoaffective disorder, sadness, anxiety, and others, rarely recognize their condition as the world's most serious problem. In mental illness, there are a variety of emotional and physical symptoms. Anxiety attacks, sweating, palpitations, grief, worry, overthinking, delusions, and illusions are all symptoms of mental illness, and each symptom indicates the kind of mental disorder. Our study outlined the standardized approach for diagnostic depression, including data extraction, pre-processing, ML classifier training, identification classification, and performance assessment that enhances human–machine interaction. This study utilized five machine learning methods: k-nearest neighbor, linear regression, gaussian classifier, random forest, decision tree, and logistic regression. The accuracy, precision, recall, and F1-score metrics are used to evaluate the efficacy of machine learning models. The algorithms are categorised according to their accuracy, and explainability shows that the Gaussian classifier (Minmax scaler), which reaches 91 per cent accuracy, is the most accurate. Furthermore, given that the characteristics are predicated on potential indications of depression, the approach is capable of producing substantial justifications for the determination via machine learning models employing the SHapley Additive Explanations (SHAP) and Local Interpretable Model-Agnostic Explanations (LIME) algorithms of explainable Artificial Intelligence (XAI). Thus, the approach to predicting depression can aid in the advancement of intelligent chatbots and other technologies that improve mental health treatment. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.","Article","Article in press","","Scopus","2-s2.0-85185906248"
"Sharma R.; Mangla M.; Patil S.; Gonsalves P.; Agarwal N.","Sharma, Richa (59035361600); Mangla, Monika (35107840200); Patil, Sharvari (58094109100); Gonsalves, Priyanca (58981318900); Agarwal, Neha (58881807000)","59035361600; 35107840200; 58094109100; 58981318900; 58881807000","Lung Disease Detection from Chest X-Ray Using GANs","2024","2nd International Conference on Intelligent Data Communication Technologies and Internet of Things, IDCIoT 2024","","","","565","572","7","0","10.1109/IDCIoT59759.2024.10467535","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190117502&doi=10.1109%2fIDCIoT59759.2024.10467535&partnerID=40&md5=5cb937f83f065d914523bde3d5a7c901","Lung diseases such as pneumonia, tuberculosis (TB), chronic obstructive pulmonary disease (COPD), and COVID-19 cause serious lung damage. According to the World Health Organization, long-Term illnesses account for upto 10.38% of the total mortality rate in India. The early and accurate diagnosis of these diseases can have a significant impact on the lives of patients, as a delayed diagnosis prevents counselling. As a result, early discovery is critical for human survival, necessitating the use of innovative techniques and cutting-edge technologies to accelerate recovery and enhance long-Term survival rates. To solve this issue, Generative Adversarial Networks (GAN) and Convolutional Neural Networks (CNN) are used to create an automated method for detecting and classifying various lung diseases. This study intends to construct a comprehensive framework to effectively recognize and differentiate various lung diseases by leveraging the capabilities of GANs and CNNs. The proposed system will deliver categorization results that will be supplemented by explanations based on Explainable AI techniques. The proposed strategy ensures transparency and provides insights into the system's decision-making process. The proposed system enables the early detection of lung diseases with minimal effort and time. The power of GANs, CNNs, and XAI can be used to improve the efficiency and accuracy of identifying lung diseases, thereby improving outcomes and long-Term survival rates.  © 2024 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85190117502"
"Bharati S.; Mondal M.R.H.; Podder P.","Bharati, Subrato (57207728258); Mondal, M. Rubaiyat Hossain (57219750977); Podder, Prajoy (56607203500)","57207728258; 57219750977; 56607203500","A Review on Explainable Artificial Intelligence for Healthcare: Why, How, and When?","2024","IEEE Transactions on Artificial Intelligence","5","4","","1429","1442","13","15","10.1109/TAI.2023.3266418","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153484384&doi=10.1109%2fTAI.2023.3266418&partnerID=40&md5=eb3ebca018bb23d86c200209fa141e9a","Artificial intelligence (AI) models are increasingly finding applications in the field of medicine. Concerns have been raised about the explainability of the decisions that are made by these AI models. In this article, we give a systematic analysis of explainable artificial intelligence (XAI), with a primary focus on models that are currently being used in the field of healthcare. The literature search is conducted following the preferred reporting items for systematic reviews and meta-analyses standards for relevant work published from 1 January 2012 to 2 February 2022. The review analyzes the prevailing trends in XAI and lays out the major directions in which research is headed. We investigate the why, how, and when of the uses of these XAI models and their implications. We present a comprehensive examination of XAI methodologies as well as an explanation of how a trustworthy AI can be derived from describing AI models for healthcare fields. The discussion of this work will contribute to the formalization of the XAI field.  © 2020 IEEE.","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85153484384"
"Duell J.; Seisenberger M.; Fan X.","Duell, Jamie (57222478271); Seisenberger, Monika (23566807400); Fan, Xiuyi (35788022100)","57222478271; 23566807400; 35788022100","A Comparison of Global Explanations Given on Electronic Health Records","2024","Lecture Notes in Networks and Systems","794","","","413","424","11","0","10.1007/978-3-031-44981-9_34","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192180873&doi=10.1007%2f978-3-031-44981-9_34&partnerID=40&md5=bf9e318cee2a12a4f51fcdc265fb7d86","Explainable Artificial Intelligence (XAI) is a cutting-edge research area within Artificial Intelligence (AI), playing a crucial role in enhancing predictions in the medical field. However, the formal evaluation of XAI methods remains insufficient. In this study, we delve into state-of-the-art XAI techniques, namely SHapley Additive Predictions (SHAP) and Local Interpretable Model-Agnostic Explanations (LIME), to provide comprehensive explanations using factual and counterfactual data generated by Diverse Counterfactual Explanations (DiCE). To compare these explanation methods effectively, we propose a novel data generative method. Our experiments encompass both factual and generative counterfactual data, enabling us to measure the level of consistency across various XAI methods. We introduce Attribution Space, a new metric for comparing feature attribution methods, alongside well-known measures like the Jaccard similarity index and the Pearson correlation coefficient. Our findings demonstrate that SHAP exhibits greater variability when explaining both factual and counterfactual datasets. Although LIME and SHAP display a strong correlation in certain scenarios, there is seldom agreement on the most significant features returned and the attribution space. This highlights the necessity of employing multiple XAI methods when analyzing Electronic Health Records. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.","Conference paper","Final","","Scopus","2-s2.0-85192180873"
"Nowakowska K.; Sakellarios A.; Kaźmierski J.; Fotiadis D.I.; Pezoulas V.C.","Nowakowska, Karina (57196066937); Sakellarios, Antonis (36476633700); Kaźmierski, Jakub (15049414300); Fotiadis, Dimitrios I. (55938920100); Pezoulas, Vasileios C. (57194013364)","57196066937; 36476633700; 15049414300; 55938920100; 57194013364","AI-Enhanced Predictive Modeling for Identifying Depression and Delirium in Cardiovascular Patients Scheduled for Cardiac Surgery","2024","Diagnostics","14","1","67","","","","1","10.3390/diagnostics14010067","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181956690&doi=10.3390%2fdiagnostics14010067&partnerID=40&md5=3f93a11ee5e6ac3055194aa3d089da12","Several studies have demonstrated a critical association between cardiovascular disease (CVD) and mental health, revealing that approximately one-third of individuals with CVD also experience depression. This comorbidity significantly increases the risk of cardiac complications and mortality, a risk that persists regardless of traditional factors. Addressing this issue, our study pioneers a straightforward, explainable, and data-driven pipeline for predicting depression in CVD patients. Methods: Our study was conducted at a cardiac surgical intensive care unit. A total of 224 participants who were scheduled for elective coronary artery bypass graft surgery (CABG) were enrolled in the study. Prior to surgery, each patient underwent psychiatric evaluation to identify major depressive disorder (MDD) based on the DSM-5 criteria. An advanced data curation workflow was applied to eliminate outliers and inconsistencies and improve data quality. An explainable AI-empowered pipeline was developed, where sophisticated machine learning techniques, including the AdaBoost, random forest, and XGBoost algorithms, were trained and tested on the curated data based on a stratified cross-validation approach. Results: Our findings identified a significant correlation between the biomarker “sRAGE” and depression (r = 0.32, p = 0.038). Among the applied models, the random forest classifier demonstrated superior accuracy in predicting depression, with notable scores in accuracy (0.62), sensitivity (0.71), specificity (0.53), and area under the curve (0.67). Conclusions: This study provides compelling evidence that depression in CVD patients, particularly those with elevated “sRAGE” levels, can be predicted with a 62% accuracy rate. Our AI-driven approach offers a promising way for early identification and intervention, potentially revolutionizing care strategies in this vulnerable population. © 2023 by the authors.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85181956690"
"Mathew J.; Chitra R.; Stephen C.; Koshy R.S.","Mathew, Joel (59045906700); Chitra, R. (57197224018); Stephen, Caleb (59045906800); Koshy, Richie Suresh (59047003100)","59045906700; 57197224018; 59045906800; 59047003100","Integration of Explainable Artificial Intelligence (XAI) in the Development of Disease Prediction and Medicine Recommendation System","2024","2024 IEEE International Conference on Interdisciplinary Approaches in Technology and Management for Social Innovation, IATMSI 2024","","","","","","","0","10.1109/IATMSI60426.2024.10503250","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192253394&doi=10.1109%2fIATMSI60426.2024.10503250&partnerID=40&md5=32aaee4c67ea2f1866818ef57c0dee4d","The contemporary healthcare landscape necessitates innovative solutions to improve transparency and understanding in medical decision-making. This paper proposes an advanced medicine recommendation system and a robust disease prediction model integrating explainable AI (XAI). Recognizing the prevalence of misinformation and the urgent need for user-friendly applications, the system empowers users to manage their health proactively. It can be extended beyond common diseases, encompassing rare diseases, and employs XAI algorithms, specifically SHAP and LIME, to enhance transparency. The system incorporates Random Forest Classifier and Decision Tree models, showcasing high accuracy and robustness. The explanation models contribute to user understanding, while performance metrics offer insights into model strengths and generalization abilities. Figures depict SHAP outputs for Decision Tree and Random Forest models, emphasizing transparency in medical predictions. This proposed system addresses critical healthcare challenges, fostering informed decision-making and user trust.  © 2024 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85192253394"
"Hervella Á.S.; Ramos L.; Rouco J.; Novo J.; Ortega M.","Hervella, Álvaro S. (57204014400); Ramos, Lucía (36171157000); Rouco, José (23475243900); Novo, Jorge (57695901400); Ortega, Marcos (24475406900)","57204014400; 36171157000; 23475243900; 57695901400; 24475406900","Explainable artificial intelligence for the automated assessment of the retinal vascular tortuosity","2024","Medical and Biological Engineering and Computing","62","3","","865","881","16","0","10.1007/s11517-023-02978-w","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178906703&doi=10.1007%2fs11517-023-02978-w&partnerID=40&md5=547f3c94fbbc5fe3e1050c5c5d53ae85","Abstract: Retinal vascular tortuosity is an excessive bending and twisting of the blood vessels in the retina that is associated with numerous health conditions. We propose a novel methodology for the automated assessment of the retinal vascular tortuosity from color fundus images. Our methodology takes into consideration several anatomical factors to weigh the importance of each individual blood vessel. First, we use deep neural networks to produce a robust extraction of the different anatomical structures. Then, the weighting coefficients that are required for the integration of the different anatomical factors are adjusted using evolutionary computation. Finally, the proposed methodology also provides visual representations that explain the contribution of each individual blood vessel to the predicted tortuosity, hence allowing us to understand the decisions of the model. We validate our proposal in a dataset of color fundus images providing a consensus ground truth as well as the annotations of five clinical experts. Our proposal outperforms previous automated methods and offers a performance that is comparable to that of the clinical experts. Therefore, our methodology demonstrates to be a viable alternative for the assessment of the retinal vascular tortuosity. This could facilitate the use of this biomarker in clinical practice and medical research. Graphical abstract: (Figure presented.) © The Author(s) 2023.","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85178906703"
"Fania A.; Monaco A.; Pantaleo E.; Maggipinto T.; Bellantuono L.; Cilli R.; Lacalamita A.; La Rocca M.; Tangaro S.; Amoroso N.; Bellotti R.","Fania, Alessandro (58549028700); Monaco, Alfonso (7201639219); Pantaleo, Ester (16245945500); Maggipinto, Tommaso (56107969600); Bellantuono, Loredana (56166549700); Cilli, Roberto (57218566813); Lacalamita, Antonio (57226689619); La Rocca, Marianna (57194093154); Tangaro, Sabina (8712490600); Amoroso, Nicola (55419832300); Bellotti, Roberto (8419904800)","58549028700; 7201639219; 16245945500; 56107969600; 56166549700; 57218566813; 57226689619; 57194093154; 8712490600; 55419832300; 8419904800","Estimation of Daily Ground Level Air Pollution in Italian Municipalities with Machine Learning Models Using Sentinel-5P and ERA5 Data","2024","Remote Sensing","16","7","1206","","","","0","10.3390/rs16071206","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190293274&doi=10.3390%2frs16071206&partnerID=40&md5=faed23f2ab17c881b667023fc5395ba4","Recent years have witnessed an increasing interest in air pollutants and their effects on human health. More generally, it has become evident how human, animal and environmental health are deeply interconnected within a One Health framework. Ground level air monitoring stations are sparse and thus have limited coverage due to high costs. Satellite and reanalysis data represent an alternative with high spatio-temporal resolution. The idea of this work is to build an Artificial Intelligence model for the estimation of surface-level daily concentrations of air pollutants over the entire Italian territory using satellite, climate reanalysis, geographical and social data. As ground truth we use data from the monitoring stations of the Regional Environmental Protection Agency (ARPA) covering the period 2019–2022 at municipal level. The analysis compares different models and applies an Explainable Artificial Intelligence approach to evaluate the role of individual features in the model. The best model reaches an average (Formula presented.) of 0.84 ± 0.01 and MAE of 5.00 ± 0.01 (Formula presented.) g/m3 across all pollutants which compare well with the body of literature. The XAI analysis highlights the pivotal role of satellite and climate reanalysis data. Our work can facilitate One Health surveys and help researchers and policy makers. © 2024 by the authors.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85190293274"
"Samaras A.-D.; Tsimara M.; Voidila S.; Papandrianos N.; Zampakis P.; Moustakidis S.; Papageorgiou E.; Kalogeropoulou C.","Samaras, Agorastos-Dimitrios (57200145284); Tsimara, Maria (8687465500); Voidila, Sofia (57220644998); Papandrianos, Nikolaos (24779749100); Zampakis, Petros (16231572800); Moustakidis, Serafeim (23969428400); Papageorgiou, Elpiniki (56429800100); Kalogeropoulou, Christina (6602188135)","57200145284; 8687465500; 57220644998; 24779749100; 16231572800; 23969428400; 56429800100; 6602188135","Explainable Classification of Patients with Primary Hyperparathyroidism Using Highly Imbalanced Clinical Data Derived from Imaging and Biochemical Procedures","2024","Applied Sciences (Switzerland)","14","5","2171","","","","0","10.3390/app14052171","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192433502&doi=10.3390%2fapp14052171&partnerID=40&md5=c552e94c0651ba39abdbe8b3fd920868","Featured Application: A computer-aided diagnosis system for parathyroid disease classification can be a valuable tool for primary health care. Medical experts can utilize such tools to pinpoint unhealthy patients accurately and early, hence decongesting the National Healthcare Service (NHS). Primary hyperthyroidism (PHPT) is a common endocrine disorder characterized by hypercalcemia and elevated parathyroid hormone (PTH) levels. The most common cause is a single parathyroid adenoma, though the rest of the cases are due to multiglandular disease [double adenoma/hyperplasia]. The main focus driving this work is to develop a computer-aided classification model relying on clinical data to classify PHPT instances and, at the same time, offer explainability for the classification process. A highly imbalanced dataset was created using biometric and clinical data from 134 patients (six total features, 20.2% multiglandular instances). The features used by the current study are age, sex, max diameter index, number of deficiencies, Wisconsin index, and the reference variable indicating the type of PHPT. State-of-the-art machine learning (ML) classification algorithms were used in order to create trained prediction models and give predicted classifications based on all features/indexes. Of the ML models considered (Support Vector Machines, CatBoost, LightGBM, and AdaBoost), LightGBM was able to procure the best performing prediction model. Given the highly imbalanced nature of the particular dataset, oversampling was opted for, so as to increase prediction robustness for both classes. The ML model’s performance was then evaluated using common metrics and stratified ten-fold validation. The significance of this work is rooted in two axes: firstly, in the incorporation of oversampling to smooth out the highly imbalanced dataset and offer good prediction accuracy for both classes, and secondly, in offering an explainability aspect to an otherwise black-box ML prediction model. The maximum achievable accuracy for adenoma is 86.9% and for multigland disease 81.5%. Summarizing the above, this study demonstrates the potential for an ML approach to improve the diagnosis of PHPT and also highlights the importance of explainable artificial intelligence (AI). © 2024 by the authors.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85192433502"
"Gupta J.; Seeja K.R.","Gupta, Jyoti (59054787100); Seeja, K.R. (25653275800)","59054787100; 25653275800","A Comparative Study and Systematic Analysis of XAI Models and their Applications in Healthcare","2024","Archives of Computational Methods in Engineering","","","","","","","0","10.1007/s11831-024-10103-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190553212&doi=10.1007%2fs11831-024-10103-9&partnerID=40&md5=dcd81080c0f1fa1e78477e59ea714372","Artificial intelligence technologies such as machine learning and deep learning employ techniques to anticipate results more effectively without human involvement. Since AI models are viewed as opaque models, their application in healthcare is still restricted. Explainable artificial intelligence (XAI) has been designed to increase the use of artificial intelligence (AI) algorithms in the healthcare sector by increasing trust in the model's predictions and explaining how they are developed. The aim of this article is to critically review, compare, and summarize existing research and to find new research possibilities of XAI for applications in healthcare. This study is conducted by finding articles related to XAI in biological and healthcare domains from the PubMed, Science Direct, and Web of Science databases using the PRISMA method. A comparative study of the state-of-the-art XAI techniques to evaluate its applications in healthcare has also been done using an experimental demonstration on the Diabetes dataset. XAI techniques, namely LIME, SHAP, PDP, and decision tree, were used to explain how various input attributes contributed to the outcome of the model. This study found that the explanations provided by these models are not easily understandable for different users of the model, like doctors and patients, and need expertise. This study found that the potential of XAI in the medical domain is high as it increases trust in the AI model. This survey will motivate the researchers to build more XAI techniques that provide user-friendly explanations, especially for the less explored areas of medical data, such as biomedical signals and biomedical text. © The Author(s) under exclusive licence to International Center for Numerical Methods in Engineering (CIMNE) 2024.","Review","Article in press","","Scopus","2-s2.0-85190553212"
"Kumar P.; Javeed D.; Kumar R.; Islam A.K.M.N.","Kumar, Prabhat (57441546500); Javeed, Danish (57216501078); Kumar, Randhir (57220843411); Islam, A.K.M Najmul (57203666754)","57441546500; 57216501078; 57220843411; 57203666754","Blockchain and explainable AI for enhanced decision making in cyber threat detection","2024","Software - Practice and Experience","","","","","","","0","10.1002/spe.3319","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185654593&doi=10.1002%2fspe.3319&partnerID=40&md5=95694c971c18d460e93783699e0080fd","Artificial Intelligence (AI) based cyber threat detection tools are widely used to process and analyze a large amount of data for improved intrusion detection performance. However, these models are often considered as black box by the cybersecurity experts due to their inability to comprehend or interpret the reasoning behind the decisions. Moreover, AI-based threat hunting is data-driven and is usually modeled using the data provided by multiple cloud vendors. This is another critical challenge, as a malicious cloud can provide false information (i.e., insider attacks) and can degrade the threat-hunting capability. In this paper, we present a blockchain-enabled eXplainable AI (XAI) for enhancing the decision-making capability of cyber threat detection in the context of Smart Healthcare Systems. Specifically, first, we use blockchain to validate and store data between multiple cloud vendors by implementing a Clique Proof-of-Authority (C-PoA) consensus. Second, a novel deep learning-based threat-hunting model is built by combining Parallel Stacked Long Short Term Memory (PSLSTM) networks with a multi-head attention mechanism for improved attack detection. The extensive experiment confirms its potential to be used as an enhanced decision support system by cybersecurity analysts. © 2024 The Authors. Software: Practice and Experience published by John Wiley & Sons Ltd.","Article","Article in press","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85185654593"
"Liu X.; Sangers T.E.; Nijsten T.; Kayser M.; Pardo L.M.; Wolvius E.B.; Roshchupkin G.V.; Wakkee M.","Liu, Xianjing (57225920936); Sangers, Tobias E. (57215932573); Nijsten, Tamar (57223020989); Kayser, Manfred (26643477000); Pardo, Luba M. (9634103400); Wolvius, Eppo B. (57201454707); Roshchupkin, Gennady V. (57190279096); Wakkee, Marlies (15137282300)","57225920936; 57215932573; 57223020989; 26643477000; 9634103400; 57201454707; 57190279096; 15137282300","Predicting skin cancer risk from facial images with an explainable artificial intelligence (XAI) based approach: a proof-of-concept study","2024","eClinicalMedicine","71","","102550","","","","0","10.1016/j.eclinm.2024.102550","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188509695&doi=10.1016%2fj.eclinm.2024.102550&partnerID=40&md5=3e8b181568539f9dce269f57d123afa5","Background: Efficient identification of individuals at high risk of skin cancer is crucial for implementing personalized screening strategies and subsequent care. While Artificial Intelligence holds promising potential for predictive analysis using image data, its application for skin cancer risk prediction utilizing facial images remains unexplored. We present a neural network-based explainable artificial intelligence (XAI) approach for skin cancer risk prediction based on 2D facial images and compare its efficacy to 18 established skin cancer risk factors using data from the Rotterdam Study. Methods: The study employed data from the Rotterdam population-based study in which both skin cancer risk factors and 2D facial images and the occurrence of skin cancer were collected from 2010 to 2018. We conducted a deep-learning survival analysis based on 2D facial images using our developed XAI approach. We subsequently compared these results with survival analysis based on skin cancer risk factors using cox proportional hazard regression. Findings: Among the 2810 participants (mean Age = 68.5 ± 9.3 years, average Follow-up = 5.0 years), 228 participants were diagnosed with skin cancer after photo acquisition. Our XAI approach achieved superior predictive accuracy based on 2D facial images (c-index = 0.72, 95% CI: 0.70–0.74), outperforming that of the known risk factors (c-index = 0.59, 95% CI 0.57–0.61). Interpretation: This proof-of-concept study underscores the high potential of harnessing facial images and a tailored XAI approach as an easily accessible alternative over known risk factors for identifying individuals at high risk of skin cancer. Funding: The Rotterdam Study is funded through unrestricted research grants from Erasmus Medical Center and Erasmus University, Rotterdam, Netherlands Organization for the Health Research and Development (ZonMw), the Research Institute for Diseases in the Elderly (RIDE), the Ministry of Education, Culture and Science, the Ministry for Health, Welfare and Sports, the European Commission (DG XII), and the Municipality of Rotterdam. G.V. Roshchupkin is supported by the ZonMw Veni grant (Veni, 549 1936320). © 2024 The Author(s)","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85188509695"
"Prasad S.S.; Deo R.C.; Salcedo-Sanz S.; Downs N.J.; Casillas-Pérez D.; Parisi A.V.","Prasad, Salvin S. (57212757541); Deo, Ravinesh C. (8630380500); Salcedo-Sanz, Sancho (12789591800); Downs, Nathan J. (35611774800); Casillas-Pérez, David (57189662377); Parisi, Alfio V. (7102752359)","57212757541; 8630380500; 12789591800; 35611774800; 57189662377; 7102752359","Enhanced joint hybrid deep neural network explainable artificial intelligence model for 1-hr ahead solar ultraviolet index prediction","2023","Computer Methods and Programs in Biomedicine","241","","107737","","","","3","10.1016/j.cmpb.2023.107737","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167616103&doi=10.1016%2fj.cmpb.2023.107737&partnerID=40&md5=4893fdff5e16dde979bb3f3d2a4f9515","Background and Objective: Exposure to solar ultraviolet (UV) radiation can cause malignant keratinocyte cancer and eye disease. Developing a user-friendly, portable, real-time solar UV alert system especially or wearable electronic mobile devices can help reduce the exposure to UV as a key measure for personal and occupational management of the UV risks. This research aims to design artificial intelligence-inspired early warning tool tailored for short-term forecasting of UV index (UVI) integrating satellite-derived and ground-based predictors for Australian hotspots receiving high UV exposures. The study further improves the trustworthiness of the newly designed tool using an explainable artificial intelligence approach. Methods: An enhanced joint hybrid explainable deep neural network model (called EJH-X-DNN) is constructed involving two phases of feature selection and hyperparameter tuning using Bayesian optimization. A comprehensive assessment of EJH-X- DNN is conducted with six other competing benchmarked models. The proposed model is explained locally and globally using robust model-agnostic explainable artificial intelligence frameworks such as Local Interpretable Model-Agnostic Explanations (LIME), Shapley additive explanations (SHAP), and permutation feature importance (PFI). Results: The newly proposed model outperformed all benchmarked models for forecasting hourly horizons UVI, with correlation coefficients of 0.900, 0.960, 0.897, and 0.913, respectively, for Darwin, Alice Springs, Townsville, and Emerald hotspots. According to the combined local and global explainable model outcomes, the site-based results indicate that antecedent lagged memory of UVI and solar zenith angle are influential features. Predictions made by EJH-X-DNN model are strongly influenced by factors such as ozone effect, cloud conditions, and precipitation. Conclusion: With its superiority and skillful interpretation, the UVI prediction system reaffirms its benefits for providing real-time UV alerts to mitigate risks of skin and eye health complications, reducing healthcare costs and contributing to outdoor exposure policy. © 2023 The Author(s)","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85167616103"
"Dalvi-Esfahani M.; Mosharaf-Dehkordi M.; Leong L.W.; Ramayah T.; Jamal Kanaan-Jebna A.M.","Dalvi-Esfahani, Mohammad (57189758024); Mosharaf-Dehkordi, Mehdi (35789320900); Leong, Lam Wai (57214352737); Ramayah, T. (57222416490); Jamal Kanaan-Jebna, Abdulkarim M. (58514656500)","57189758024; 35789320900; 57214352737; 57222416490; 58514656500","Exploring the drivers of XAI-enhanced clinical decision support systems adoption: Insights from a stimulus-organism-response perspective","2023","Technological Forecasting and Social Change","195","","122768","","","","3","10.1016/j.techfore.2023.122768","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166342121&doi=10.1016%2fj.techfore.2023.122768&partnerID=40&md5=2fe44be6cdae9b4f2b0ce598b4f856cc","The concept of Explainable Artificial Intelligence (XAI) provides a clear and comprehensible explanation for the reasoning behind a system's output, allowing users to understand the context in which it operates. In the realm of Clinical Decision Support Systems (CDSS), XAI is particularly crucial, as it helps healthcare professionals (HCPs) in their decision-making processes. Without XAI, there is a risk of over-reliance on the system's output, potentially leading to subpar results. Despite the numerous benefits that XAI-enhanced CDSS hold in the healthcare industry, there has been a limited number of studies examining their implementation and acceptance. Thus, the objective of this study was to examine the adoption of XAI-based CDSS through the Stimulus-Organism-Response model. The sample consisted of 172 HCPs from Malaysian public and private hospitals, and the research model was tested using Partial Least Squares Structural Equation Modeling (PLS-SEM) in conjunction with the bootstrapping method. The results showed a significant positive correlation between the stimulus factors of informed action, transparent interaction, and representational fidelity and the positive attitude towards XAI-based CDSS as an organism factor. Additionally, the study found that attitude was a significant predictor of the intention to adopt as a response factor. The analysis also revealed a negative and significant moderation effect of perceived performance risk on the relationship between attitude and intention, while the positive moderating effect of perceived fairness was not supported. The findings of this study have significant implications for both theoretical and practical considerations and highlight the importance of XAI in the field of Clinical Decision Support Systems. © 2023 Elsevier Inc.","Article","Final","","Scopus","2-s2.0-85166342121"
"Choi H.S.; Heo H.B.; Park S.H.","Choi, Hyeon Sik (57222145778); Heo, Hyo Beom (58191395500); Park, Seung Hwan (57191672394)","57222145778; 58191395500; 57191672394","Deep Learning Modeling for Centrifugal Pump Multi-Fault Type Classification and Analysis","2023","Transactions of the Korean Society of Mechanical Engineers, A","47","3","","263","272","9","1","10.3795/KSME-A.2023.47.3.263","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153058130&doi=10.3795%2fKSME-A.2023.47.3.263&partnerID=40&md5=711efd00ebcd6950edee1e2527709c03","With the development of industrial artificial intelligence, research to classify defect types using sensor data-based machine learning and deep learning techniques is being actively conducted. However, since most machine learning, including deep learning, cannot interpret the results due to the black box structure, an explainable artificial intelligence that can interpret the model’s decision-making is proposed to secure diagnostic model’s usability and reliability. In this study, we propose a deep learning model for fault classification and interpretation of centrifugal pumps. The vibration signal was converted into an image through continuous wavelet transformation, a time-frequency analysis technique, and after learning with a convolution neural network, gradient weighted class activation map was applied to the test results to extract and verify the characteristic frequency band. © 2023 The Korean Society of Mechanical Engineers.","Article","Final","","Scopus","2-s2.0-85153058130"
"Wani N.A.; Kumar R.; Bedi J.","Wani, Niyaz Ahmad (58660356800); Kumar, Ravinder (57225853354); Bedi, Jatin (57201504761)","58660356800; 57225853354; 57201504761","DeepXplainer: An interpretable deep learning based approach for lung cancer detection using explainable artificial intelligence","2024","Computer Methods and Programs in Biomedicine","243","","107879","","","","9","10.1016/j.cmpb.2023.107879","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174702078&doi=10.1016%2fj.cmpb.2023.107879&partnerID=40&md5=140ad34f8f7b6c3cb75b5fdda2590ca2","Background and Objective: Artificial intelligence (AI) has several uses in the healthcare industry, some of which include healthcare management, medical forecasting, practical making of decisions, and diagnosis. AI technologies have reached human-like performance, but their use is limited since they are still largely viewed as opaque black boxes. This distrust remains the primary factor for their limited real application, particularly in healthcare. As a result, there is a need for interpretable predictors that provide better predictions and also explain their predictions. Methods: This study introduces “DeepXplainer”, a new interpretable hybrid deep learning-based technique for detecting lung cancer and providing explanations of the predictions. This technique is based on a convolutional neural network and XGBoost. XGBoost is used for class label prediction after “DeepXplainer” has automatically learned the features of the input using its many convolutional layers. For providing explanations or explainability of the predictions, an explainable artificial intelligence method known as “SHAP” is implemented. Results: The open-source “Survey Lung Cancer” dataset was processed using this method. On multiple parameters, including accuracy, sensitivity, F1-score, etc., the proposed method outperformed the existing methods. The proposed method obtained an accuracy of 97.43%, a sensitivity of 98.71%, and an F1-score of 98.08. After the model has made predictions with this high degree of accuracy, each prediction is explained by implementing an explainable artificial intelligence method at both the local and global levels. Conclusions: A deep learning-based classification model for lung cancer is proposed with three primary components: one for feature learning, another for classification, and a third for providing explanations for the predictions made by the proposed hybrid (ConvXGB) model. The proposed “DeepXplainer” has been evaluated using a variety of metrics, and the results demonstrate that it outperforms the current benchmarks. Providing explanations for the predictions, the proposed approach may help doctors in detecting and treating lung cancer patients more effectively. © 2023 Elsevier B.V.","Article","Final","","Scopus","2-s2.0-85174702078"
"Carlini L.P.; Coutrin G.D.A.S.; Ferreira L.A.; Soares J.D.C.A.; Silva G.V.T.; Heiderich T.M.; Balda R.D.C.X.; Barros M.C.D.M.; Guinsburg R.; Thomaz C.E.","Carlini, Lucas Pereira (57217701010); Coutrin, Gabriel de Almeida Sá (57444054600); Ferreira, Leonardo Antunes (57444245600); Soares, Juliana do Carmo Azevedo (57376340200); Silva, Giselle Valério Teixeira (57217703483); Heiderich, Tatiany Marcondes (56458763600); Balda, Rita de Cássia Xavier (11839554200); Barros, Marina Carvalho de Moraes (55423333100); Guinsburg, Ruth (6602113035); Thomaz, Carlos Eduardo (16023624400)","57217701010; 57444054600; 57444245600; 57376340200; 57217703483; 56458763600; 11839554200; 55423333100; 6602113035; 16023624400","Human vs machine towards neonatal pain assessment: A comprehensive analysis of the facial features extracted by health professionals, parents, and convolutional neural networks","2024","Artificial Intelligence in Medicine","147","","102724","","","","1","10.1016/j.artmed.2023.102724","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178665174&doi=10.1016%2fj.artmed.2023.102724&partnerID=40&md5=8f2684850d129115a65ca6dfa7620569","Neonates are not able to verbally communicate pain, hindering the correct identification of this phenomenon. Several clinical scales have been proposed to assess pain, mainly using the facial features of the neonate, but a better comprehension of these features is yet required, since several related works have shown the subjectivity of these scales. Meanwhile, computational methods have been implemented to automate neonatal pain assessment and, although performing accurately, these methods still lack the interpretability of the corresponding decision-making processes. To address this issue, we propose in this work a facial feature extraction framework to gather information and investigate the human and machine neonatal pain assessments, comparing the visual attention of the facial features perceived by health-professionals and parents of neonates with the most relevant ones extracted by eXplainable Artificial Intelligence (XAI) methods, considering the VGG-Face and N-CNN deep learning architectures. Our experimental results show that the information extracted by the computational methods are clinically relevant to neonatal pain assessment, but yet do not agree with the facial visual attention of health-professionals and parents, suggesting that humans and machines can learn from each other to improve their decision-making processes. We believe that these findings might advance our understanding of how humans and machines code and decode neonatal facial responses to pain, enabling further improvements in clinical scales widely used in practical situations and in face-based automatic pain assessment tools as well. © 2023 Elsevier B.V.","Article","Final","","Scopus","2-s2.0-85178665174"
"Nakamura K.; Uchino E.; Sato N.; Araki A.; Terayama K.; Kojima R.; Murashita K.; Itoh K.; Mikami T.; Tamada Y.; Okuno Y.","Nakamura, Kazuki (57224096400); Uchino, Eiichiro (57194200112); Sato, Noriaki (57204976602); Araki, Ayano (57768555600); Terayama, Kei (55934935600); Kojima, Ryosuke (56806485100); Murashita, Koichi (57193679048); Itoh, Ken (26643559300); Mikami, Tatsuya (56284616500); Tamada, Yoshinori (7005752498); Okuno, Yasushi (7202193433)","57224096400; 57194200112; 57204976602; 57768555600; 55934935600; 56806485100; 57193679048; 26643559300; 56284616500; 7005752498; 7202193433","Individual health-disease phase diagrams for disease prevention based on machine learning","2023","Journal of Biomedical Informatics","144","","104448","","","","1","10.1016/j.jbi.2023.104448","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165977069&doi=10.1016%2fj.jbi.2023.104448&partnerID=40&md5=5019d659dd094dabda2bfb0a6bc590d6","Early disease detection and prevention methods based on effective interventions are gaining attention worldwide. Progress in precision medicine has revealed that substantial heterogeneity exists in health data at the individual level and that complex health factors are involved in chronic disease development. Machine-learning techniques have enabled precise personal-level disease prediction by capturing individual differences in multivariate data. However, it is challenging to identify what aspects should be improved for disease prevention based on future disease-onset prediction because of the complex relationships among multiple biomarkers. Here, we present a health-disease phase diagram (HDPD) that represents an individual's health state by visualizing the future-onset boundary values of multiple biomarkers that fluctuate early in the disease progression process. In HDPDs, future-onset predictions are represented by perturbing multiple biomarker values while accounting for dependencies among variables. We constructed HDPDs for 11 diseases using longitudinal health checkup cohort data of 3,238 individuals, comprising 3,215 measurement items and genetic data. The improvement of biomarker values to the non-onset region in HDPD remarkably prevented future disease onset in 7 out of 11 diseases. HDPDs can represent individual physiological states in the onset process and be used as intervention goals for disease prevention. © 2023 The Authors","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85165977069"
"Chadaga K.; Prabhu S.; Sampathila N.; Chadaga R.","Chadaga, Krishnaraj (57226665664); Prabhu, Srikanth (57197645702); Sampathila, Niranjana (56584740000); Chadaga, Rajagopala (57393370000)","57226665664; 57197645702; 56584740000; 57393370000","A machine learning and explainable artificial intelligence approach for predicting the efficacy of hematopoietic stem cell transplant in pediatric patients","2023","Healthcare Analytics","3","","100170","","","","4","10.1016/j.health.2023.100170","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152518067&doi=10.1016%2fj.health.2023.100170&partnerID=40&md5=192c411cd85b97230d10082fef93e112","Cancer is a fatal disease that affects people of all ages, including children. It is one of the leading causes of death worldwide. According to World Health Organization, an estimated 400,000 children develop cancer yearly. Bone marrow transplantation (BMT) is a specialized treatment for patients suffering from certain types of cancer, such as myeloma, lymphoma, leukemia, and others. It usually includes extracting healthy cells from the donor's bone marrow and replacing the existing ones in the patient's body. However, the treatment can also cause complications such as graft-versus-host disease, organ damage, stem cell failure, new cancers, and infections. In this study, we use machine learning and explainable artificial intelligence (XAI) techniques to predict the survival rate of children undergoing Hematopoietic Stem Cell Transplants. Three feature selection techniques have been utilized for feature selection: Harris Hawks optimization, salp swarm optimization, and mutual information. The final custom stacked model delivered optimal results with accuracy, precision (89%), recall (88%), f1-score (88%), area under curve (AUC) (92%), and average precision (86%). In addition, XAI techniques such as Shapley additive values (SHAP), local interpretable model-agnostic explanations (LIME), ELI5, and QLattice have been used to make the models more precise, understandable, and interpretable. According to XAI, the most important features were relapse, donor age, recipient age, and platelet recovery time. The promising results point to the potential use of artificial intelligence in understanding the effectiveness of bone marrow transplants in children. © 2023 The Author(s)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85152518067"
"Dhiman P.; Bonkra A.; Kaur A.; Gulzar Y.; Hamid Y.; Mir M.S.; Soomro A.B.; Elwasila O.","Dhiman, Pummy (57766235600); Bonkra, Anupam (57765800900); Kaur, Amandeep (57222024069); Gulzar, Yonis (57191033584); Hamid, Yasir (57191428882); Mir, Mohammad Shuaib (57194212100); Soomro, Arjumand Bano (57204077872); Elwasila, Osman (58512180900)","57766235600; 57765800900; 57222024069; 57191033584; 57191428882; 57194212100; 57204077872; 58512180900","Healthcare Trust Evolution with Explainable Artificial Intelligence: Bibliometric Analysis","2023","Information (Switzerland)","14","10","541","","","","5","10.3390/info14100541","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175068595&doi=10.3390%2finfo14100541&partnerID=40&md5=a784d174ae29d1b73b1f129ac44cc138","Recent developments in IoT, big data, fog and edge networks, and AI technologies have had a profound impact on a number of industries, including medical. The use of AI for therapeutic purposes has been hampered by its inexplicability. Explainable Artificial Intelligence (XAI), a revolutionary movement, has arisen to solve this constraint. By using decision-making and prediction outputs, XAI seeks to improve the explicability of standard AI models. In this study, we examined global developments in empirical XAI research in the medical field. The bibliometric analysis tools VOSviewer and Biblioshiny were used to examine 171 open access publications from the Scopus database (2019–2022). Our findings point to several prospects for growth in this area, notably in areas of medicine like diagnostic imaging. With 109 research articles using XAI for healthcare classification, prediction, and diagnosis, the USA leads the world in research output. With 88 citations, IEEE Access has the greatest number of publications of all the journals. Our extensive survey covers a range of XAI applications in healthcare, such as diagnosis, therapy, prevention, and palliation, and offers helpful insights for researchers who are interested in this field. This report provides a direction for future healthcare industry research endeavors. © 2023 by the authors.","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85175068595"
"Allen B.","Allen, Ben (57853800500)","57853800500","An interpretable machine learning model of cross-sectional U.S. county-level obesity prevalence using explainable artificial intelligence","2023","PLoS ONE","18","10 October","e0292341","","","","0","10.1371/journal.pone.0292341","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173651897&doi=10.1371%2fjournal.pone.0292341&partnerID=40&md5=172abbb0d90fc20bf0cbf893231999f1","Background There is considerable geographic heterogeneity in obesity prevalence across counties in the United States. Machine learning algorithms accurately predict geographic variation in obesity prevalence, but the models are often uninterpretable and viewed as a black-box. Objective The goal of this study is to extract knowledge from machine learning models for county-level variation in obesity prevalence. Methods This study shows the application of explainable artificial intelligence methods to machine learning models of cross-sectional obesity prevalence data collected from 3,142 counties in the United States. County-level features from 7 broad categories: health outcomes, health behaviors, clinical care, social and economic factors, physical environment, demographics, and severe housing conditions. Explainable methods applied to random forest prediction models include feature importance, accumulated local effects, global surrogate decision tree, and local interpretable model-agnostic explanations. Results The results show that machine learning models explained 79% of the variance in obesity prevalence, with physical inactivity, diabetes, and smoking prevalence being the most important factors in predicting obesity prevalence. Conclusions Interpretable machine learning models of health behaviors and outcomes provide substantial insight into obesity prevalence variation across counties in the United States. Copyright: © 2023 Ben Allen. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85173651897"
"Ganesh S.; Chithambaram T.; Krishnan N.R.; Vincent D.R.; Kaliappan J.; Srinivasan K.","Ganesh, Sowmiyalakshmi (58750435400); Chithambaram, Thillai (57784848400); Krishnan, Nadesh Ramu (55660384100); Vincent, Durai Raj (55808710700); Kaliappan, Jayakumar (56780261200); Srinivasan, Kathiravan (57192191217)","58750435400; 57784848400; 55660384100; 55808710700; 56780261200; 57192191217","Exploring Huntington’s Disease Diagnosis via Artificial Intelligence Models: A Comprehensive Review","2023","Diagnostics","13","23","3592","","","","0","10.3390/diagnostics13233592","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178897164&doi=10.3390%2fdiagnostics13233592&partnerID=40&md5=cf03656ebf67d07b5329851a7bbd3132","Huntington’s Disease (HD) is a devastating neurodegenerative disorder characterized by progressive motor dysfunction, cognitive impairment, and psychiatric symptoms. The early and accurate diagnosis of HD is crucial for effective intervention and patient care. This comprehensive review provides a comprehensive overview of the utilization of Artificial Intelligence (AI) powered algorithms in the diagnosis of HD. This review systematically analyses the existing literature to identify key trends, methodologies, and challenges in this emerging field. It also highlights the potential of ML and DL approaches in automating HD diagnosis through the analysis of clinical, genetic, and neuroimaging data. This review also discusses the limitations and ethical considerations associated with these models and suggests future research directions aimed at improving the early detection and management of Huntington’s disease. It also serves as a valuable resource for researchers, clinicians, and healthcare professionals interested in the intersection of machine learning and neurodegenerative disease diagnosis. © 2023 by the authors.","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85178897164"
"Oberste L.; Heinzl A.","Oberste, Luis (57224832897); Heinzl, Armin (6603170209)","57224832897; 6603170209","User-Centric Explainability in Healthcare: A Knowledge-Level Perspective of Informed Machine Learning","2023","IEEE Transactions on Artificial Intelligence","4","4","","840","857","17","2","10.1109/TAI.2022.3227225","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144767412&doi=10.1109%2fTAI.2022.3227225&partnerID=40&md5=01e9033a8a591bca702f1dbd54bf1412","Explaining increasingly complex machine learning will remain crucial to cope with risks, regulations, responsibilities, and human support in healthcare. However, extant explainable systems mostly provide explanations that mismatch clinical users' conceptions and fail their expectations to leverage validated and clinically relevant information. A key to more user-centric and satisfying explanations can be seen in combining data-driven and knowledge-based systems, i.e., to utilize prior knowledge jointly with the patterns learned from data. We conduct a structured review of knowledge-informed machine learning in healthcare. In this article, we build on a framework to characterize user knowledge and prior knowledge embodied in explanations. Specifically, we explicate the types and contexts of knowledge to examine the fit between knowledge-informed approaches and users. Our results highlight that knowledge-informed machine learning is a promising paradigm to enrich former data-driven systems, yielding explanations that can increase formal understanding, convey useful medical knowledge, and are more intuitive. Although complying with medical conception, it still needs to be investigated whether knowledge-informed explanations increase medical user acceptance and trust in clinical machine learning-based information systems. © 2020 IEEE.","Article","Final","","Scopus","2-s2.0-85144767412"
"Sivari E.; Senirkentli G.B.; Bostanci E.; Guzel M.S.; Acici K.; Asuroglu T.","Sivari, Esra (57219892926); Senirkentli, Guler Burcu (57211993553); Bostanci, Erkan (55364555800); Guzel, Mehmet Serdar (36349844700); Acici, Koray (56529555400); Asuroglu, Tunc (56780249800)","57219892926; 57211993553; 55364555800; 36349844700; 56529555400; 56780249800","Deep Learning in Diagnosis of Dental Anomalies and Diseases: A Systematic Review","2023","Diagnostics","13","15","2512","","","","4","10.3390/diagnostics13152512","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167686867&doi=10.3390%2fdiagnostics13152512&partnerID=40&md5=558a5524bc309df287bf91ca6801692d","Deep learning and diagnostic applications in oral and dental health have received significant attention recently. In this review, studies applying deep learning to diagnose anomalies and diseases in dental image material were systematically compiled, and their datasets, methodologies, test processes, explainable artificial intelligence methods, and findings were analyzed. Tests and results in studies involving human-artificial intelligence comparisons are discussed in detail to draw attention to the clinical importance of deep learning. In addition, the review critically evaluates the literature to guide and further develop future studies in this field. An extensive literature search was conducted for the 2019–May 2023 range using the Medline (PubMed) and Google Scholar databases to identify eligible articles, and 101 studies were shortlisted, including applications for diagnosing dental anomalies (n = 22) and diseases (n = 79) using deep learning for classification, object detection, and segmentation tasks. According to the results, the most commonly used task type was classification (n = 51), the most commonly used dental image material was panoramic radiographs (n = 55), and the most frequently used performance metric was sensitivity/recall/true positive rate (n = 87) and accuracy (n = 69). Dataset sizes ranged from 60 to 12,179 images. Although deep learning algorithms are used as individual or at least individualized architectures, standardized architectures such as pre-trained CNNs, Faster R-CNN, YOLO, and U-Net have been used in most studies. Few studies have used the explainable AI method (n = 22) and applied tests comparing human and artificial intelligence (n = 21). Deep learning is promising for better diagnosis and treatment planning in dentistry based on the high-performance results reported by the studies. For all that, their safety should be demonstrated using a more reproducible and comparable methodology, including tests with information about their clinical applicability, by defining a standard set of tests and performance metrics. © 2023 by the authors.","Review","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85167686867"
"Najam-ul-Lail X.; Muzammil I.; Naseer M.A.; Tabussam I.; Muzmmal S.; Muzammil A.","Najam-ul-Lail, X. (58455081200); Muzammil, Iqra (57218658443); Naseer, Muhammad Aamir (57218658765); Tabussam, Iqra (58452257400); Muzmmal, Sidra (57694915200); Muzammil, Aqsa (58451562500)","58455081200; 57218658443; 57218658765; 58452257400; 57694915200; 58451562500","Application of explainable artificial intelligence in drug discovery and drug design","2023","Explainable Artificial Intelligence for Biomedical Applications","","","","213","243","30","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163997699&partnerID=40&md5=71140ba5de6195790afa79f0e9fb15be","Drug discovery is the process of introducing a novel drug molecule into medical practice. Drug discovery is a very costly and time-consuming process, and that is why initiatives that contribute to facilitating and accelerating the drug discovery process are of major interest. Artificial intelligence is the investigation of complicated medical data utilizing powerful algorithms and software to replicate human cognition and investigate the relationships between preventive or curative interventions and health outcomes. In recent years, several artificial intelligence (AI) approaches have been effectively used for computer-assisted drug discovery like deep learning (DL), machine learning (ML), and neural networks (NNs). Explainable artificial intelligence (XAI) makes an effort to help researchers comprehend how the model came to a certain conclusion and provide reasons for why the model's response is reasonable. To make the decision-making process transparent, XAI also offers thorough explanations in addition to the mathematical models. In this chapter, we have outlined the most important artificial intelligence approaches that aid in drug discovery. We have discussed the uses, prospects, and limitations of XAI. © 2023 River Publishers.","Book chapter","Final","","Scopus","2-s2.0-85163997699"
"Tasnim N.; Al Mamun S.; Shahidul Islam M.; Kaiser M.S.; Mahmud M.","Tasnim, Nusrat (57216868201); Al Mamun, Shamim (58276770300); Shahidul Islam, Mohammad (56811281600); Kaiser, M. Shamim (56446362000); Mahmud, Mufti (35173453700)","57216868201; 58276770300; 56811281600; 56446362000; 35173453700","Explainable Mortality Prediction Model for Congestive Heart Failure with Nature-Based Feature Selection Method","2023","Applied Sciences (Switzerland)","13","10","6138","","","","6","10.3390/app13106138","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160819779&doi=10.3390%2fapp13106138&partnerID=40&md5=479a478292ebcb0b8d5b987f3ebbf547","A mortality prediction model can be a great tool to assist physicians in decision making in the intensive care unit (ICU) in order to ensure optimal allocation of ICU resources according to the patient’s health conditions. The entire world witnessed a severe ICU patient capacity crisis a few years ago during the COVID-19 pandemic. Various widely utilized machine learning (ML) models in this research field can provide poor performance due to a lack of proper feature selection. Despite the fact that nature-based algorithms in other sectors perform well for feature selection, no comparative study on the performance of nature-based algorithms in feature selection has been conducted in the ICU mortality prediction field. Therefore, in this research, a comparison of the performance of ML models with and without feature selection was performed. In addition, explainable artificial intelligence (AI) was used to examine the contribution of features to the decision-making process. Explainable AI focuses on establishing transparency and traceability for statistical black-box machine learning techniques. Explainable AI is essential in the medical industry to foster public confidence and trust in machine learning model predictions. Three nature-based algorithms, namely the flower pollination algorithm (FPA), particle swarm algorithm (PSO), and genetic algorithm (GA), were used in this study. For the classification job, the most widely used and diversified classifiers from the literature were used, including logistic regression (LR), decision tree (DT) classifier, the gradient boosting (GB) algorithm, and the random forest (RF) algorithm. The Medical Information Mart for Intensive Care III (MIMIC-III) dataset was used to collect data on heart failure patients. On the MIMIC-III dataset, it was discovered that feature selection significantly improved the performance of the described ML models. Without applying any feature selection process on the MIMIC-III heart failure patient dataset, the accuracy of the four mentioned ML models, namely LR, DT, RF, and GB was 69.9%, 82.5%, 90.6%, and 91.0%, respectively, whereas with feature selection in combination with the FPA, the accuracy increased to 71.6%, 84.8%, 92.8%, and 91.1%, respectively, for the same dataset. Again, the FPA showed the highest area under the receiver operating characteristic (AUROC) value of 83.0% with the RF algorithm among all other algorithms utilized in this study. Thus, it can be concluded that the use of feature selection with FPA has a profound impact on the outcome of ML models. Shapley additive explanation (SHAP) was used in this study to interpret the ML models. SHAP was used in this study because it offers mathematical assurances for the precision and consistency of explanations. It is trustworthy and suitable for both local and global explanations. It was found that the features that were selected by SHAP as most important were also most common with the features selected by the FPA. Therefore, we hope that this study will help physicians to predict ICU mortality for heart failure patients with a limited number of features and with high accuracy. © 2023 by the authors.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85160819779"
"Pawar V.; Patil A.; Tamboli F.; Gaikwad D.; Mali D.; Shinde A.","Pawar, Vijaykumar (57929960500); Patil, Abhinandan (57213470079); Tamboli, Firoj (54881937900); Gaikwad, Dinanath (57192830325); Mali, Dipak (57213002525); Shinde, Anilkumar (36107182000)","57929960500; 57213470079; 54881937900; 57192830325; 57213002525; 36107182000","Harnessing the Power of AI in Pharmacokinetics and Pharmacodynamics: A Comprehensive Review","2023","International Journal of Pharmaceutical Quality Assurance","14","2","","426","439","13","1","10.25258/ijpqa.14.2.31","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165982383&doi=10.25258%2fijpqa.14.2.31&partnerID=40&md5=3dda00a8b3c6fafc2d374a3c92e96250","Personalized medicine, medication discovery, and development might all benefit greatly from AI’s incorporation into pharmacokinetics and pharmacodynamics. Target identification, therapeutic effectiveness prediction, drug design optimization, obstacles, and future possibilities are all explored in this survey of AI applications in these areas. An overview of pharmacokinetics and pharmacodynamics is presented first, stressing the significance of knowing how drugs are absorbed, distributed, metabolized, and excreted and the correlation between drug concentration and pharmacological effect. The article then looks into the function of AI in target identification, exploring how machine learning algorithms and data integration may be used to discover new drug targets and enhance the design of existing ones. Classification and regression methods are also investigated for their potential use in the prediction of therapeutic efficacy using AI. Patient data, molecular interaction data, and clinical response data are just a few examples of the types of data that may be used to fuel the creation of predictive models that might assist in dosage and efficacy optimization. Metrics and procedures for validating these models are addressed to evaluate their efficacy. Additionally, de novo drug design, virtual screening, and structure-based drug design are all discussed in relation to the use of AI in optimizing drug development. The paper provides examples of how AI has been applied successfully in different settings, demonstrating its potential to hasten the drug discovery process and enhance treatment outcomes. We examine data availability, interpretability, and ethical implications as challenges and limits of AI in pharmacokinetics and pharmacodynamics. To guarantee these technologies’ proper and ethical use, we also discuss the regulatory elements and rules for applying AI in drug research. Possibilities and prospects for the use of AI in pharmacokinetics and pharmacodynamics are discussed as a conclusion to the review. It stresses the significance of regulatory standards and clinical translation, as well as the incorporation of multiomics data, deep learning methods, real-time monitoring, explainable artificial intelligence, collaborative networks, and more. © 2023, Dr. Yashwant Research Labs Pvt. Ltd.. All rights reserved.","Review","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85165982383"
"Dindorf C.; Ludwig O.; Simon S.; Becker S.; Fröhlich M.","Dindorf, Carlo (57212503893); Ludwig, Oliver (23967098400); Simon, Steven (57262017100); Becker, Stephan (57209469974); Fröhlich, Michael (7006415804)","57212503893; 23967098400; 57262017100; 57209469974; 7006415804","Machine Learning and Explainable Artificial Intelligence Using Counterfactual Explanations for Evaluating Posture Parameters","2023","Bioengineering","10","5","511","","","","2","10.3390/bioengineering10050511","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160801854&doi=10.3390%2fbioengineering10050511&partnerID=40&md5=df51c220c06f332a037ec22468288569","Postural deficits such as hyperlordosis (hollow back) or hyperkyphosis (hunchback) are relevant health issues. Diagnoses depend on the experience of the examiner and are, therefore, often subjective and prone to errors. Machine learning (ML) methods in combination with explainable artificial intelligence (XAI) tools have proven useful for providing an objective, data-based orientation. However, only a few works have considered posture parameters, leaving the potential for more human-friendly XAI interpretations still untouched. Therefore, the present work proposes an objective, data-driven ML system for medical decision support that enables especially human-friendly interpretations using counterfactual explanations (CFs). The posture data for 1151 subjects were recorded by means of stereophotogrammetry. An expert-based classification of the subjects regarding the presence of hyperlordosis or hyperkyphosis was initially performed. Using a Gaussian progress classifier, the models were trained and interpreted using CFs. The label errors were flagged and re-evaluated using confident learning. Very good classification performances for both hyperlordosis and hyperkyphosis were found, whereby the re-evaluation and correction of the test labels led to a significant improvement (MPRAUC = 0.97). A statistical evaluation showed that the CFs seemed to be plausible, in general. In the context of personalized medicine, the present study’s approach could be of importance for reducing diagnostic errors and thereby improving the individual adaptation of therapeutic measures. Likewise, it could be a basis for the development of apps for preventive posture assessment. © 2023 by the authors.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85160801854"
"Recio-Garcia J.A.; Diaz-Agudo B.; Acuaviva A.","Recio-Garcia, Juan A. (8896318000); Diaz-Agudo, Belen (6602489855); Acuaviva, Arturo (57915956600)","8896318000; 6602489855; 57915956600","Becalm: Intelligent Monitoring of Respiratory Patients","2023","IEEE Journal of Biomedical and Health Informatics","27","8","","3806","3817","11","2","10.1109/JBHI.2023.3276638","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160219881&doi=10.1109%2fJBHI.2023.3276638&partnerID=40&md5=0e4292f692b1cd2d1de8686a0ef40539","The Becalm project is an open and low-cost solution for the remote monitoring of respiratory support therapies like the ones used in COVID-19 patients. Becalm combines a decision-making system based on Case-Based Reasoning with a low-cost, non-invasive mask that enables the remote monitoring, detection, and explanation of risk situations for respiratory patients. This paper first describes the mask and the sensors that allow remote monitoring. Then, it describes the intelligent decision-making system that detects anomalies and raises early warnings. This detection is based on the comparison of cases that represent patients using a set of static variables plus the dynamic vector of the patient time series from sensors. Finally, personalized visual reports are created to explain the causes of the warning, data patterns, and patient context to the healthcare professional. To evaluate the case-based early-warning system, we use a synthetic data generator that simulates patients' clinical evolution from the physiological features and factors described in healthcare literature. This generation process has been verified with a real dataset and allows the validation of the reasoning system with noisy and incomplete data, threshold values, and life/death situations. The evaluation demonstrates promising results and good accuracy (0.91) for the proposed low-cost solution to monitor respiratory patients.  © 2013 IEEE.","Article","Final","","Scopus","2-s2.0-85160219881"
"Sosa-Espadas C.E.; Orozco-del-Castillo M.G.; Cuevas-Cuevas N.; Recio-Garcia J.A.","Sosa-Espadas, Cristian E. (58266764200); Orozco-del-Castillo, Mauricio G. (36802492900); Cuevas-Cuevas, Nora (57205404939); Recio-Garcia, Juan A. (8896318000)","58266764200; 36802492900; 57205404939; 8896318000","IREX: Iterative Refinement and Explanation of classification models for tabular datasets","2023","SoftwareX","23","","101420","","","","0","10.1016/j.softx.2023.101420","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161061622&doi=10.1016%2fj.softx.2023.101420&partnerID=40&md5=b94942b6263fcdaad4a6fad593f1edbf","Tabular datasets, collections of rows and columns, are fundamental in data analysis in basically all areas of research. Self-report questionnaires are a very common and useful tool for gathering data from users, patients, or customers. Often, experts can label each item of these questionnaires as a measure of a given condition or behavior, e.g., mental health conditions such as depression. Considering this, many artificial intelligence techniques, particularly those related to machine learning, have been proposed to analyze the data provided by these tools. However, self-report questionnaires can be very extensive, which often affects the quality of the responses, complicates the data analysis, and renders them more time-consuming. In this paper, the software IREX is presented. IREX iteratively refines tabular datasets, such as self-report questionnaires, while providing an explanation of a given classification model. © 2023 The Author(s)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85161061622"
"Di Martino F.; Delmastro F.","Di Martino, Flavio (57201348697); Delmastro, Franca (14218855800)","57201348697; 14218855800","Explainable AI for clinical and remote health applications: a survey on tabular and time series data","2023","Artificial Intelligence Review","56","6","","5261","5315","54","23","10.1007/s10462-022-10304-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140845313&doi=10.1007%2fs10462-022-10304-3&partnerID=40&md5=31ab9867908d7b987748dd0c07027345","Nowadays Artificial Intelligence (AI) has become a fundamental component of healthcare applications, both clinical and remote, but the best performing AI systems are often too complex to be self-explaining. Explainable AI (XAI) techniques are defined to unveil the reasoning behind the system’s predictions and decisions, and they become even more critical when dealing with sensitive and personal health data. It is worth noting that XAI has not gathered the same attention across different research areas and data types, especially in healthcare. In particular, many clinical and remote health applications are based on tabular and time series data, respectively, and XAI is not commonly analysed on these data types, while computer vision and Natural Language Processing (NLP) are the reference applications. To provide an overview of XAI methods that are most suitable for tabular and time series data in the healthcare domain, this paper provides a review of the literature in the last 5 years, illustrating the type of generated explanations and the efforts provided to evaluate their relevance and quality. Specifically, we identify clinical validation, consistency assessment, objective and standardised quality evaluation, and human-centered quality assessment as key features to ensure effective explanations for the end users. Finally, we highlight the main research challenges in the field as well as the limitations of existing XAI methods. © 2022, The Author(s).","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85140845313"
"","","","Healthcare Transformation with Informatics and Artificial Intelligence","2023","Studies in Health Technology and Informatics","305","","","","","681","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171258987&partnerID=40&md5=2bf83d57ee8ac0825cf6d8bebca33d7c","The proceedings contain 176 papers. The topics discussed include: a hybrid ai-based method for ICD classification of medical documents; the representation of trust in artificial intelligence healthcare research; leveraging multi-word concepts to predict acute kidney injury in intensive care; increasing trust in AI using explainable artificial intelligence for histopathology – an overview; health data democratization in Austria: patients’ perspective; patient-generated health data interoperability through master patient index: the DH-Convener approach; data quality and data quantity: complements or contradictions?; using digital tools to train health emergencies personnel in fragile contexts; deep learning framework for categorical emotional states assessment using electrodermal activity signals; social media analysis tools for public health: a cross-sectional survey; and infodemic insights on trust in a health emergency: a narrative deep-dive.","Conference review","Final","","Scopus","2-s2.0-85171258987"
"Salih A.; Boscolo Galazzo I.; Gkontra P.; Lee A.M.; Lekadir K.; Raisi-Estabragh Z.; Petersen S.E.","Salih, Ahmed (57224318381); Boscolo Galazzo, Ilaria (57218114400); Gkontra, Polyxeni (54787597900); Lee, Aaron Mark (57193254265); Lekadir, Karim (15042517700); Raisi-Estabragh, Zahra (57200574845); Petersen, Steffen E. (35430477200)","57224318381; 57218114400; 54787597900; 57193254265; 15042517700; 57200574845; 35430477200","Explainable Artificial Intelligence and Cardiac Imaging: Toward More Interpretable Models","2023","Circulation: Cardiovascular Imaging","16","4","","E014519","","","12","10.1161/CIRCIMAGING.122.014519","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152972909&doi=10.1161%2fCIRCIMAGING.122.014519&partnerID=40&md5=955b061f4b7baab9120bf9a979ae0ac8","Artificial intelligence applications have shown success in different medical and health care domains, and cardiac imaging is no exception. However, some machine learning models, especially deep learning, are considered black box as they do not provide an explanation or rationale for model outcomes. Complexity and vagueness in these models necessitate a transition to explainable artificial intelligence (XAI) methods to ensure that model results are both transparent and understandable to end users. In cardiac imaging studies, there are a limited number of papers that use XAI methodologies. This article provides a comprehensive literature review of state-of-the-art works using XAI methods for cardiac imaging. Moreover, it provides simple and comprehensive guidelines on XAI. Finally, open issues and directions for XAI in cardiac imaging are discussed.  © 2023 American Heart Association, Inc.","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85152972909"
"Islam M.M.; Alam M.J.; Maniruzzaman M.; Ahmed N.A.M.F.; Ali M.S.; Rahman M.J.; Roy D.C.","Islam, Md Merajul (57222070699); Alam, Md Jahangir (57785247600); Maniruzzaman, Md (57218178766); Ahmed, N.A.M. Faisal (37022996400); Ali, Md Sujan (56882980800); Rahman, Md Jahanur (57200275996); Roy, Dulal Chandra (7402438962)","57222070699; 57785247600; 57218178766; 37022996400; 56882980800; 57200275996; 7402438962","Predicting the risk of hypertension using machine learning algorithms: A cross sectional study in Ethiopia","2023","PLoS ONE","18","8 August","e0289613","","","","4","10.1371/journal.pone.0289613","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168748816&doi=10.1371%2fjournal.pone.0289613&partnerID=40&md5=ef64cb2331f1d6994cd2608d44cf9e46","Background and objectives Hypertension (HTN), a major global health concern, is a leading cause of cardiovascular disease, premature death and disability, worldwide. It is important to develop an automated system to diagnose HTN at an early stage. Therefore, this study devised a machine learning (ML) system for predicting patients with the risk of developing HTN in Ethiopia. Materials and methods The HTN data was taken from Ethiopia, which included 612 respondents with 27 factors. We employed Boruta-based feature selection method to identify the important risk factors of HTN. The four well-known models [logistics regression, artificial neural network, random forest, and extreme gradient boosting (XGB)] were developed to predict HTN patients on the training set using the selected risk factors. The performances of the models were evaluated by accuracy, precision, recall, F1-score, and area under the curve (AUC) on the testing set. Additionally, the SHapley Additive exPlanations (SHAP) method is one of the explainable artificial intelligences (XAI) methods, was used to investigate the associated predictive risk factors of HTN. Results The overall prevalence of HTN patients is 21.2%. This study showed that XGB-based model was the most appropriate model for predicting patients with the risk of HTN and achieved the accuracy of 88.81%, precision of 89.62%, recall of 97.04%, F1-score of 93.18%, and AUC of 0. 894. The XBG with SHAP analysis reveal that age, weight, fat, income, body mass index, diabetes mulitas, salt, history of HTN, drinking, and smoking were the associated risk factors of developing HTN. Conclusions The proposed framework provides an effective tool for accurately predicting individuals in Ethiopia who are at risk for developing HTN at an early stage and may help with early prevention and individualized treatment.  © 2023 Wei et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85168748816"
"Kim R.; Kim C.-W.; Park H.; Lee K.-S.","Kim, Ranyeong (57311717500); Kim, Chae-Won (58497553900); Park, Hyuntae (55573899100); Lee, Kwang-Sig (57221177656)","57311717500; 58497553900; 55573899100; 57221177656","Explainable artificial intelligence on life satisfaction, diabetes mellitus and its comorbid condition","2023","Scientific Reports","13","1","11651","","","","2","10.1038/s41598-023-36285-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165319554&doi=10.1038%2fs41598-023-36285-z&partnerID=40&md5=3a8b004171247716f4f2fc71b7790abc","This study uses artificial intelligence for testing (1) whether the comorbidity of diabetes and its comorbid condition is very strong in the middle-aged or old (hypothesis 1) and (2) whether major determinants of the comorbidity are similar for different pairs of diabetes and its comorbid condition (hypothesis 2). Three pairs are considered, diabetes-cancer, diabetes-heart disease and diabetes-mental disease. Data came from the Korean Longitudinal Study of Ageing (2016–2018), with 5527 participants aged 56 or more. The evaluation of the hypotheses were based on (1) whether diabetes and its comorbid condition in 2016 were top-5 determinants of the comorbidity in 2018 (hypothesis 1) and (2) whether top-10 determinants of the comorbidity in 2018 were similar for different pairs of diabetes and its comorbid condition (hypothesis 2). Based on random forest variable importance, diabetes and its comorbid condition in 2016 were top-2 determinants of the comorbidity in 2018. Top-10 determinants of the comorbidity in 2018 were the same for different pairs of diabetes and its comorbid condition: body mass index, income, age, life satisfaction—health, life satisfaction—economic, life satisfaction—overall, subjective health and children alive in 2016. In terms of SHAP values, the probability of the comorbidity is expected to decrease by 0.02–0.03 in case life satisfaction overall is included to the model. This study supports the two hypotheses, highlighting the importance of preventive measures for body mass index, socioeconomic status, life satisfaction and family support to manage diabetes and its comorbid condition. © 2023, The Author(s).","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85165319554"
"Silva-Aravena F.; Núñez Delafuente H.; Gutiérrez-Bahamondes J.H.; Morales J.","Silva-Aravena, Fabián (57215435139); Núñez Delafuente, Hugo (57963689800); Gutiérrez-Bahamondes, Jimmy H. (57034145700); Morales, Jenny (56396018600)","57215435139; 57963689800; 57034145700; 56396018600","A Hybrid Algorithm of ML and XAI to Prevent Breast Cancer: A Strategy to Support Decision Making","2023","Cancers","15","9","2443","","","","7","10.3390/cancers15092443","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159171609&doi=10.3390%2fcancers15092443&partnerID=40&md5=e2257825e3ce433e2d88e2f7c584a8e1","Worldwide, the coronavirus has intensified the management problems of health services, significantly harming patients. Some of the most affected processes have been cancer patients’ prevention, diagnosis, and treatment. Breast cancer is the most affected, with more than 20 million cases and at least 10 million deaths by 2020. Various studies have been carried out to support the management of this disease globally. This paper presents a decision support strategy for health teams based on machine learning (ML) tools and explainability algorithms (XAI). The main methodological contributions are: first, the evaluation of different ML algorithms that allow classifying patients with and without cancer from the available dataset; and second, an ML methodology mixed with an XAI algorithm, which makes it possible to predict the disease and interpret the variables and how they affect the health of patients. The results show that first, the XGBoost Algorithm has a better predictive capacity, with an accuracy of 0.813 for the train data and 0.81 for the test data; and second, with the SHAP algorithm, it is possible to know the relevant variables and their level of significance in the prediction, and to quantify the impact on the clinical condition of the patients, which will allow health teams to offer early and personalized alerts for each patient. © 2023 by the authors.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85159171609"
"Rahman A.; Hossain M.S.; Muhammad G.; Kundu D.; Debnath T.; Rahman M.; Khan M.S.I.; Tiwari P.; Band S.S.","Rahman, Anichur (57212819402); Hossain, Md. Sazzad (57210729196); Muhammad, Ghulam (56605566900); Kundu, Dipanjali (57219256611); Debnath, Tanoy (57820641800); Rahman, Muaz (57225857374); Khan, Md. Saikat Islam (57221116813); Tiwari, Prayag (57193601962); Band, Shahab S. (57221738247)","57212819402; 57210729196; 56605566900; 57219256611; 57820641800; 57225857374; 57221116813; 57193601962; 57221738247","Federated learning-based AI approaches in smart healthcare: concepts, taxonomies, challenges and open issues","2023","Cluster Computing","26","4","","2271","2311","40","52","10.1007/s10586-022-03658-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136806835&doi=10.1007%2fs10586-022-03658-4&partnerID=40&md5=f7b6de75be0fdc1a5358ae07d6d8eb51","Federated Learning (FL), Artificial Intelligence (AI), and Explainable Artificial Intelligence (XAI) are the most trending and exciting technology in the intelligent healthcare field. Traditionally, the healthcare system works based on centralized agents sharing their raw data. Therefore, huge vulnerabilities and challenges are still existing in this system. However, integrating with AI, the system would be multiple agent collaborators who are capable of communicating with their desired host efficiently. Again, FL is another interesting feature, which works decentralized manner; it maintains the communication based on a model in the preferred system without transferring the raw data. The combination of FL, AI, and XAI techniques can be capable of minimizing several limitations and challenges in the healthcare system. This paper presents a complete analysis of FL using AI for smart healthcare applications. Initially, we discuss contemporary concepts of emerging technologies such as FL, AI, XAI, and the healthcare system. We integrate and classify the FL-AI with healthcare technologies in different domains. Further, we address the existing problems, including security, privacy, stability, and reliability in the healthcare field. In addition, we guide the readers to solving strategies of healthcare using FL and AI. Finally, we address extensive research areas as well as future potential prospects regarding FL-based AI research in the healthcare management system. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Article","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85136806835"
"Quakulinski L.; Koumpis A.; Beyan O.D.","Quakulinski, Lars (57994129400); Koumpis, Adamantios (6602999236); Beyan, Oya Deniz (35785660100)","57994129400; 6602999236; 35785660100","Transparency in Medical Arti¯cial Intelligence Systems","2023","International Journal of Semantic Computing","17","4","","495","510","15","0","10.1142/S1793351X23630011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165929197&doi=10.1142%2fS1793351X23630011&partnerID=40&md5=fe5e91aa161c77d5c5b28fd6f45c9998","Many of the artificial intelligence (AI) systems used nowadays have a very high level of accuracy but fail to explain their decisions. This is critical, especially in sensitive areas such as medicine and the health area at large but also for applications of the law, finance etc., where explanations for certain decisions are needed and are often useful and valuable as the decision itself. This paper presents a review of four different methods for creating transparency in AI systems. It also suggests a list of criteria under which circumstances one should use which methods. © World Scientific Publishing Company.","Article","Final","","Scopus","2-s2.0-85165929197"
"Aksoy B.; Yücel M.; Sayin H.; Salman M.O.K.; Eylence M.; Özmen M.M.","Aksoy, B. (57209338440); Yücel, M. (58451566600); Sayin, H. (58453666500); Salman, M.O.K. (58450162500); Eylence, M. (58077428700); Özmen, M.M. (58076596600)","57209338440; 58451566600; 58453666500; 58450162500; 58077428700; 58076596600","Explainable artificial intelligence applications in dentistry: A theoretical research","2023","Explainable Artificial Intelligence for Biomedical Applications","","","","189","212","23","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163946758&partnerID=40&md5=e135c3fd97386e5f5e66592e94f3772d","With the rapid development of technology, the use of artificial intelligence and related technologies in almost every field has increased significantly. Artificial intelligence applications are used in almost all areas, such as education, engineering, and the defense industry. One of the essential areas of artificial intelligence applications in the health sector. In this chapter of the book, artificial intelligence methods that are frequently used in dentistry and explainable artificial intelligence methods are examined, along with an overview of the dental field, which is one of the critical sectors in health. In addition, theoretical research was carried out by examining the academic studies published in the field of artificial intelligence and explainable artificial intelligence in dentistry. © 2023 River Publishers.","Book chapter","Final","","Scopus","2-s2.0-85163946758"
"Loh H.W.; Ooi C.P.; Oh S.L.; Barua P.D.; Tan Y.R.; Molinari F.; March S.; Acharya U.R.; Fung D.S.S.","Loh, Hui Wen (57220933535); Ooi, Chui Ping (55663773200); Oh, Shu Lih (57185991600); Barua, Prabal Datta (36993665100); Tan, Yi Ren (57202098810); Molinari, Filippo (7004289592); March, Sonja (14040626200); Acharya, U. Rajendra (7004510847); Fung, Daniel Shuen Sheng (7103139903)","57220933535; 55663773200; 57185991600; 36993665100; 57202098810; 7004289592; 14040626200; 7004510847; 7103139903","Deep neural network technique for automated detection of ADHD and CD using ECG signal","2023","Computer Methods and Programs in Biomedicine","241","","107775","","","","10","10.1016/j.cmpb.2023.107775","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169001367&doi=10.1016%2fj.cmpb.2023.107775&partnerID=40&md5=182432f1d4049cfafcac2af408373c89","Background and objective: Attention Deficit Hyperactivity problem (ADHD) is a common neurodevelopment problem in children and adolescents that can lead to long-term challenges in life outcomes if left untreated. Also, ADHD is frequently associated with Conduct Disorder (CD), and multiple research have found similarities in clinical signs and behavioral symptoms between both diseases, making differentiation between ADHD, ADHD comorbid with CD (ADHD+CD), and CD a subjective diagnosis. Therefore, the goal of this pilot study is to create the first explainable deep learning (DL) model for objective ECG-based ADHD/CD diagnosis as having an objective biomarker may improve diagnostic accuracy. Methods: The dataset used in this study consist of ECG data collected from 45 ADHD, 62 ADHD+CD, and 16 CD patients at the Child Guidance Clinic in Singapore. The ECG data were segmented into 2 s epochs and directly used to train our 1-dimensional (1D) convolutional neural network (CNN) model. Results: The proposed model yielded 96.04% classification accuracy, 96.26% precision, 95.99% sensitivity, and 96.11% F1-score. The Gradient-weighted class activation mapping (Grad-CAM) function was also used to highlight the important ECG characteristics at specific time points that most impact the classification score. Conclusion: In addition to achieving model performance results with our suggested DL method, Grad-CAM's implementation also offers vital temporal data that clinicians and other mental healthcare professionals can use to make wise medical judgments. We hope that by conducting this pilot study, we will be able to encourage larger-scale research with a larger biosignal dataset. Hence allowing biosignal-based computer-aided diagnostic (CAD) tools to be implemented in healthcare and ambulatory settings, as ECG can be easily obtained via wearable devices such as smartwatches. © 2023 The Author(s)","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85169001367"
"Susmita S.; Chadaga K.; Sampathila N.; Prabhu S.; Chadaga R.; Swathi Katta S.","Susmita, S. (58549407000); Chadaga, Krishnaraj (57226665664); Sampathila, Niranjana (56584740000); Prabhu, Srikanth (57197645702); Chadaga, Rajagopala (57393370000); Swathi Katta, S. (58880972700)","58549407000; 57226665664; 56584740000; 57197645702; 57393370000; 58880972700","Multiple Explainable Approaches to Predict the Risk of Stroke Using Artificial Intelligence","2023","Information (Switzerland)","14","8","435","","","","0","10.3390/info14080435","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168763523&doi=10.3390%2finfo14080435&partnerID=40&md5=74430b5105a5f317858bafc9cacfcd08","Stroke occurs when a brain’s blood artery ruptures or the brain’s blood supply is interrupted. Due to rupture or obstruction, the brain’s tissues cannot receive enough blood and oxygen. Stroke is a common cause of mortality among older people. Hence, loss of life and severe brain damage can be avoided if stroke is recognized and diagnosed early. Healthcare professionals can discover solutions more quickly and accurately using artificial intelligence (AI) and machine learning (ML). As a result, we have shown how to predict stroke in patients using heterogeneous classifiers and explainable artificial intelligence (XAI). The multistack of ML models surpassed all other classifiers, with accuracy, recall, and precision of 96%, 96%, and 96%, respectively. Explainable artificial intelligence is a collection of frameworks and tools that aid in understanding and interpreting predictions provided by machine learning algorithms. Five diverse XAI methods, such as Shapley Additive Values (SHAP), ELI5, QLattice, Local Interpretable Model-agnostic Explanations (LIME) and Anchor, have been used to decipher the model predictions. This research aims to enable healthcare professionals to provide patients with more personalized and efficient care, while also providing a screening architecture with automated tools that can be used to revolutionize stroke prevention and treatment. © 2023 by the authors.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85168763523"
"Diaz Resendiz J.L.; Ponomaryov V.; Reyes Reyes R.; Sadovnychiy S.","Diaz Resendiz, Jose Luis (57207034774); Ponomaryov, Volodymyr (7003385069); Reyes Reyes, Rogelio (57188751244); Sadovnychiy, Sergiy (12775969500)","57207034774; 7003385069; 57188751244; 12775969500","Explainable CAD System for Classification of Acute Lymphoblastic Leukemia Based on a Robust White Blood Cell Segmentation","2023","Cancers","15","13","3376","","","","6","10.3390/cancers15133376","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164951702&doi=10.3390%2fcancers15133376&partnerID=40&md5=adf1dbedcaa507d3396c8dd21bb099e4","Leukemia is a significant health challenge, with high incidence and mortality rates. Computer-aided diagnosis (CAD) has emerged as a promising approach. However, deep-learning methods suffer from the “black box problem”, leading to unreliable diagnoses. This research proposes an Explainable AI (XAI) Leukemia classification method that addresses this issue by incorporating a robust White Blood Cell (WBC) nuclei segmentation as a hard attention mechanism. The segmentation of WBC is achieved by combining image processing and U-Net techniques, resulting in improved overall performance. The segmented images are fed into modified ResNet-50 models, where the MLP classifier, activation functions, and training scheme have been tested for leukemia subtype classification. Additionally, we add visual explainability and feature space analysis techniques to offer an interpretable classification. Our segmentation algorithm achieves an Intersection over Union (IoU) of 0.91, in six databases. Furthermore, the deep-learning classifier achieves an accuracy of 99.9% on testing. The Grad CAM methods and clustering space analysis confirm improved network focus when classifying segmented images compared to non-segmented images. Overall, the proposed visual explainable CAD system has the potential to assist physicians in diagnosing leukemia and improving patient outcomes. © 2023 by the authors.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85164951702"
"Wang H.; Doumard E.; Soule-Dupuy C.; Kemoun P.; Aligon J.; Monsarrat P.","Wang, Haomiao (58104812500); Doumard, Emmanuel (57211402569); Soule-Dupuy, Chantal (23398246300); Kemoun, Philippe (6504041881); Aligon, Julien (36702555800); Monsarrat, Paul (57188850708)","58104812500; 57211402569; 23398246300; 6504041881; 36702555800; 57188850708","Explanations as a New Metric for Feature Selection: A Systematic Approach","2023","IEEE Journal of Biomedical and Health Informatics","27","8","","4131","4142","11","1","10.1109/JBHI.2023.3279340","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161008077&doi=10.1109%2fJBHI.2023.3279340&partnerID=40&md5=9693888e315506b6feeafdea4098d16a","With the extensive use of Machine Learning (ML) in the biomedical field, there was an increasing need for Explainable Artificial Intelligence (XAI) to improve transparency and reveal complex hidden relationships between variables for medical practitioners, while meeting regulatory requirements. Feature Selection (FS) is widely used as a part of a biomedical ML pipeline to significantly reduce the number of variables while preserving as much information as possible. However, the choice of FS methods affects the entire pipeline including the final prediction explanations, whereas very few works investigate the relationship between FS and model explanations. Through a systematic workflow performed on 145 datasets and an illustration on medical data, the present work demonstrated the promising complementarity of two metrics based on explanations (using ranking and influence changes) in addition to accuracy and retention rate to select the most appropriate FS/ML models. Measuring how much explanations differ with/without FS are particularly promising for FS methods recommendation. While reliefF generally performs the best on average, the optimal choice may vary for each dataset. Positioning FS methods in a tridimensional space, integrating explanations-based metrics, accuracy and retention rate, would allow the user to choose the priorities to be given on each of the dimensions. In biomedical applications, where each medical condition may have its own preferences, this framework will make it possible to offer the healthcare professional the appropriate FS technique, to select the variables that have an important explainable impact, even if this comes at the expense of a limited drop of accuracy.  © 2013 IEEE.","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85161008077"
"Huang W.; Suominen H.; Liu T.; Rice G.; Salomon C.; Barnard A.S.","Huang, Weitong (58037482100); Suominen, Hanna (15056903600); Liu, Tommy (57211238990); Rice, Gregory (36004505600); Salomon, Carlos (36572383100); Barnard, Amanda S. (7005812199)","58037482100; 15056903600; 57211238990; 36004505600; 36572383100; 7005812199","Explainable discovery of disease biomarkers: The case of ovarian cancer to illustrate the best practice in machine learning and Shapley analysis","2023","Journal of Biomedical Informatics","141","","104365","","","","10","10.1016/j.jbi.2023.104365","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153078314&doi=10.1016%2fj.jbi.2023.104365&partnerID=40&md5=fcbecee82cc54c544d7836f61bfadc04","Objective: Ovarian cancer is a significant health issue with lasting impacts on the community. Despite recent advances in surgical, chemotherapeutic and radiotherapeutic interventions, they have had only marginal impacts due to an inability to identify biomarkers at an early stage. Biomarker discovery is challenging, yet essential for improving drug discovery and clinical care. Machine learning (ML) techniques are invaluable for recognising complex patterns in biomarkers compared to conventional methods, yet they can lack physical insights into diagnosis. eXplainable Artificial Intelligence (XAI) is capable of providing deeper insights into the decision-making of complex ML algorithms increasing their applicability. We aim to introduce best practice for combining ML and XAI techniques for biomarker validation tasks. Methods: We focused on classification tasks and a game theoretic approach based on Shapley values to build and evaluate models and visualise results. We described the workflow and apply the pipeline in a case study using the CDAS PLCO Ovarian Biomarkers dataset to demonstrate the potential for accuracy and utility. Results: The case study results demonstrate the efficacy of the ML pipeline, its consistency, and advantages compared to conventional statistical approaches. Conclusion: The resulting guidelines provide a general framework for practical application of XAI in medical research that can inform clinicians and validate and explain cancer biomarkers. © 2023 The Authors","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85153078314"
"Bienefeld N.; Boss J.M.; Lüthy R.; Brodbeck D.; Azzati J.; Blaser M.; Willms J.; Keller E.","Bienefeld, Nadine (54580527400); Boss, Jens Michael (56526392500); Lüthy, Rahel (57208209974); Brodbeck, Dominique (6701660119); Azzati, Jan (57750954400); Blaser, Mirco (58285212900); Willms, Jan (57192418552); Keller, Emanuela (7202811861)","54580527400; 56526392500; 57208209974; 6701660119; 57750954400; 58285212900; 57192418552; 7202811861","Solving the explainable AI conundrum by bridging clinicians’ needs and developers’ goals","2023","npj Digital Medicine","6","1","94","","","","13","10.1038/s41746-023-00837-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160021440&doi=10.1038%2fs41746-023-00837-4&partnerID=40&md5=71592c85edf5536584140161c980f0b8","Explainable artificial intelligence (XAI) has emerged as a promising solution for addressing the implementation challenges of AI/ML in healthcare. However, little is known about how developers and clinicians interpret XAI and what conflicting goals and requirements they may have. This paper presents the findings of a longitudinal multi-method study involving 112 developers and clinicians co-designing an XAI solution for a clinical decision support system. Our study identifies three key differences between developer and clinician mental models of XAI, including opposing goals (model interpretability vs. clinical plausibility), different sources of truth (data vs. patient), and the role of exploring new vs. exploiting old knowledge. Based on our findings, we propose design solutions that can help address the XAI conundrum in healthcare, including the use of causal inference models, personalized explanations, and ambidexterity between exploration and exploitation mindsets. Our study highlights the importance of considering the perspectives of both developers and clinicians in the design of XAI systems and provides practical recommendations for improving the effectiveness and usability of XAI in healthcare. © 2023, The Author(s).","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85160021440"
"Maouche I.; Terrissa L.S.; Benmohammed K.; Zerhouni N.","Maouche, Ikram (57488253500); Terrissa, Labib Sadek (57188877429); Benmohammed, Karima (53363087300); Zerhouni, Noureddine (7003536461)","57488253500; 57188877429; 53363087300; 7003536461","An Explainable AI Approach for Breast Cancer Metastasis Prediction Based on Clinicopathological Data","2023","IEEE Transactions on Biomedical Engineering","70","12","","3321","3329","8","1","10.1109/TBME.2023.3282840","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161587131&doi=10.1109%2fTBME.2023.3282840&partnerID=40&md5=11ff6f02f46df2a9d020ea6acbc11861","Objective: Breast Cancer is the most prevalent cancer and the first cause of cancer deaths among women worldwide. In 90% of the cases, mortality is related to distant metastasis. Computer-aided prognosis systems using machine learning models have been widely used to predict breast cancer metastasis. Despite that, these systems still face several challenges. First, the models are generally biased toward the majority class due to datasets unbalance. Second, their increased complexity is associated with decreased interpretability which causes clinicians to distrust their prognosis. Methods: To tackle these issues, we have proposed an explainable approach for predicting breast cancer metastasis using clinicopathological data. Our approach is based on cost-sensitive CatBoost classifier and utilises LIME explainer to provide patient-level explanations. Results: We used a public dataset of 716 breast cancer patients to assess our approach. The results demonstrate the superiority of cost-sensitive CatBoost in precision (76.5%), recall (79.5%), and f1-score (77%) over classical and boosting models. The LIME explainer was used to quantify the impact of patient and treatment characteristics on breast cancer metastasis, revealing that they have different impacts ranging from high impact like the non-use of adjuvant chemotherapy, and moderate impact including carcinoma with medullary features histological type, to low impact like oral contraception use. The code is available at https://github.com/IkramMaouche/CS-CatBoost Conclusion: Our approach serves as a first step toward introducing more efficient and explainable computer-aided prognosis systems for breast cancer metastasis prediction. Significance: This approach could help clinicians understand the factors behind metastasis and assist them in proposing more patient-specific therapeutic decisions.  © 1964-2012 IEEE.","Article","Final","","Scopus","2-s2.0-85161587131"
"Rahim N.; Abuhmed T.; Mirjalili S.; El-Sappagh S.; Muhammad K.","Rahim, Nasir (57193994970); Abuhmed, Tamer (27067512200); Mirjalili, Seyedali (51461922300); El-Sappagh, Shaker (55233800700); Muhammad, Khan (8942252200)","57193994970; 27067512200; 51461922300; 55233800700; 8942252200","Time-series visual explainability for Alzheimer's disease progression detection for smart healthcare","2023","Alexandria Engineering Journal","82","","","484","502","18","0","10.1016/j.aej.2023.09.050","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174673552&doi=10.1016%2fj.aej.2023.09.050&partnerID=40&md5=6dd40b0939e6c6089b5c8a9fff0ed0cd","Artificial intelligence (AI)-based diagnostic systems provide less error-prone and safer support to clinicians, enhancing the medical decision-making process. This study presents a smart and reliable healthcare framework for detecting Alzheimer's disease (AD) progression. Early detection of AD before the onset of clinical symptoms is the most crucial step in starting timely treatment. To predict the conversion of cognitively normal patients to those with AD, three-dimensional 3D magnetic resonance imaging (MRI) whole-brain neuroimaging methods have been extensively studied. However, depending on the 3D volume, this method is computationally expensive. To solve this problem, we used an approximate rank pooling method originally designed for video action recognition with a 3D MRI volume to obtain a compressed representation of multiple two-dimensional (2D) MRI slices. This study proposes a hybrid multimodal CNN-BiLSTM deep model for AD progression detection, in which the resulting dynamic 2D images are fused with cognitive features. Moreover, a novel explainable AI approach is proposed to provide visual explanations using the resulting longitudinal 2D dynamic images. Temporal explanations were provided by visualizing the affected brain regions captured using longitudinal 2D MRIs. By utilizing a sample of 1,692 subjects with multimodal data from the Alzheimer's Disease Neuroimaging Initiative dataset, our method was assessed using a 10-fold cross-validation process. The model achieved an area under the receiver operating characteristics curve (AUC) of 94% using longitudinal 2D three-time-step dynamic image data. The fusion of 2D dynamic images with cognitive features enhanced the performance by 2% in terms of the AUC. Patients who gradually develop AD, show changes in various brain regions. For such patients, our system highlights the critical role of the hippocampus, medial amygdala, caudal hippocampus, and lateral amygdala at the initial time steps. In the late stages of AD, the system detects abnormalities in extra brain regions such as the medial temporal gyrus, superior temporal gyrus, fusiform gyrus, and caudal hippocampus; indicating that patients have completely progressed to AD. © 2023 THE AUTHORS","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85174673552"
"Youness G.; Aalah A.","Youness, Genane (35249914900); Aalah, Adam (58296787200)","35249914900; 58296787200","An Explainable Artificial Intelligence Approach for Remaining Useful Life Prediction","2023","Aerospace","10","5","474","","","","4","10.3390/aerospace10050474","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160767283&doi=10.3390%2faerospace10050474&partnerID=40&md5=45945ff74fbfb664b278d8a6f68fb13a","Prognosis and health management depend on sufficient prior knowledge of the degradation process of critical components to predict the remaining useful life. This task is composed of two phases: learning and prediction. The first phase uses the available information to learn the system’s behavior. The second phase predicts future behavior based on the available information of the system and estimates its remaining lifetime. Deep learning approaches achieve good prognostic performance but usually suffer from a high computational load and a lack of interpretability. Complex feature extraction models do not solve this problem, as they lose information in the learning phase and thus have a poor prognosis for the remaining lifetime. A new prepossessing approach is used with feature clustering to address this issue. It allows for restructuring the data into homogeneous groups strongly related to each other using a simple architecture of the LSTM model. It is advantageous in terms of learning time and the possibility of using limited computational capabilities. Then, we focus on the interpretability of deep learning prognosis using Explainable AI to achieve interpretable RUL prediction. The proposed approach offers model improvement and enhanced interpretability, enabling a better understanding of feature contributions. Experimental results on the available NASA C-MAPSS dataset show the performance of the proposed model compared to other common methods. © 2023 by the authors.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85160767283"
"Thavanesan N.; Bodala I.; Walters Z.; Ramchurn S.; Underwood T.J.; Vigneswaran G.","Thavanesan, Navamayooran (53980612300); Bodala, Indu (56648184300); Walters, Zoë (41361936300); Ramchurn, Sarvapali (6603260372); Underwood, Timothy J. (23978947100); Vigneswaran, Ganesh (57208137593)","53980612300; 56648184300; 41361936300; 6603260372; 23978947100; 57208137593","Machine learning to predict curative multidisciplinary team treatment decisions in oesophageal cancer","2023","European Journal of Surgical Oncology","49","11","106986","","","","1","10.1016/j.ejso.2023.106986","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165098365&doi=10.1016%2fj.ejso.2023.106986&partnerID=40&md5=6f70a3fc0c2da3aef379ff892c6bdc26","Background: Rising workflow pressures within the oesophageal cancer (OC) multidisciplinary team (MDT) can lead to variability in decision-making, and health inequality. Machine learning (ML) offers a potential automated data-driven approach to address inconsistency and standardize care. The aim of this experimental pilot study was to develop ML models able to predict curative OC MDT treatment decisions and determine the relative importance of underlying decision-critical variables. Methods: Retrospective complete-case analysis of oesophagectomy patients ± neoadjuvant chemotherapy (NACT) or chemoradiotherapy (NACRT) between 2010 and 2020. Established ML algorithms (Multinomial Logistic regression (MLR), Random Forests (RF), Extreme Gradient Boosting (XGB)) and Decision Tree (DT) were used to train models predicting OC MDT treatment decisions: surgery (S), NACT + S or NACRT + S. Performance metrics included Area Under the Curve (AUC), Accuracy, Kappa, LogLoss, F1 and Precision -Recall AUC. Variable importance was calculated for each model. Results: We identified 399 cases with a male-to-female ratio of 3.6:1 and median age of 66.1yrs (range 32–83). MLR outperformed RF, XGB and DT across performance metrics (mean AUC of 0.793 [±0.045] vs 0.757 [±0.068], 0.740 [±0.042], and 0.709 [±0.021] respectively). Variable importance analysis identified age as a major factor in the decision to offer surgery alone or NACT + S across models (p < 0.05). Conclusions: ML techniques can use limited feature-sets to predict curative UGI MDT treatment decisions. Explainable Artificial Intelligence methods provide insight into decision-critical variables, highlighting underlying subconscious biases in cancer care decision-making. Such models may allow prioritization of caseload, improve efficiency, and offer data-driven decision-assistance to MDTs in the future. © 2023 The Author(s)","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85165098365"
"Kose U.; Gupta D.; Chen X.","Kose, Utku (36544118500); Gupta, Deepak (56985108600); Chen, Xi (58454363400)","36544118500; 56985108600; 58454363400","Explainable artificial intelligence for biomedical applications","2023","Explainable Artificial Intelligence for Biomedical Applications","","","","1","380","379","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164002295&partnerID=40&md5=cddfe55fe133e85d075739ba4577f0f7","Since its first appearance, artificial intelligence has been ensuring revolutionary outcomes in the context of real-world problems. At this point, it has strong relations with biomedical and today's intelligent systems compete with human capabilities in medical tasks. However, advanced use of artificial intelligence causes intelligent systems to be black-box. That situation is not good for building trustworthy intelligent systems in medical applications. For a remarkable amount of time, researchers have tried to solve the black-box issue by using modular additions, which have led to the rise of the term: interpretable artificial intelligence. As the literature matured (as a result of, in particular, deep learning), that term transformed into explainable artificial intelligence (XAI). This book provides an essential edited work regarding the latest advancements in explainable artificial intelligence (XAI) for biomedical applications. It includes not only introductive perspectives but also applied touches and discussions regarding critical problems as well as future insights. Topics discussed in the book include: XAI for the applications with medical images XAI use cases for alternative medical data/task Different XAI methods for biomedical applications Reviews for the XAI research for critical biomedical problems. Explainable Artificial Intelligence for Biomedical Applications is ideal for academicians, researchers, students, engineers, and experts from the fields of computer science, biomedical, medical, and health sciences. It also welcomes all readers of different fields to be informed about use cases of XAI in black-box artificial intelligence. In this sense, the book can be used for both teaching and reference source purposes. © 2023 River Publishers. All rights reserved.","Book","Final","","Scopus","2-s2.0-85164002295"
"Kha Q.-H.; Le V.-H.; Hung T.N.K.; Nguyen N.T.K.; Le N.Q.K.","Kha, Quang-Hien (57224131231); Le, Viet-Huan (57226057228); Hung, Truong Nguyen Khanh (57219355602); Nguyen, Ngan Thi Kim (57211071169); Le, Nguyen Quoc Khanh (57208281644)","57224131231; 57226057228; 57219355602; 57211071169; 57208281644","Development and Validation of an Explainable Machine Learning-Based Prediction Model for Drug–Food Interactions from Chemical Structures","2023","Sensors","23","8","3962","","","","15","10.3390/s23083962","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153951370&doi=10.3390%2fs23083962&partnerID=40&md5=a5a2f519dc7346a37a2e8d3426de0c4c","Possible drug–food constituent interactions (DFIs) could change the intended efficiency of particular therapeutics in medical practice. The increasing number of multiple-drug prescriptions leads to the rise of drug–drug interactions (DDIs) and DFIs. These adverse interactions lead to other implications, e.g., the decline in medicament’s effect, the withdrawals of various medications, and harmful impacts on the patients’ health. However, the importance of DFIs remains underestimated, as the number of studies on these topics is constrained. Recently, scientists have applied artificial intelligence-based models to study DFIs. However, there were still some limitations in data mining, input, and detailed annotations. This study proposed a novel prediction model to address the limitations of previous studies. In detail, we extracted 70,477 food compounds from the FooDB database and 13,580 drugs from the DrugBank database. We extracted 3780 features from each drug–food compound pair. The optimal model was eXtreme Gradient Boosting (XGBoost). We also validated the performance of our model on one external test set from a previous study which contained 1922 DFIs. Finally, we applied our model to recommend whether a drug should or should not be taken with some food compounds based on their interactions. The model can provide highly accurate and clinically relevant recommendations, especially for DFIs that may cause severe adverse events and even death. Our proposed model can contribute to developing more robust predictive models to help patients, under the supervision and consultants of physicians, avoid DFI adverse effects in combining drugs and foods for therapy. © 2023 by the authors.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85153951370"
"Kim B.; Srinivasan K.; Kong S.H.; Kim J.H.; Shin C.S.; Ram S.","Kim, Buomsoo (57215896795); Srinivasan, Karthik (57189235644); Kong, Sung Hye (57192955603); Kim, Jung Hee (58136319100); Shin, Chan Soo (35079248800); Ram, Sudha (35608028600)","57215896795; 57189235644; 57192955603; 58136319100; 35079248800; 35608028600","ROLEX: A NOVEL METHOD FOR INTERPRETABLE MACHINE LEARNING USING ROBUST LOCAL EXPLANATIONS","2023","MIS Quarterly: Management Information Systems","47","3","","","","","2","10.25300/MISQ/2022/17141","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180132848&doi=10.25300%2fMISQ%2f2022%2f17141&partnerID=40&md5=80b67af6c049a261a7e31b6f049586d2","Recent developments in big data technologies are revolutionizing the field of healthcare predictive analytics (HPA), enabling researchers to explore challenging problems using complex prediction models. Nevertheless, healthcare practitioners are reluctant to adopt those models as they are less transparent and accountable due to their black-box structure. We believe that instance-level, or local, explanations enhance patient safety and foster trust by enabling patient-level interpretations and medical knowledge discovery. Therefore, we propose the RObust Local EXplanations (ROLEX) method to develop robust, instance-level explanations for HPA models in this study. ROLEX adapts state-of-the-art methods and ameliorates their shortcomings in explaining individual-level predictions made by black-box machine learning models. Our analysis with a large real-world dataset related to a prevalent medical condition called fragility fracture and two publicly available healthcare datasets reveals that ROLEX outperforms widely accepted benchmark methods in terms of local faithfulness of explanations. In addition, ROLEX is more robust since it does not rely on extensive hyperparameter tuning or heuristic algorithms. Explanations generated by ROLEX, along with the prototype user interface presented in this study, have the potential to promote personalized care and precision medicine by providing patient-level interpretations and novel insights. We discuss the theoretical implications of our study in healthcare, big data, and design science. © 2023 University of Minnesota. All rights reserved.","Article","Final","","Scopus","2-s2.0-85180132848"
"Sharma M.; Kaur P.","Sharma, Mradula (55508158800); Kaur, Parmeet (24366355500)","55508158800; 24366355500","XLAAM: explainable LSTM-based activity and anomaly monitoring in a fog environment","2023","Journal of Reliable Intelligent Environments","9","4","","463","477","14","2","10.1007/s40860-022-00185-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134520195&doi=10.1007%2fs40860-022-00185-2&partnerID=40&md5=1e74a29bb25f1e44f4e43dffe91fed87","Study of activities of daily life is gaining wide attention in today’s smart world powered by advanced sensing technologies. It is particularly significant in context of health applications useful for monitoring of elderly living alone and checking on patients in isolation or suffering from chronic diseases. Any significant deviation from an individual’s routine behaviour, such as a fall or ill-health, can be identified as an anomaly. This paper proposes XLAAM, an eXplainable LSTM-based framework to classify the activities of daily life and detect anomalies within a fog-enhanced smart home. Data from sensors in a smart home are forwarded to fog nodes where the classification and anomaly detection tasks are carried out. In case of abnormal activity detection, an alarm is raised or a notification is sent to a health worker or family. Entire data are also streamed to a cloud-based server where eXplainable Artificial Intelligence (XAI) tools are used to interpret explanations of the LSTM model decisions. This is crucial considering the impact of the framework on health and lives of patients. Interpretation of the model and its decision increases the reliability of the model for the patients or users as well as the health practitioners. We have evaluated the proposed approach on a standard dataset to demonstrate its application and feasibility in real-world applications. © 2022, The Author(s), under exclusive licence to Springer Nature Switzerland AG.","Article","Final","","Scopus","2-s2.0-85134520195"
"Wang Y.-C.; Chen T.-C.T.; Chiu M.-C.","Wang, Yu-Cheng (54415093100); Chen, Tin-Chih Toly (8298017700); Chiu, Min-Chi (37076950000)","54415093100; 8298017700; 37076950000","A systematic approach to enhance the explainability of artificial intelligence in healthcare with application to diagnosis of diabetes","2023","Healthcare Analytics","3","","100183","","","","3","10.1016/j.health.2023.100183","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85154588120&doi=10.1016%2fj.health.2023.100183&partnerID=40&md5=15d5cdc7add2720e2e2c8d2d80f992ae","Explainable artificial intelligence (XAI) tools are used to enhance the applications of existing artificial intelligence (AI) technologies by explaining their execution processes and results. In most past research, XAI tools and techniques are typically applied to only the inference part of the AI application. This study proposes a systematic approach to enhance the explainability of AI applications in healthcare. Several AI applications for type 2 diabetes diagnosis are taken as examples to illustrate the applicability of the proposed methodology. According to experimental results, the XAI tools and technologies in the proposed methodology were more diverse than those in the past research. In addition, an artificial neural network was approximated to a simpler and more intuitive classification and regression tree (CART) using local interpretable model-agnostic explanation (LIME). The extracted rules were used to recommend actions to the users to restore their health. © 2023 The Author(s)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85154588120"
"Luschi A.; Nesi P.; Iadanza E.","Luschi, Alessio (55977629100); Nesi, Paolo (7003901227); Iadanza, Ernesto (56256047500)","55977629100; 7003901227; 56256047500","Evidence-based clinical engineering: Health information technology adverse events identification and classification with natural language processing","2023","Heliyon","9","11","e21723","","","","1","10.1016/j.heliyon.2023.e21723","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175538897&doi=10.1016%2fj.heliyon.2023.e21723&partnerID=40&md5=edb3905bd98800d2ae028510deb16603","The primary goal of this project is to create a framework to extract Real-World Evidence to support Health Technology Assessment, Health Technology Management, Evidence-Based Maintenance, and Post Market Surveillance (as outlined in the EU Medical Device Regulation 2017/745) of medical devices using Natural Language Processing (NLP) and Artificial Intelligence. An initial literature review on Spontaneous Reporting System databases, Health Information Technologies (HIT) fault classification, and Natural Language Processing has been conducted, from which it clearly emerges that adverse events related to HIT are increasing over time. The proposed framework uses NLP techniques and Explainable Artificial Intelligence models to automatically identify HIT-related adverse event reports. The designed model employs a pre-trained version of ClinicalBERT that has been fine-tuned and tested on 3,075 adverse event reports extracted from the FDA MAUDE database and manually labelled by experts. © 2023 The Author(s)","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85175538897"
"Jean-Quartier C.; Bein K.; Hejny L.; Hofer E.; Holzinger A.; Jeanquartier F.","Jean-Quartier, Claire (46461386000); Bein, Katharina (58289630500); Hejny, Lukas (58289630600); Hofer, Edith (55549313400); Holzinger, Andreas (23396282000); Jeanquartier, Fleur (36608273700)","46461386000; 58289630500; 58289630600; 55549313400; 23396282000; 36608273700","The Cost of Understanding—XAI Algorithms towards Sustainable ML in the View of Computational Cost","2023","Computation","11","5","92","","","","1","10.3390/computation11050092","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160245402&doi=10.3390%2fcomputation11050092&partnerID=40&md5=e440bf2d5c4b6089f1fe7c9d941250f2","In response to socioeconomic development, the number of machine learning applications has increased, along with the calls for algorithmic transparency and further sustainability in terms of energy efficient technologies. Modern computer algorithms that process large amounts of information, particularly artificial intelligence methods and their workhorse machine learning, can be used to promote and support sustainability; however, they consume a lot of energy themselves. This work focuses and interconnects two key aspects of artificial intelligence regarding the transparency and sustainability of model development. We identify frameworks for measuring carbon emissions from Python algorithms and evaluate energy consumption during model development. Additionally, we test the impact of explainability on algorithmic energy consumption during model optimization, particularly for applications in health and, to expand the scope and achieve a widespread use, civil engineering and computer vision. Specifically, we present three different models of classification, regression and object-based detection for the scenarios of cancer classification, building energy, and image detection, each integrated with explainable artificial intelligence (XAI) or feature reduction. This work can serve as a guide for selecting a tool to measure and scrutinize algorithmic energy consumption and raise awareness of emission-based model optimization by highlighting the sustainability of XAI. © 2023 by the authors.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85160245402"
"Jahan S.; Taher K.A.; Kaiser M.S.; Mahmud M.; Rahman M.S.; Hosen A.S.M.S.; Ra I.-H.","Jahan, Sobhana (57943342700); Taher, Kazi Abu (36069993500); Kaiser, M. Shamim (56446362000); Mahmud, Mufti (35173453700); Rahman, Md Sazzadur (58265736900); Hosen, A.S.M. Sanwar (55354658100); Ra, In-Ho (8895759300)","57943342700; 36069993500; 56446362000; 35173453700; 58265736900; 55354658100; 8895759300","Explainable AI-based Alzheimer’s prediction and management using multimodal data","2023","PLoS ONE","18","11 November","e0294253","","","","4","10.1371/journal.pone.0294253","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177460959&doi=10.1371%2fjournal.pone.0294253&partnerID=40&md5=671bf48104e5a2fa8dc57a99ea4dfd94","Background According to the World Health Organization (WHO), dementia is the seventh leading reason of death among all illnesses and one of the leading causes of disability among the world’s elderly people. Day by day the number of Alzheimer’s patients is rising. Considering the increasing rate and the dangers, Alzheimer’s disease should be diagnosed carefully. Machine learning is a potential technique for Alzheimer’s diagnosis but general users do not trust machine learning models due to the black-box nature. Even, some of those models do not provide the best performance because of using only neuroimaging data. Objective To solve these issues, this paper proposes a novel explainable Alzheimer’s disease prediction model using a multimodal dataset. This approach performs a data-level fusion using clinical data, MRI segmentation data, and psychological data. However, currently, there is very little understanding of multimodal five-class classification of Alzheimer’s disease. Method For predicting five class classifications, 9 most popular Machine Learning models are used. These models are Random Forest (RF), Logistic Regression (LR), Decision Tree (DT), Multi-Layer Perceptron (MLP), K-Nearest Neighbor (KNN), Gradient Boosting (GB), Adaptive Boosting (AdaB), Support Vector Machine (SVM), and Naive Bayes (NB). Among these models RF has scored the highest value. Besides for explainability, SHapley Additive exPlanation (SHAP) is used in this research work. Results and conclusions The performance evaluation demonstrates that the RF classifier has a 10-fold cross-validation accuracy of 98.81% for predicting Alzheimer’s disease, cognitively normal, non-Alzheimer’s dementia, uncertain dementia, and others. In addition, the study utilized Explainable Artificial Intelligence based on the SHAP model and analyzed the causes of prediction. To the best of our knowledge, we are the first to present this multimodal (Clinical, Psychological, and MRI segmentation data) five-class classification of Alzheimer’s disease using Open Access Series of Imaging Studies (OASIS-3) dataset. Besides, a novel Alzheimer’s patient management architecture is also proposed in this work. © 2023 Jahan et al.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85177460959"
"Sannala G.S.; Rohith K.V.G.; Vyas A.G.; Kavitha C.R.","Sannala, Gayatri Sanjana (58745955000); Rohith, K.V.G. (58745123300); Vyas, Aashutosh G. (58745123400); Kavitha, C.R. (57201949373)","58745955000; 58745123300; 58745123400; 57201949373","Explainable Artificial Intelligence-Based Disease Prediction with Symptoms Using Machine Learning Models","2024","Lecture Notes in Networks and Systems","789 LNNS","","","523","538","15","0","10.1007/978-981-99-6586-1_36","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178561044&doi=10.1007%2f978-981-99-6586-1_36&partnerID=40&md5=9aa87fce54d50f646b608c214d4dad8b","Artificial intelligence (AI) has the potential to revolutionize the field of healthcare by automating many tasks, enabling more efficient diagnosis and treatment. However, one of the challenges with AI in healthcare is the need for explainability, as the decisions made by these systems can have serious consequences for patients. AI can be particularly useful in the classification of diseases based on symptoms. This involves using machine learning algorithms to analyze a patient’s symptoms and classify them as having a particular disease or condition. While using black box machine learning algorithms can be highly accurate, there is little to no understanding on how these models work. Therefore, using techniques such as feature importance analysis and Explainable AI, it is possible to provide clear explanations for the decision-making process, which can improve trust and understanding among healthcare providers and patients. © 2024, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Conference paper","Final","","Scopus","2-s2.0-85178561044"
"Riva G.; Sajno E.; DE GASPARI S.; Pupillo C.; Wiederhold B.K.","Riva, Giuseppe (56962750600); Sajno, Elena (57550702300); DE GASPARI, Stefano (57961978400); Pupillo, Chiara (58814617700); Wiederhold, Brenda K. (7003634518)","56962750600; 57550702300; 57961978400; 58814617700; 7003634518","Navigating the Ethical Crossroads: Bridging the gap between Predictive Power and Explanation in the use of Artificial Intelligence in Medicine","2023","Annual Review of CyberTherapy and Telemedicine","21","","","3","7","4","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182484748&partnerID=40&md5=2fa6f24fe85dfe8e990dd2d62f0a1468","Artificial Intelligence (AI) has emerged as a transformative force in medicine, provoking both awe and apprehension in clinicians and patients. In fact, the challenges posed by medical AI extend beyond mere technological hiccups; they delve into the very core of ethics and human decision-making. This paper delves into the intricate dichotomy between the clinical predictive prowess of AI and the human ability to explain decisions, highlighting the ethical challenges arising from this disparity. While humans can elucidate their choices, AI often operates in opaque realms, generating predictions without transparent reasoning. The paper explores the cognitive underpinnings of prediction and explanation, emphasizing the essential interplay between these processes in human intelligence. It critically analyzes the limitations of current medical AI systems, emphasizing their vulnerability to errors and lack of transparency, especially in a critical domain like healthcare. In this paper, we contend that explainability serves as a vital tool to ensure that patients remain at the core of healthcare. It empowers patients and clinicians to make informed, autonomous decisions regarding their health. Explainable Artificial Intelligence (XAI) tackles these challenges. However, achieving it is not easy, and it is strongly dependent from different technical, social and psychological variables. Achieving this objective highlights the urgent requirement for a multidisciplinary approach in XAI that integrates technological knowledge with psychological, cognitive and social perspectives. This alignment will foster innovation, empathy, and responsible implementation, shaping a healthcare landscape that prioritizes both technological advancement and ethical considerations. © 2023, Interactive Media Institute. All rights reserved.","Editorial","Final","","Scopus","2-s2.0-85182484748"
"Bhowmik A.; Sannigrahi M.; Chowdhury D.; Dey A.; Gill S.S.","Bhowmik, Abhimanyu (58101457000); Sannigrahi, Madhushree (57955040100); Chowdhury, Deepraj (57371532800); Dey, Ajoy (57416695100); Gill, Sukhpal Singh (57216940144)","58101457000; 57955040100; 57371532800; 57416695100; 57216940144","CloudAISim: A toolkit for modelling and simulation of modern applications in AI-driven cloud computing environments","2023","BenchCouncil Transactions on Benchmarks, Standards and Evaluations","3","4","100150","","","","1","10.1016/j.tbench.2024.100150","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185547011&doi=10.1016%2fj.tbench.2024.100150&partnerID=40&md5=edb0fdcc7a543d88529d0799015464bc","There is a very significant knowledge gap between Artificial Intelligence (AI) and a multitude of industries that exist in today's modern world. This is primarily attributable to the limited availability of resources and technical expertise. However, a major obstacle is that AI needs to be flexible enough to work in many different applications, utilising a wide variety of datasets through cloud computing. As a result, we developed a benchmark toolkit called CloudAISim to make use of the power of AI and cloud computing in order to satisfy the requirements of modern applications. The goal of this study is to come up with a strategy for building a bridge so that AI can be utilised in order to assist those who are not very knowledgeable about technological advancements. In addition, we modelled a healthcare application as a case study in order to verify the scientific reliability of the CloudAISim toolkit and simulated it in a cloud computing environment using Google Cloud Functions to increase its real-time efficiency. A non-expert-friendly interface built with an interactive web app has also been developed. Any user without any technical knowledge can operate the entire model, which has a 98% accuracy rate. The proposed use case is designed to put AI to work in the healthcare industry, but CloudAISim would be useful and adaptable for other applications in the future. © 2024 The Authors","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85185547011"
"Zimmerman R.M.; Hernandez E.J.; Watkins W.S.; Blue N.; Tristani-Firouzi M.; Yandell M.; Steinberg B.A.","Zimmerman, Raquel M. (57913855600); Hernandez, Edgar J. (57200856898); Watkins, W. Scott (58000463800); Blue, Nathan (56257914800); Tristani-Firouzi, Martin (6602683284); Yandell, Mark (6601908759); Steinberg, Benjamin A. (7102870320)","57913855600; 57200856898; 58000463800; 56257914800; 6602683284; 6601908759; 7102870320","An Explainable Artificial Intelligence Approach for Discovering Social Determinants of Health and Risk Interactions for Stroke in Patients With Atrial Fibrillation","2023","American Journal of Cardiology","201","","","224","226","2","0","10.1016/j.amjcard.2023.05.064","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163397749&doi=10.1016%2fj.amjcard.2023.05.064&partnerID=40&md5=f7600a542f539142d222db87f11412e2","[No abstract available]","Article","Final","","Scopus","2-s2.0-85163397749"
"Drobnič F.; Starc G.; Jurak G.; Kos A.; Pustišek M.","Drobnič, Franc (57210560544); Starc, Gregor (14051136400); Jurak, Gregor (36930488000); Kos, Andrej (57197882237); Pustišek, Matevž (35616021100)","57210560544; 14051136400; 36930488000; 57197882237; 35616021100","Explained Learning and Hyperparameter Optimization of Ensemble Estimator on the Bio-Psycho-Social Features of Children and Adolescents","2023","Electronics (Switzerland)","12","19","4097","","","","0","10.3390/electronics12194097","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173878137&doi=10.3390%2felectronics12194097&partnerID=40&md5=343ea114ff10face16962f41c6ac354b","For decades, projects have been carried out in various countries to assess the developmental status of children and adolescents using anthropometry and specific kinesiological measurements. There is a need for the ability to evaluate this developmental status using a sufficiently simple method or a calculation to be applicable in practice. The most commonly used feature for this purpose is currently body mass index (BMI). From recent experience, this feature may cause problems if used indiscriminately in the developmental phase of life. Therefore, we aimed to find a more suitable feature set. We used data from Artos, the national program monitoring school children and adolescents in Slovenia. The data was analyzed using machine learning (ML) tools to find the most important features to predict a motor efficiency index (MEI), which has been shown to correlate strongly with a person’s health prospects. After data preparation and training a baseline model, a feature selection process was performed, which promoted some features as candidates to predict the motor efficiency index sufficiently. By implementing a hyperparameter optimization, we tuned the ML model to improve its generalization and present the feature interaction more elaborately. We show that besides the single feature’s importance, the features’ interaction should be considered. In the case of MEI, we find that the skin fold thicknesses can complement BMI and contribute to a better development status assessment of children and adolescents. © 2023 by the authors.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85173878137"
"Albahri A.S.; Duhaim A.M.; Fadhel M.A.; Alnoor A.; Baqer N.S.; Alzubaidi L.; Albahri O.S.; Alamoodi A.H.; Bai J.; Salhi A.; Santamaría J.; Ouyang C.; Gupta A.; Gu Y.; Deveci M.","Albahri, A.S. (57201009814); Duhaim, Ali M. (58165003600); Fadhel, Mohammed A. (57192639808); Alnoor, Alhamzah (57204894353); Baqer, Noor S. (57934798200); Alzubaidi, Laith (57195380379); Albahri, O.S. (57201013684); Alamoodi, A.H. (57205435311); Bai, Jinshuai (57217198195); Salhi, Asma (57196190467); Santamaría, Jose (56211885400); Ouyang, Chun (14008574600); Gupta, Ashish (57198676774); Gu, Yuantong (7403046386); Deveci, Muhammet (55734383000)","57201009814; 58165003600; 57192639808; 57204894353; 57934798200; 57195380379; 57201013684; 57205435311; 57217198195; 57196190467; 56211885400; 14008574600; 57198676774; 7403046386; 55734383000","A systematic review of trustworthy and explainable artificial intelligence in healthcare: Assessment of quality, bias risk, and data fusion","2023","Information Fusion","96","","","156","191","35","103","10.1016/j.inffus.2023.03.008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151265419&doi=10.1016%2fj.inffus.2023.03.008&partnerID=40&md5=ed8929df202fd77c56bb836cbd97bccd","In the last few years, the trend in health care of embracing artificial intelligence (AI) has dramatically changed the medical landscape. Medical centres have adopted AI applications to increase the accuracy of disease diagnosis and mitigate health risks. AI applications have changed rules and policies related to healthcare practice and work ethics. However, building trustworthy and explainable AI (XAI) in healthcare systems is still in its early stages. Specifically, the European Union has stated that AI must be human-centred and trustworthy, whereas in the healthcare sector, low methodological quality and high bias risk have become major concerns. This study endeavours to offer a systematic review of the trustworthiness and explainability of AI applications in healthcare, incorporating the assessment of quality, bias risk, and data fusion to supplement previous studies and provide more accurate and definitive findings. Likewise, 64 recent contributions on the trustworthiness of AI in healthcare from multiple databases (i.e., ScienceDirect, Scopus, Web of Science, and IEEE Xplore) were identified using a rigorous literature search method and selection criteria. The considered papers were categorised into a coherent and systematic classification including seven categories: explainable robotics, prediction, decision support, blockchain, transparency, digital health, and review. In this paper, we have presented a systematic and comprehensive analysis of earlier studies and opened the door to potential future studies by discussing in depth the challenges, motivations, and recommendations. In this study a systematic science mapping analysis in order to reorganise and summarise the results of earlier studies to address the issues of trustworthiness and objectivity was also performed. Moreover, this work has provided decisive evidence for the trustworthiness of AI in health care by presenting eight current state-of-the-art critical analyses regarding those more relevant research gaps. In addition, to the best of our knowledge, this study is the first to investigate the feasibility of utilising trustworthy and XAI applications in healthcare, by incorporating data fusion techniques and connecting various important pieces of information from available healthcare datasets and AI algorithms. The analysis of the revised contributions revealed crucial implications for academics and practitioners, and then potential methodological aspects to enhance the trustworthiness of AI applications in the medical sector were reviewed. Successively, the theoretical concept and current use of 17 XAI methods in health care were addressed. Finally, several objectives and guidelines were provided to policymakers to establish electronic health-care systems focused on achieving relevant features such as legitimacy, morality, and robustness. Several types of information fusion in healthcare were focused on in this study, including data, feature, image, decision, multimodal, hybrid, and temporal. © 2023","Article","Final","","Scopus","2-s2.0-85151265419"
"Joyce D.W.; Kormilitzin A.; Smith K.A.; Cipriani A.","Joyce, Dan W. (8840173400); Kormilitzin, Andrey (21834209900); Smith, Katharine A. (57223754356); Cipriani, Andrea (7005521860)","8840173400; 21834209900; 57223754356; 7005521860","Explainable artificial intelligence for mental health through transparency and interpretability for understandability","2023","npj Digital Medicine","6","1","6","","","","33","10.1038/s41746-023-00751-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146612095&doi=10.1038%2fs41746-023-00751-9&partnerID=40&md5=cef9c6fdf0cd02ae9bd268a988761193","The literature on artificial intelligence (AI) or machine learning (ML) in mental health and psychiatry lacks consensus on what “explainability” means. In the more general XAI (eXplainable AI) literature, there has been some convergence on explainability meaning model-agnostic techniques that augment a complex model (with internal mechanics intractable for human understanding) with a simpler model argued to deliver results that humans can comprehend. Given the differing usage and intended meaning of the term “explainability” in AI and ML, we propose instead to approximate model/algorithm explainability by understandability defined as a function of transparency and interpretability. These concepts are easier to articulate, to “ground” in our understanding of how algorithms and models operate and are used more consistently in the literature. We describe the TIFU (Transparency and Interpretability For Understandability) framework and examine how this applies to the landscape of AI/ML in mental health research. We argue that the need for understandablity is heightened in psychiatry because data describing the syndromes, outcomes, disorders and signs/symptoms possess probabilistic relationships to each other—as do the tentative aetiologies and multifactorial social- and psychological-determinants of disorders. If we develop and deploy AI/ML models, ensuring human understandability of the inputs, processes and outputs of these models is essential to develop trustworthy systems fit for deployment. © 2023, The Author(s).","Review","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85146612095"
"Alsaleh M.M.; Allery F.; Choi J.W.; Hama T.; McQuillin A.; Wu H.; Thygesen J.H.","Alsaleh, Mohanad M. (57416895000); Allery, Freya (57472464600); Choi, Jung Won (58235805100); Hama, Tuankasfee (57777574700); McQuillin, Andrew (57214356802); Wu, Honghan (34874014100); Thygesen, Johan H. (42162169300)","57416895000; 57472464600; 58235805100; 57777574700; 57214356802; 34874014100; 42162169300","Prediction of disease comorbidity using explainable artificial intelligence and machine learning techniques: A systematic review","2023","International Journal of Medical Informatics","175","","105088","","","","7","10.1016/j.ijmedinf.2023.105088","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158046088&doi=10.1016%2fj.ijmedinf.2023.105088&partnerID=40&md5=8faad542eba9601f70b4ab59301fcd1a","Objective: Disease comorbidity is a major challenge in healthcare affecting the patient's quality of life and costs. AI-based prediction of comorbidities can overcome this issue by improving precision medicine and providing holistic care. The objective of this systematic literature review was to identify and summarise existing machine learning (ML) methods for comorbidity prediction and evaluate the interpretability and explainability of the models. Materials and methods: The Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) framework was used to identify articles in three databases: Ovid Medline, Web of Science and PubMed. The literature search covered a broad range of terms for the prediction of disease comorbidity and ML, including traditional predictive modelling. Results: Of 829 unique articles, 58 full-text papers were assessed for eligibility. A final set of 22 articles with 61 ML models was included in this review. Of the identified ML models, 33 models achieved relatively high accuracy (80–95%) and AUC (0.80–0.89). Overall, 72% of studies had high or unclear concerns regarding the risk of bias. Discussion: This systematic review is the first to examine the use of ML and explainable artificial intelligence (XAI) methods for comorbidity prediction. The chosen studies focused on a limited scope of comorbidities ranging from 1 to 34 (mean = 6), and no novel comorbidities were found due to limited phenotypic and genetic data. The lack of standard evaluation for XAI hinders fair comparisons. Conclusion: A broad range of ML methods has been used to predict the comorbidities of various disorders. With further development of explainable ML capacity in the field of comorbidity prediction, there is a significant possibility of identifying unmet health needs by highlighting comorbidities in patient groups that were not previously recognised to be at risk for particular comorbidities. © 2023 The Author(s)","Review","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85158046088"
"Lee M.H.; Chew C.J.","Lee, Min Hun (57200519147); Chew, Chong Jun (58546811500)","57200519147; 58546811500","Understanding the Effect of Counterfactual Explanations on Trust and Reliance on AI for Human-AI Collaborative Clinical Decision Making","2023","Proceedings of the ACM on Human-Computer Interaction","7","CSCW2","369","","","","0","10.1145/3610218","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174692019&doi=10.1145%2f3610218&partnerID=40&md5=782cb00cf7b3bcc9cbf0df6fa15968cc","Artificial intelligence (AI) is increasingly being considered to assist human decision-making in high-stake domains (e.g. health). However, researchers have discussed an issue that humans can over-rely on wrong suggestions of the AI model instead of achieving human AI complementary performance. In this work, we utilized salient feature explanations along with what-if, counterfactual explanations to make humans review AI suggestions more analytically to reduce overreliance on AI and explored the effect of these explanations on trust and reliance on AI during clinical decision-making. We conducted an experiment with seven therapists and ten laypersons on the task of assessing post-stroke survivors' quality of motion, and analyzed their performance, agreement level on the task, and reliance on AI without and with two types of AI explanations. Our results showed that the AI model with both salient features and counterfactual explanations assisted therapists and laypersons to improve their performance and agreement level on the task when 'right' AI outputs are presented. While both therapists and laypersons over-relied on 'wrong' AI outputs, counterfactual explanations assisted both therapists and laypersons to reduce their over-reliance on 'wrong' AI outputs by 21% compared to salient feature explanations. Specifically, laypersons had higher performance degrades by 18.0 f1-score with salient feature explanations and 14.0 f1-score with counterfactual explanations than therapists with performance degrades of 8.6 and 2.8 f1-scores respectively. Our work discusses the potential of counterfactual explanations to better estimate the accuracy of an AI model and reduce over-reliance on 'wrong' AI outputs and implications for improving human-AI collaborative decision-making. © 2023 Owner/Author.","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85174692019"
"Kalyakulina A.; Yusipov I.; Moskalev A.; Franceschi C.; Ivanchenko M.","Kalyakulina, Alena (57193713346); Yusipov, Igor (57193381016); Moskalev, Alexey (7003730453); Franceschi, Claudio (56236886700); Ivanchenko, Mikhail (6603861723)","57193713346; 57193381016; 7003730453; 56236886700; 6603861723","eXplainable Artificial Intelligence (XAI) in aging clock models","2024","Ageing Research Reviews","93","","102144","","","","1","10.1016/j.arr.2023.102144","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179430178&doi=10.1016%2fj.arr.2023.102144&partnerID=40&md5=c6dfead0d307e551e83ea9f9f6d800f0","XAI is a rapidly progressing field of machine learning, aiming to unravel the predictions of complex models. XAI is especially required in sensitive applications, e.g. in health care, when diagnosis, recommendations and treatment choices might rely on the decisions made by artificial intelligence systems. AI approaches have become widely used in aging research as well, in particular, in developing biological clock models and identifying biomarkers of aging and age-related diseases. However, the potential of XAI here awaits to be fully appreciated. We discuss the application of XAI for developing the “aging clocks” and present a comprehensive analysis of the literature categorized by the focus on particular physiological systems. © 2023 Elsevier B.V.","Review","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85179430178"
"Lacalamita A.; Serino G.; Pantaleo E.; Monaco A.; Amoroso N.; Bellantuono L.; Piccinno E.; Scalavino V.; Dituri F.; Tangaro S.; Bellotti R.; Giannelli G.","Lacalamita, Antonio (57226689619); Serino, Grazia (37027069800); Pantaleo, Ester (16245945500); Monaco, Alfonso (7201639219); Amoroso, Nicola (55419832300); Bellantuono, Loredana (56166549700); Piccinno, Emanuele (57362281200); Scalavino, Viviana (57215006303); Dituri, Francesco (25925736100); Tangaro, Sabina (8712490600); Bellotti, Roberto (8419904800); Giannelli, Gianluigi (7005015716)","57226689619; 37027069800; 16245945500; 7201639219; 55419832300; 56166549700; 57362281200; 57215006303; 25925736100; 8712490600; 8419904800; 7005015716","Artificial Intelligence and Complex Network Approaches Reveal Potential Gene Biomarkers for Hepatocellular Carcinoma","2023","International Journal of Molecular Sciences","24","20","15286","","","","2","10.3390/ijms242015286","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175272833&doi=10.3390%2fijms242015286&partnerID=40&md5=fd172dc4375bc8ff6517b8d49ee9d187","Hepatocellular carcinoma (HCC) is one of the most common cancers worldwide, and the number of cases is constantly increasing. Early and accurate HCC diagnosis is crucial to improving the effectiveness of treatment. The aim of the study is to develop a supervised learning framework based on hierarchical community detection and artificial intelligence in order to classify patients and controls using publicly available microarray data. With our methodology, we identified 20 gene communities that discriminated between healthy and cancerous samples, with an accuracy exceeding 90%. We validated the performance of these communities on an independent dataset, and with two of them, we reached an accuracy exceeding 80%. Then, we focused on two communities, selected because they were enriched with relevant biological functions, and on these we applied an explainable artificial intelligence (XAI) approach to analyze the contribution of each gene to the classification task. In conclusion, the proposed framework provides an effective methodological and quantitative tool helping to find gene communities, which may uncover pivotal mechanisms responsible for HCC and thus discover new biomarkers. © 2023 by the authors.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85175272833"
"Vidal I.P.; Pereira M.R.; Freire A.P.; Resende U.; Maziero E.G.","Vidal, Igor Pereira (58509983400); Pereira, Marluce Rodrigues (24468494700); Freire, André Pimenta (58856510800); Resende, Uanderson (57203264738); Maziero, Erick Galani (26025713700)","58509983400; 24468494700; 58856510800; 57203264738; 26025713700","Comparison of Explainable Machine-Learning Models for Decision-Making in Health Intensive Care Using SHapley Additive exPlanations","2023","ACM International Conference Proceeding Series","","","","300","307","7","0","10.1145/3592813.3592918","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166021178&doi=10.1145%2f3592813.3592918&partnerID=40&md5=33d56432dfd5c5e0c9185541dd79a189","Context: Intensive Care Units (ICUs) treat patients in serious condition, demanding qualified professional assistance, modern equipment for full-time monitoring of patients, information systems for data collection, medications and other supplies. Problem: Patients can recover or die, and sepsis is one of the main causes of death. Predicting the likelihood of death in sepsis patients can help coordinate medical efforts, as incorrect initial decisions can increase the mortality rate. However, it is important that prediction models with machine learning are explainable to medical staff, so decision-making may be made conscientiously. Solution: This study aimed to identify which Machine Learning algorithms are best for predicting death by sepsis using SHapley Additive exPlanations (SHAP) to provide explainable models. Theoretical Approach: The paper draws from information processing theories based on Machine Learning and explainable artificial intelligence models. Method: 196 observations of real data were used to create Machine Learning models. Data characteristics were analyzed, followed by missing data imputation, preprocessing, feature selection and training of predictive models for SVM, Random Forest, Logistic Regression, KNN and Decision Tree. Two metrics were used to validate the models: accuracy and f1-weighted. For each generated method, SHAP values and models to generate an explainable model listing the factors that most contributed to death predictions. Summary of results: The study showed algorithms with best algorithms were SVM and Logistic Regression (80% for both metrics). The results also showed what models converged in their interpretation using the SHAP values. Contributions and Impact on the IS area: The analysis of the models generated with applied to different machine learning algorithms allow for explainable and transparent analyses by health specialists in decision-making contexts. © 2023 Copyright held by the owner/author(s).","Conference paper","Final","","Scopus","2-s2.0-85166021178"
"Sufriyana H.; Wu Y.-W.; Su E.C.-Y.","Sufriyana, Herdiantri (57216224065); Wu, Yu-Wei (56813986000); Su, Emily Chia-Yu (22939281200)","57216224065; 56813986000; 22939281200","Human-guided deep learning with ante-hoc explainability by convolutional network from non-image data for pregnancy prognostication","2023","Neural Networks","162","","","99","116","17","0","10.1016/j.neunet.2023.02.020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149700436&doi=10.1016%2fj.neunet.2023.02.020&partnerID=40&md5=77c2950ef4641f6334f4b6d39429fe5f","Background and Objective: Deep learning is applied in medicine mostly due to its state-of-the-art performance for diagnostic imaging. Supervisory authorities also require the model to be explainable, but most explain the model after development (post hoc) instead of incorporating explanation into the design (ante hoc). This study aimed to demonstrate a human-guided deep learning with ante-hoc explainability by convolutional network from non-image data to develop, validate, and deploy a prognostic prediction model for PROM and an estimator of time of delivery using a nationwide health insurance database. Methods: To guide modeling, we constructed and verified association diagrams respectively from literatures and electronic health records. Non-image data were transformed into meaningful images utilizing predictor-to-predictor similarities, harnessing the power of convolutional neural network mostly used for diagnostic imaging. The network architecture was also inferred from the similarities. Results: This resulted the best model for prelabor rupture of membranes (n=883, 376) with the area under curves 0.73 (95% CI 0.72 to 0.75) and 0.70 (95% CI 0.69 to 0.71) respectively by internal and external validations, and outperformed previous models found by systematic review. It was explainable by knowledge-based diagrams and model representation. Conclusions: This allows prognostication with actionable insights for preventive medicine. © 2023 Elsevier Ltd","Article","Final","","Scopus","2-s2.0-85149700436"
"Rietberg M.T.; Nguyen V.B.; Geerdink J.; Vijlbrief O.; Seifert C.","Rietberg, Max Tigo (57372224300); Nguyen, Van Bach (57219807408); Geerdink, Jeroen (57200256008); Vijlbrief, Onno (57200259980); Seifert, Christin (8850014900)","57372224300; 57219807408; 57200256008; 57200259980; 8850014900","Accurate and Reliable Classification of Unstructured Reports on Their Diagnostic Goal Using BERT Models","2023","Diagnostics","13","7","1251","","","","4","10.3390/diagnostics13071251","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152713370&doi=10.3390%2fdiagnostics13071251&partnerID=40&md5=501935275d7757cf16c1e13baa68afe0","Understanding the diagnostic goal of medical reports is valuable information for understanding patient flows. This work focuses on extracting the reason for taking an MRI scan of Multiple Sclerosis (MS) patients using the attached free-form reports: Diagnosis, Progression or Monitoring. We investigate the performance of domain-dependent and general state-of-the-art language models and their alignment with domain expertise. To this end, eXplainable Artificial Intelligence (XAI) techniques are used to acquire insight into the inner workings of the model, which are verified on their trustworthiness. The verified XAI explanations are then compared with explanations from a domain expert, to indirectly determine the reliability of the model. BERTje, a Dutch Bidirectional Encoder Representations from Transformers (BERT) model, outperforms RobBERT and MedRoBERTa.nl in both accuracy and reliability. The latter model (MedRoBERTa.nl) is a domain-specific model, while BERTje is a generic model, showing that domain-specific models are not always superior. Our validation of BERTje in a small prospective study shows promising results for the potential uptake of the model in a practical setting. © 2023 by the authors.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85152713370"
"Qiu W.; Chen H.; Kaeberlein M.; Lee S.-I.","Qiu, Wei (57222119175); Chen, Hugh (57219509586); Kaeberlein, Matt (6602710772); Lee, Su-In (7601391623)","57222119175; 57219509586; 6602710772; 7601391623","ExplaiNAble BioLogical Age (ENABL Age): an artificial intelligence framework for interpretable biological age","2023","The Lancet Healthy Longevity","4","12","","e711","e723","12","2","10.1016/S2666-7568(23)00189-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176102544&doi=10.1016%2fS2666-7568%2823%2900189-7&partnerID=40&md5=15768b16117e97aa7beb67374b75f929","Background: Biological age is a measure of health that offers insights into ageing. The existing age clocks, although valuable, often trade off accuracy and interpretability. We introduce ExplaiNAble BioLogical Age (ENABL Age), a computational framework that combines machine-learning models with explainable artificial intelligence (XAI) methods to accurately estimate biological age with individualised explanations. Methods: To construct the ENABL Age clock, we first predicted an age-related outcome (eg, all-cause or cause-specific mortality), and then rescaled these predictions to estimate biological age, using UK Biobank and National Health and Nutrition Examination Survey (NHANES) datasets. We adapted existing XAI methods to decompose individual ENABL Ages into contributing risk factors. For broad accessibility, we developed two versions: ENABL Age-L, based on blood tests, and ENABL Age-Q, based on questionnaire characteristics. Finally, we validated diverse ageing mechanisms captured by each ENABL Age clock through genome-wide association studies (GWAS) association analyses. Findings: Our ENABL Age clock was significantly correlated with chronological age (r=0·7867, p<0·0001 for UK Biobank; r=0·7126, p<0·0001 for NHANES). These clocks distinguish individuals who are healthy (ie, their ENABL Age is lower than their chronological age) from those who are unhealthy (ie, their ENABL Age is higher than their chronological age), predicting mortality more effectively than existing clocks. Groups of individuals who were unhealthy showed approximately three to 12 times higher log hazard ratio than healthy groups, as per ENABL Age. The clocks achieved high mortality prediction power with an area under the receiver operating characteristic curve of 0·8179 for 5-year mortality and 0·8115 for 10-year mortality on the UK Biobank dataset, and 0·8935 for 5-year mortality and 0·9107 for 10-year mortality on the NHANES dataset. The individualised explanations that revealed the contribution of specific characteristics to ENABL Age provided insights into the important characteristics for ageing. An association analysis with risk factors and ageing-related morbidities and GWAS results on ENABL Age clocks trained on different mortality causes showed that each clock captures distinct ageing mechanisms. Interpretation: ENABL Age brings an important leap forward in the application of XAI for interpreting biological age clocks. ENABL Age also carries substantial potential in practical settings, assisting medical professionals in untangling the complexity of ageing mechanisms, and potentially becoming a valuable tool in informed clinical decision-making processes. Funding: National Science Foundation and National Institutes of Health. © 2023 The Author(s). Published by Elsevier Ltd. This is an Open Access article under the CC BY-NC-ND 4.0 license","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85176102544"
"Prasath Alias S.S.; Manikandan R.; Kumar A.","Prasath Alias, Surendhar S. (57211279085); Manikandan, R. (37061393600); Kumar, Ambeshwar (57202315403)","57211279085; 37061393600; 57202315403","Class activation mapping and deep learning for explainable biomedical applications","2023","Explainable Artificial Intelligence for Biomedical Applications","","","","123","143","20","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163958840&partnerID=40&md5=6310d19d7477db7a2f3fbf6aa666de45","For a number of medical diagnostic tasks, deep learning (DL) methods have proven to be quite successful, sometimes even outperforming human experts. The algorithms' black-box nature has, however, limited their therapeutic application. Recent studies on explainability seek to identify the factors most responsible for a model's choice. In the biomedical domain, deep neural networks (DNNs) now represent most successful machine learning (ML) technologies. The various topics of interest in this field include BBMI (study of interface between the brain as well as body's mechanical systems), bioimaging (the study of biological cells and tissues), medical imaging (study of human organs through the creation of visual representations), and public and medical health management (PmHM). This study provides an overview of explainable artificial intelligence (XAI) applied in class activation mapping-based DL medical picture analysis. For the purpose of categorizing DL-based medical image analysis (MIA) techniques, a framework of XAI criteria is presented. The papers are then surveyed and categorized in accordance with framework as well as based on anatomical location for use in MIA. © 2023 River Publishers.","Book chapter","Final","","Scopus","2-s2.0-85163958840"
"Abuzinadah N.; Kumar Posa S.; Alarfaj A.A.; Alabdulqader E.A.; Umer M.; Kim T.-H.; Alsubai S.; Ashraf I.","Abuzinadah, Nihal (58099302300); Kumar Posa, Sarath (58782407900); Alarfaj, Aisha Ahmed (57210585559); Alabdulqader, Ebtisam Abdullah (36968004500); Umer, Muhammad (58255137000); Kim, Tai-Hoon (56981749100); Alsubai, Shtwai (57194975731); Ashraf, Imran (57195478761)","58099302300; 58782407900; 57210585559; 36968004500; 58255137000; 56981749100; 57194975731; 57195478761","Improved Prediction of Ovarian Cancer Using Ensemble Classifier and Shaply Explainable AI","2023","Cancers","15","24","5793","","","","0","10.3390/cancers15245793","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180638128&doi=10.3390%2fcancers15245793&partnerID=40&md5=c547f42d3fc34114bb2798b0aaed50de","The importance of detecting and preventing ovarian cancer is of utmost significance for women’s overall health and wellness. Referred to as the “silent killer,” ovarian cancer exhibits inconspicuous symptoms during its initial phases, posing a challenge for timely identification. Identification of ovarian cancer during its advanced stages significantly diminishes the likelihood of effective treatment and survival. Regular screenings, such as pelvic exams, ultrasound, and blood tests for specific biomarkers, are essential tools for detecting the disease in its early, more treatable stages. This research makes use of the Soochow University ovarian cancer dataset, containing 50 features for the accurate detection of ovarian cancer. The proposed predictive model makes use of a stacked ensemble model, merging the strengths of bagging and boosting classifiers, and aims to enhance predictive accuracy and reliability. This combination harnesses the benefits of variance reduction and improved generalization, contributing to superior ovarian cancer prediction outcomes. The proposed model gives 96.87% accuracy, which is currently the highest model result obtained on this dataset so far using all features. Moreover, the outcomes are elucidated utilizing the explainable artificial intelligence method referred to as SHAPly. The excellence of the suggested model is demonstrated through a comparison of its performance with that of other cutting-edge models. © 2023 by the authors.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85180638128"
"Raab D.; Theissler A.; Spiliopoulou M.","Raab, Dominik (57218408547); Theissler, Andreas (55337910800); Spiliopoulou, Myra (56248430300)","57218408547; 55337910800; 56248430300","XAI4EEG: spectral and spatio-temporal explanation of deep learning-based seizure detection in EEG time series","2023","Neural Computing and Applications","35","14","","10051","10068","17","8","10.1007/s00521-022-07809-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139161193&doi=10.1007%2fs00521-022-07809-x&partnerID=40&md5=57a3590a02001ad2a6d4eb05d97a6146","In clinical practice, algorithmic predictions may seriously jeopardise patients’ health and thus are required to be validated by medical experts before a final clinical decision is met. Towards that aim, there is need to incorporate explainable artificial intelligence techniques into medical research. In the specific field of epileptic seizure detection there are several machine learning algorithms but less methods on explaining them in an interpretable way. Therefore, we introduce XAI4EEG: an application-aware approach for an explainable and hybrid deep learning-based detection of seizures in multivariate EEG time series. In XAI4EEG, we combine deep learning models and domain knowledge on seizure detection, namely (a) frequency bands, (b) location of EEG leads and (c) temporal characteristics. XAI4EEG encompasses EEG data preparation, two deep learning models and our proposed explanation module visualizing feature contributions that are obtained by two SHAP explainers, each explaining the predictions of one of the two models. The resulting visual explanations provide an intuitive identification of decision-relevant regions in the spectral, spatial and temporal EEG dimensions. To evaluate XAI4EEG, we conducted a user study, where users were asked to assess the outputs of XAI4EEG, while working under time constraints, in order to emulate the fact that clinical diagnosis is done - more often than not - under time pressure. We found that the visualizations of our explanation module (1) lead to a substantially lower time for validating the predictions and (2) leverage an increase in interpretability, trust and confidence compared to selected SHAP feature contribution plots. © 2022, The Author(s).","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85139161193"
"Jovanovic G.; Perisic M.; Bacanin N.; Zivkovic M.; Stanisic S.; Strumberger I.; Alimpic F.; Stojic A.","Jovanovic, Gordana (57225371381); Perisic, Mirjana (58354176300); Bacanin, Nebojsa (37028223900); Zivkovic, Miodrag (57208755936); Stanisic, Svetlana (57888309800); Strumberger, Ivana (57191590910); Alimpic, Filip (57803322400); Stojic, Andreja (28168051600)","57225371381; 58354176300; 37028223900; 57208755936; 57888309800; 57191590910; 57803322400; 28168051600","Potential of Coupling Metaheuristics-Optimized-XGBoost and SHAP in Revealing PAHs Environmental Fate","2023","Toxics","11","4","394","","","","7","10.3390/toxics11040394","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153715423&doi=10.3390%2ftoxics11040394&partnerID=40&md5=497994f9bca7d7d580e75ac79feb3325","Polycyclic aromatic hydrocarbons (PAHs) refer to a group of several hundred compounds, among which 16 are identified as priority pollutants, due to their adverse health effects, frequency of occurrence, and potential for human exposure. This study is focused on benzo(a)pyrene, being considered an indicator of exposure to a PAH carcinogenic mixture. For this purpose, we have applied the XGBoost model to a two-year database of pollutant concentrations and meteorological parameters, with the aim to identify the factors which were mostly associated with the observed benzo(a)pyrene concentrations and to describe types of environments that supported the interactions between benzo(a)pyrene and other polluting species. The pollutant data were collected at the energy industry center in Serbia, in the vicinity of coal mining areas and power stations, where the observed benzo(a)pyrene maximum concentration for a study period reached 43.7 (Formula presented.). The metaheuristics algorithm has been used to optimize the XGBoost hyperparameters, and the results have been compared to the results of XGBoost models tuned by eight other cutting-edge metaheuristics algorithms. The best-produced model was later on interpreted by applying Shapley Additive exPlanations (SHAP). As indicated by mean absolute SHAP values, the temperature at the surface, arsenic, PM (Formula presented.), and total nitrogen oxide (NOx) concentrations appear to be the major factors affecting benzo(a)pyrene concentrations and its environmental fate. © 2023 by the authors.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85153715423"
"Kırboğa K.K.; Küçüksille E.U.; Naldan M.E.; Işık M.; Gülcü O.; Aksakal E.","Kırboğa, Kevser Kübra (57445226400); Küçüksille, Ecir Uğur (18037773100); Naldan, Muhammet Emin (56365478600); Işık, Mesut (56597706300); Gülcü, Oktay (56731020300); Aksakal, Emrah (57190606287)","57445226400; 18037773100; 56365478600; 56597706300; 56731020300; 57190606287","CVD22: Explainable artificial intelligence determination of the relationship of troponin to D-Dimer, mortality, and CK-MB in COVID-19 patients","2023","Computer Methods and Programs in Biomedicine","233","","107492","","","","3","10.1016/j.cmpb.2023.107492","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150763177&doi=10.1016%2fj.cmpb.2023.107492&partnerID=40&md5=36c3d85762890c6935dde784dd17aaa7","Background and purpose: COVID-19, which emerged in Wuhan (China), is one of the deadliest and fastest-spreading pandemics as of the end of 2019. According to the World Health Organization (WHO), there are more than 100 million infectious cases worldwide. Therefore, research models are crucial for managing the pandemic scenario. However, because the behavior of this epidemic is so complex and difficult to understand, an effective model must not only produce accurate predictive results but must also have a clear explanation that enables human experts to act proactively. For this reason, an innovative study has been planned to diagnose Troponin levels in the COVID-19 process with explainable white box algorithms to reach a clear explanation. Methods: Using the pandemic data provided by Erzurum Training and Research Hospital (decision number: 2022/13-145), an interpretable explanation of Troponin data was provided in the COVID-19 process with SHApley Additive exPlanations (SHAP) algorithms. Five machine learning (ML) algorithms were developed. Model performances were determined based on training, test accuracies, precision, F1-score, recall, and AUC (Area Under the Curve) values. Feature importance was estimated according to Shapley values by applying the SHApley Additive exPlanations (SHAP) method to the model with high accuracy. The model created with Streamlit v.3.9 was integrated into the interface with the name CVD22. Results: Among the five-machine learning (ML) models created with pandemic data, the best model was selected with the values of 1.0, 0.83, 0.86, 0.83, 0.80, and 0.91 in train and test accuracy, precision, F1-score, recall, and AUC values, respectively. As a result of feature selection and SHApley Additive exPlanations (SHAP) algorithms applied to the XGBoost model, it was determined that DDimer mean, mortality, CKMB (creatine kinase myocardial band), and Glucose were the features with the highest importance over the model estimation. Conclusions: Recent advances in new explainable artificial intelligence (XAI) models have successfully made it possible to predict the future using large historical datasets. Therefore, throughout the ongoing pandemic, CVD22 (https://cvd22covid.streamlitapp.com/) can be used as a guide to help authorities or medical professionals make the best decisions quickly. © 2023 Elsevier B.V.","Article","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85150763177"
"Zahra Q.; Gul J.; Shah A.R.; Yasir M.; Karim A.M.","Zahra, Qandeel (57193011216); Gul, Jawaria (58154046000); Shah, Ali Raza (58638664800); Yasir, Muhammad (57212058563); Karim, Asad Mustafa (56405119200)","57193011216; 58154046000; 58638664800; 57212058563; 56405119200","Antibiotic resistance genes prevalence prediction and interpretation in beaches affected by urban wastewater discharge","2023","One Health","17","","100642","","","","1","10.1016/j.onehlt.2023.100642","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173695445&doi=10.1016%2fj.onehlt.2023.100642&partnerID=40&md5=b8a6b3d1d1041f7b711d8fba56c785b8","Background: The annual death toll of over 1.2 million worldwide is attributed to infections caused by resistant bacteria, driven by the significant impact of antibiotic misuse and overuse in spreading these bacteria and their associated antibiotic resistance genes (ARGs). While limited data suggest the presence of ARGs in beach environments, efficient prediction tools are needed for monitoring and detecting ARGs to ensure public health safety. This study aims to develop interpretable machine learning methods for predicting ARGs in beach waters, addressing the challenge of black-box models and enhancing our understanding of their internal mechanisms. Methods: In this study, we systematically collected beach water samples and subsequently isolated bacteria from these samples using various differential and selective media supplemented with different antibiotics. Resistance profiles of bacteria were determined by using Kirby-Bauer disk diffusion method. Further, ARGs were enumerated by using the quantitative polymerase chain reaction (qPCR) to detect and quantify ARGs. The obtained qPCR data and hydro-meteorological were used to create an ML model with high prediction performance and we further used two explainable artificial intelligence (xAI) model-agnostic interpretation methods to describe the internal behavior of ML model. Results: Using qPCR, we detected blaCTX−M, blaNDM, blaCMY, blaOXA, blatetX, blasul1, and blaaac(6′-Ib-cr) in the beach waters. Further, we developed ML prediction models for blaaac(6′-Ib-cr), blasul1, and blatetX using the hydro-metrological and qPCR-derived data and the models demonstrated strong performance, with R2 values of 0.957, 0.997, and 0.976, respectively. Conclusions: Our findings show that environmental factors, such as water temperature, precipitation, and tide, are among the important predictors of the abundance of resistance genes at beaches. © 2023","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85173695445"
"Bohn L.; Drouin S.M.; McFall G.P.; Rolfson D.B.; Andrew M.K.; Dixon R.A.","Bohn, Linzy (57162897300); Drouin, Shannon M. (57196067600); McFall, G. Peggy (23473648400); Rolfson, Darryl B. (6603298467); Andrew, Melissa K. (56819253000); Dixon, Roger A. (57215201699)","57162897300; 57196067600; 23473648400; 6603298467; 56819253000; 57215201699","Machine learning analyses identify multi-modal frailty factors that selectively discriminate four cohorts in the Alzheimer’s disease spectrum: a COMPASS-ND study","2023","BMC Geriatrics","23","1","837","","","","0","10.1186/s12877-023-04546-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179338190&doi=10.1186%2fs12877-023-04546-1&partnerID=40&md5=8adeebaec1821b5bcaa14f7a3c8a72f8","Background: Frailty indicators can operate in dynamic amalgamations of disease conditions, clinical symptoms, biomarkers, medical signals, cognitive characteristics, and even health beliefs and practices. This study is the first to evaluate which, among these multiple frailty-related indicators, are important and differential predictors of clinical cohorts that represent progression along an Alzheimer’s disease (AD) spectrum. We applied machine-learning technology to such indicators in order to identify the leading predictors of three AD spectrum cohorts; viz., subjective cognitive impairment (SCI), mild cognitive impairment (MCI), and AD. The common benchmark was a cohort of cognitively unimpaired (CU) older adults. Methods: The four cohorts were from the cross-sectional Comprehensive Assessment of Neurodegeneration and Dementia dataset. We used random forest analysis (Python 3.7) to simultaneously test the relative importance of 83 multi-modal frailty indicators in discriminating the cohorts. We performed an explainable artificial intelligence method (Tree Shapley Additive exPlanation values) for deep interpretation of prediction effects. Results: We observed strong concurrent prediction results, with clusters varying across cohorts. The SCI model demonstrated excellent prediction accuracy (AUC = 0.89). Three leading predictors were poorer quality of life ([QoL]; memory), abnormal lymphocyte count, and abnormal neutrophil count. The MCI model demonstrated a similarly high AUC (0.88). Five leading predictors were poorer QoL (memory, leisure), male sex, abnormal lymphocyte count, and poorer self-rated eyesight. The AD model demonstrated outstanding prediction accuracy (AUC = 0.98). Ten leading predictors were poorer QoL (memory), reduced olfaction, male sex, increased dependence in activities of daily living (n = 6), and poorer visual contrast. Conclusions: Both convergent and cohort-specific frailty factors discriminated the AD spectrum cohorts. Convergence was observed as all cohorts were marked by lower quality of life (memory), supporting recent research and clinical attention to subjective experiences of memory aging and their potentially broad ramifications. Diversity was displayed in that, of the 14 leading predictors extracted across models, 11 were selectively sensitive to one cohort. A morbidity intensity trend was indicated by an increasing number and diversity of predictors corresponding to clinical severity, especially in AD. Knowledge of differential deficit predictors across AD clinical cohorts may promote precision interventions. © 2023, The Author(s).","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85179338190"
"Siptroth J.; Moskalenko O.; Krumbiegel C.; Ackermann J.; Koch I.; Pospisil H.","Siptroth, Julienne (58101588900); Moskalenko, Olga (57195971697); Krumbiegel, Carsten (35069623300); Ackermann, Jörg (57189026922); Koch, Ina (7006792171); Pospisil, Heike (6603405785)","58101588900; 57195971697; 35069623300; 57189026922; 7006792171; 6603405785","Investigation of metabolic pathways from gut microbiome analyses regarding type 2 diabetes mellitus using artificial neural networks","2023","Discover Artificial Intelligence","3","1","19","","","","0","10.1007/s44163-023-00064-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190699644&doi=10.1007%2fs44163-023-00064-6&partnerID=40&md5=e2aa364a0f2903d4be9e343834b03545","Background: Type 2 diabetes mellitus is a prevalent disease that contributes to the development of various health issues, including kidney failure and strokes. As a result, it poses a significant challenge to the worldwide healthcare system. Research into the gut microbiome has enabled the identification and description of various diseases, with bacterial pathways playing a critical role in this context. These pathways link individual bacteria based on their biological functions. This study deals with the classification of microbiome pathway profiles of type 2 diabetes mellitus patients. Methods: Pathway profiles were determined by next-generation sequencing of 16S rDNA from stool samples, which were subsequently assigned to bacteria. Then, the involved pathways were assigned by the identified gene families. The classification of type 2 diabetes mellitus is enabled by a constructed neural network. Furthermore, a feature importance analysis was performed via a game theoretic approach (SHapley Additive exPlanations). The study not only focuses on the classification using neural networks, but also on identifying crucial bacterial pathways. Results: It could be shown that a neural network classification of type 2 diabetes mellitus and a healthy comparison group is possible with an excellent prediction accuracy. It was possible to create a ranking to identify the pathways that have a high impact on the model prediction accuracy. In this way, new associations between the alteration of, e.g. a biosynthetic pathway and the presence of diabetes mellitus type 2 disease can also be discovered. The basis is formed by 946 microbiome pathway profiles from diabetes mellitus type 2 patients (272) and healthy comparison persons (674). Conclusion: With this study of the gut microbiome, we present an approach using a neural network to obtain a classification of healthy and type 2 diabetes mellitus and to identify the critical features. Intestinal bacteria pathway profiles form the basis. © The Author(s) 2023.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85190699644"
"Sunder M.S.S.; Tikkiwal V.A.; Kumar A.; Tyagi B.","Sunder, M. S. Shyam (57571809200); Tikkiwal, Vinay Anand (56040143100); Kumar, Arun (57202315390); Tyagi, Bhishma (23978736500)","57571809200; 56040143100; 57202315390; 23978736500","Unveiling the Transparency of Prediction Models for Spatial PM2.5 over Singapore: Comparison of Different Machine Learning Approaches with eXplainable Artificial Intelligence","2023","AI (Switzerland)","4","4","","787","811","24","0","10.3390/ai4040040","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180209521&doi=10.3390%2fai4040040&partnerID=40&md5=ef20033047c743bc51ce31bb659aa94a","Aerosols play a crucial role in the climate system due to direct and indirect effects, such as scattering and absorbing radiant energy. They also have adverse effects on visibility and human health. Humans are exposed to fine PM2.5, which has adverse health impacts related to cardiovascular and respiratory-related diseases. Long-term trends in PM concentrations are influenced by emissions and meteorological variations, while meteorological factors primarily drive short-term variations. Factors such as vegetation cover, relative humidity, temperature, and wind speed impact the divergence in the PM2.5 concentrations on the surface. Machine learning proved to be a good predictor of air quality. This study focuses on predicting PM2.5 with these parameters as input for spatial and temporal information. The work analyzes the in situ observations for PM2.5 over Singapore for seven years (2014–2021) at five locations, and these datasets are used for spatial prediction of PM2.5. The study aims to provide a novel framework based on temporal-based prediction using Random Forest (RF), Gradient Boosting (GB) regression, and Tree-based Pipeline Optimization Tool (TP) Auto ML works based on meta-heuristic via genetic algorithm. TP produced reasonable Global Performance Index values; 7.4 was the highest GPI value in August 2016, and the lowest was −0.6 in June 2019. This indicates the positive performance of the TP model; even the negative values are less than other models, denoting less pessimistic predictions. The outcomes are explained with the eXplainable Artificial Intelligence (XAI) techniques which help to investigate the fidelity of feature importance of the machine learning models to extract information regarding the rhythmic shift of the PM2.5 pattern. © 2023 by the authors.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85180209521"
"Handa T.; Singhal I.; Chakraborty P.; Kaur G.","Handa, Tanvi (58855411800); Singhal, Ishita (57985795600); Chakraborty, Pooja (58855395400); Kaur, Geetpriya (59112399900)","58855411800; 57985795600; 58855395400; 59112399900","Recent trends of federated learning for smart healthcare systems","2023","Federated Learning and AI for Healthcare 5.0","","","","78","103","25","0","10.4018/979-8-3693-1022-9.ch005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183678204&doi=10.4018%2f979-8-3693-1022-9.ch005&partnerID=40&md5=38aec02ada3b29ef7306d2640e2f10ad","The Internet of Things (IoT) has brought a revolutionary change in the healthcare system. Smart devices have helped people maintain their health by collecting and storing a wide range of data. Artificial intel¬ligence (AI) has made its promising way in several areas. They help in the early diagnosis of various diseases along with storage and interpretation of health data. However, due to the lack of communica¬tion between devices and the risk of transmission of data, the efficiency of AI devices is questionable. To avoid the transmission of data, Federation learning (FL) was highlighted as an approach where issues related to the security of sensitive data can be reduced significantly. The combination of FL, AI, and Explainable Artificial Intelligence (XAI) techniques can minimize several limitations and challenges in the healthcare system. This chapter presents an overview of FL's application in healthcare. Different studies presented data about FL and its usage in healthcare. Currently, this paradigm approach is suc¬cessfully used by specialists in diagnostic purposes. © 2024, IGI Global. All rights reserved.","Book chapter","Final","","Scopus","2-s2.0-85183678204"
"Cardellicchio A.; Ruggieri S.; Nettis A.; Renò V.; Uva G.","Cardellicchio, Angelo (56786372800); Ruggieri, Sergio (57200721168); Nettis, Andrea (57214778072); Renò, Vito (56433738300); Uva, Giuseppina (12143743700)","56786372800; 57200721168; 57214778072; 56433738300; 12143743700","Physical interpretation of machine learning-based recognition of defects for the risk management of existing bridge heritage","2023","Engineering Failure Analysis","149","","107237","","","","45","10.1016/j.engfailanal.2023.107237","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151658094&doi=10.1016%2fj.engfailanal.2023.107237&partnerID=40&md5=3f3dfb743b828e9d9c25ac347ad04de0","The challenge of the research work presented in the paper is to combine the growing interest in monitoring the health condition of existing bridge heritage through systematic and periodic visual inspections with automated recognition of typical bridge defects, which can greatly facilitate the assessment of defect evolution over time. The study focused on the automated identification of defects in existing Reinforced Concrete (RC) bridges exploiting different Deep Learning (DL) approaches and techniques to interpret the obtained predictions. Ensuring the safety of infrastructures is typically a technical and economic issue. Still, in the case of the engineering infrastructure heritage, there are existing bridges and viaducts with a high historical, cultural, and symbolic value. For them, accurate knowledge and characterization of possible degradation processes become particularly important in order to define intervention strategies that combine safety and conservation requirements. With the aim to develop systematic and non-invasive investigation protocols for continuous and effective control of defects and their evolution, a database of existing RC bridge defect images was collected, and the most recurrent defect typologies were classified by domain experts. Some existing Convolutional Neural Networks (CNNs) algorithms were applied to the dataset for automatically recognizing all defects, but the specific novel contribution of the research work is the interpretation of the obtained results in a form that is humanly explainable and directly implementable in new tools for bridge inspections. To interpret the results, Class Activation Maps (CAMs) approaches were employed within available eXplainable Artificial Intelligence (XAI) techniques, which allow to observe the activation zones and nearly perfectly highlight the type of specific defect in a given image. The obtained results, besides suggesting which network works better than others and if the specific defect is effectively recognized, have been evaluated through a quasi-quantitative procedure that compared a qualitative assessment of the CNNs models reliability with two novel indexes representing new explaining metrics of the obtained results. In the end, the outcomes of the proposed study were observed also in a real-life case study. The proposed discussion opens new scenarios in the application of these techniques for supporting road management companies and public organizations in the evaluation of the road networks health state. © 2023 Elsevier Ltd","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85151658094"
"Sorayaie Azar A.; Naemi A.; Babaei Rikan S.; Bagherzadeh Mohasefi J.; Pirnejad H.; Wiil U.K.","Sorayaie Azar, Amir (57325214000); Naemi, Amin (57220065053); Babaei Rikan, Samin (57325027000); Bagherzadeh Mohasefi, Jamshid (8935288700); Pirnejad, Habibollah (23498645500); Wiil, Uffe Kock (6701690579)","57325214000; 57220065053; 57325027000; 8935288700; 23498645500; 6701690579","Monkeypox detection using deep neural networks","2023","BMC Infectious Diseases","23","1","438","","","","9","10.1186/s12879-023-08408-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163634067&doi=10.1186%2fs12879-023-08408-4&partnerID=40&md5=96ed04cbcc3a1876cf53cddd89cfa469","Background: In May 2022, the World Health Organization (WHO) European Region announced an atypical Monkeypox epidemic in response to reports of numerous cases in some member countries unrelated to those where the illness is endemic. This issue has raised concerns about the widespread nature of this disease around the world. The experience with Coronavirus Disease 2019 (COVID-19) has increased awareness about pandemics among researchers and health authorities. Methods: Deep Neural Networks (DNNs) have shown promising performance in detecting COVID-19 and predicting its outcomes. As a result, researchers have begun applying similar methods to detect Monkeypox disease. In this study, we utilize a dataset comprising skin images of three diseases: Monkeypox, Chickenpox, Measles, and Normal cases. We develop seven DNN models to identify Monkeypox from these images. Two scenarios of including two classes and four classes are implemented. Results: The results show that our proposed DenseNet201-based architecture has the best performance, with Accuracy = 97.63%, F1-Score = 90.51%, and Area Under Curve (AUC) = 94.27% in two-class scenario; and Accuracy = 95.18%, F1-Score = 89.61%, AUC = 92.06% for four-class scenario. Comparing our study with previous studies with similar scenarios, shows that our proposed model demonstrates superior performance, particularly in terms of the F1-Score metric. For the sake of transparency and explainability, Local Interpretable Model-Agnostic Explanations (LIME) and Gradient-weighted Class Activation Mapping (Grad-Cam) were developed to interpret the results. These techniques aim to provide insights into the decision-making process, thereby increasing the trust of clinicians. Conclusion: The DenseNet201 model outperforms the other models in terms of the confusion metrics, regardless of the scenario. One significant accomplishment of this study is the utilization of LIME and Grad-Cam to identify the affected areas and assess their significance in diagnosing diseases based on skin images. By incorporating these techniques, we enhance our understanding of the infected regions and their relevance in distinguishing Monkeypox from other similar diseases. Our proposed model can serve as a valuable auxiliary tool for diagnosing Monkeypox and distinguishing it from other related conditions. © 2023, The Author(s).","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85163634067"
"Liu X.; Kayser M.; Kushner S.A.; Tiemeier H.; Rivadeneira F.; Jaddoe V.W.V.; Niessen W.J.; Wolvius E.B.; Roshchupkin G.V.","Liu, X. (57225920936); Kayser, M. (26643477000); Kushner, S.A. (7005129416); Tiemeier, H. (58387761500); Rivadeneira, F. (57207897492); Jaddoe, V.W.V. (57221097806); Niessen, W.J. (7006660813); Wolvius, E.B. (57201454707); Roshchupkin, G.V. (57190279096)","57225920936; 26643477000; 7005129416; 58387761500; 57207897492; 57221097806; 7006660813; 57201454707; 57190279096","Association between prenatal alcohol exposure and children's facial shape: a prospective population-based cohort study","2023","Human Reproduction","38","5","","961","972","11","2","10.1093/humrep/dead006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85157991723&doi=10.1093%2fhumrep%2fdead006&partnerID=40&md5=57e26bd4783eaaab4b2b94d058dea973","STUDY QUESTION: Is there an association between low-to-moderate levels of prenatal alcohol exposure (PAE) and children's facial shape? SUMMARY ANSWER: PAE before and during pregnancy, even at low level (<12 g of alcohol per week), was found associated with the facial shape of children, and these associations were found attenuated as children grow older. WHAT IS KNOWN ALREADY: High levels of PAE during pregnancy can have significant adverse associations with a child's health development resulting in recognizably abnormal facial development. STUDY DESIGN, SIZE, DURATION: This study was based on the Generation R Study, a prospective cohort from fetal life onwards with maternal and offspring data. We analyzed children 3-dimensional (3D) facial images taken at ages 9 (n = 3149) and 13 years (n = 2477) together with the data of maternal alcohol consumption. PARTICIPANTS/MATERIALS, SETTING, METHODS: We defined six levels of PAE based on the frequency and dose of alcohol consumption and defined three tiers based on the timing of alcohol exposure of the unborn child. For the image analysis, we used 3D graph convolutional networks for non-linear dimensionality reduction, which compressed the high-dimensional images into 200 traits representing facial morphology. These 200 traits were used for statistical analysis to search for associations with PAE. Finally, we generated heatmaps to display the facial phenotypes associated with PAE. MAIN RESULTS AND THE ROLE OF CHANCE: The results of the linear regression in the 9-year-old children survived correction for multiple testing with false discovery rate (FDR). In Tier 1 where we examined PAE only before pregnancy (exposed N = 278, unexposed N = 760), we found three traits survived FDR correction. The lowest FDR-P is 1.7e-05 (beta = 0.021, SE = 0.0040) in Trait #29; In Tier 2b where we examine any PAE during first trimester (exposed N = 756; unexposed N = 760), we found eight traits survived FDR correction. The lowest FDR-P is 9.0e-03 (beta = -0.013, SE = 0.0033) in Trait #139. Moreover, more statistically significant facial traits were found in higher levels of PAE. No FDR-significant results were found in the 13-year-old children. We map these significant traits back to the face, and found the most common detected facial phenotypes included turned-up nose tip, shortened nose, turned-out chin, and turned-in lower-eyelid-related regions. LIMITATIONS, REASONS FOR CAUTION: We had no data for alcohol consumption more than three months prior to pregnancy and thus do not know if maternal drinking had chronic effects. The self-reported questionnaire might not reflect accurate alcohol measurements because mothers may have denied their alcohol consumption. WIDER IMPLICATIONS OF THE FINDINGS: Our results imply that facial morphology, such as quantified by the approach we proposed here, can be used as a biomarker in further investigations. Furthermore, our study suggests that for women who are pregnant or want to become pregnant soon, should quit alcohol consumption several months before conception and completely during pregnancy to avoid adverse health outcomes in the offspring. STUDY FUNDING/COMPETING INTEREST(S): This work was supported by Erasmus Medical Centre, Rotterdam, the Erasmus University Rotterdam, and the Netherlands Organization for Health Research. V.W.V.J. reports receipt of funding from the Netherlands Organization for Health Research (ZonMw 90700303). W.J.N. is a founder, a scientific lead, and a shareholder of Quantib BV. TRIAL REGISTRATION NUMBER: N/A.  © 2023 The Author(s). Published by Oxford University Press on behalf of European Society of Human Reproduction and Embryology.","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85157991723"
"Du Y.; McNestry C.; Wei L.; Antoniadi A.M.; McAuliffe F.M.; Mooney C.","Du, Yuhan (57220115062); McNestry, Catherine (57460941500); Wei, Lan (57219011267); Antoniadi, Anna Markella (57215383205); McAuliffe, Fionnuala M. (7003459825); Mooney, Catherine (56251250500)","57220115062; 57460941500; 57219011267; 57215383205; 7003459825; 56251250500","Machine learning-based clinical decision support systems for pregnancy care: A systematic review","2023","International Journal of Medical Informatics","173","","105040","","","","8","10.1016/j.ijmedinf.2023.105040","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150364541&doi=10.1016%2fj.ijmedinf.2023.105040&partnerID=40&md5=00102f87c5b7fc352bd66468926dbf6c","Background: Clinical decision support systems (CDSSs) can provide various functions and advantages to healthcare delivery. Quality healthcare during pregnancy and childbirth is of vital importance, and machine learning-based CDSSs have shown positive impact on pregnancy care. Objective: This paper aims to investigate what has been done in CDSSs in the context of pregnancy care using machine learning, and what aspects require attention from future researchers. Methods: We conducted a systematic review of existing literature following a structured process of literature search, paper selection and filtering, and data extraction and synthesis. Results: 17 research papers were identified on the topic of CDSS development for different aspects of pregnancy care using various machine learning algorithms. We discovered an overall lack of explainability in the proposed models. We also observed a lack of experimentation, external validation and discussion around culture, ethnicity and race from the source data, with most studies using data from a single centre or country, and an overall lack of awareness of applicability and generalisability of the CDSSs regarding different populations. Finally, we found a gap between machine learning practices and CDSS implementation, and an overall lack of user testing. Conclusion: Machine learning-based CDSSs are still under-explored in the context of pregnancy care. Despite the open problems that remain, the few studies that tested a CDSS for pregnancy care reported positive effects, reinforcing the potential of such systems to improve clinical practice. We encourage future researchers to take into consideration the aspects we identified in order for their work to translate into clinical use. © 2023 The Author(s)","Review","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85150364541"
"Ahmad K.; Khan M.S.; Ahmed F.; Driss M.; Boulila W.; Alazeb A.; Alsulami M.; Alshehri M.S.; Ghadi Y.Y.; Ahmad J.","Ahmad, Khubab (58611919700); Khan, Muhammad Shahbaz (57223043082); Ahmed, Fawad (9334969400); Driss, Maha (36952645100); Boulila, Wadii (37088273900); Alazeb, Abdulwahab (57210184877); Alsulami, Mohammad (57970172700); Alshehri, Mohammed S. (57210193521); Ghadi, Yazeed Yasin (55797735700); Ahmad, Jawad (56645911000)","58611919700; 57223043082; 9334969400; 36952645100; 37088273900; 57210184877; 57970172700; 57210193521; 55797735700; 56645911000","FireXnet: an explainable AI-based tailored deep learning model for wildfire detection on resource-constrained devices","2023","Fire Ecology","19","1","54","","","","2","10.1186/s42408-023-00216-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171896502&doi=10.1186%2fs42408-023-00216-0&partnerID=40&md5=9a4054b8be93f1cd9e8baa31429cd07f","Background: Forests cover nearly one-third of the Earth’s land and are some of our most biodiverse ecosystems. Due to climate change, these essential habitats are endangered by increasing wildfires. Wildfires are not just a risk to the environment, but they also pose public health risks. Given these issues, there is an indispensable need for efficient and early detection methods. Conventional detection approaches fall short due to spatial limitations and manual feature engineering, which calls for the exploration and development of data-driven deep learning solutions. This paper, in this regard, proposes 'FireXnet', a tailored deep learning model designed for improved efficiency and accuracy in wildfire detection. FireXnet is tailored to have a lightweight architecture that exhibits high accuracy with significantly less training and testing time. It contains considerably reduced trainable and non-trainable parameters, which makes it suitable for resource-constrained devices. To make the FireXnet model visually explainable and trustable, a powerful explainable artificial intelligence (AI) tool, SHAP (SHapley Additive exPlanations) has been incorporated. It interprets FireXnet’s decisions by computing the contribution of each feature to the prediction. Furthermore, the performance of FireXnet is compared against five pre-trained models — VGG16, InceptionResNetV2, InceptionV3, DenseNet201, and MobileNetV2 — to benchmark its efficiency. For a fair comparison, transfer learning and fine-tuning have been applied to the aforementioned models to retrain the models on our dataset. Results: The test accuracy of the proposed FireXnet model is 98.42%, which is greater than all other models used for comparison. Furthermore, results of reliability parameters confirm the model’s reliability, i.e., a confidence interval of [0.97, 1.00] validates the certainty of the proposed model’s estimates and a Cohen’s kappa coefficient of 0.98 proves that decisions of FireXnet are in considerable accordance with the given data. Conclusion: The integration of the robust feature extraction of FireXnet with the transparency of explainable AI using SHAP enhances the model’s interpretability and allows for the identification of key characteristics triggering wildfire detections. Extensive experimentation reveals that in addition to being accurate, FireXnet has reduced computational complexity due to considerably fewer training and non-training parameters and has significantly fewer training and testing times. © 2023, Association for Fire Ecology.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85171896502"
"Ali S.; Akhlaq F.; Imran A.S.; Kastrati Z.; Daudpota S.M.; Moosa M.","Ali, Subhan (58190535600); Akhlaq, Filza (58634067600); Imran, Ali Shariq (56109077100); Kastrati, Zenun (56960506200); Daudpota, Sher Muhammad (57190295678); Moosa, Muhammad (58634299000)","58190535600; 58634067600; 56109077100; 56960506200; 57190295678; 58634299000","The enlightening role of explainable artificial intelligence in medical & healthcare domains: A systematic literature review","2023","Computers in Biology and Medicine","166","","107555","","","","12","10.1016/j.compbiomed.2023.107555","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173530976&doi=10.1016%2fj.compbiomed.2023.107555&partnerID=40&md5=57243ea423a9f0615d62e7e69509f568","In domains such as medical and healthcare, the interpretability and explainability of machine learning and artificial intelligence systems are crucial for building trust in their results. Errors caused by these systems, such as incorrect diagnoses or treatments, can have severe and even life-threatening consequences for patients. To address this issue, Explainable Artificial Intelligence (XAI) has emerged as a popular area of research, focused on understanding the black-box nature of complex and hard-to-interpret machine learning models. While humans can increase the accuracy of these models through technical expertise, understanding how these models actually function during training can be difficult or even impossible. XAI algorithms such as Local Interpretable Model-Agnostic Explanations (LIME) and SHapley Additive exPlanations (SHAP) can provide explanations for these models, improving trust in their predictions by providing feature importance and increasing confidence in the systems. Many articles have been published that propose solutions to medical problems by using machine learning models alongside XAI algorithms to provide interpretability and explainability. In our study, we identified 454 articles published from 2018–2022 and analyzed 93 of them to explore the use of these techniques in the medical domain. © 2023 The Author(s)","Review","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85173530976"
"Suhasini S.; Tatini N.B.; Arslan F.; Bansal S.K.; Babu S.; Umaralievich M.S.","Suhasini, S. (58529564700); Tatini, Narendra Babu (57658931100); Arslan, Farrukh (55270484900); Bansal, Sushil Kumar (58587779600); Babu, Suresh (58413437000); Umaralievich, Mekhmonov Sultonali (58515336300)","58529564700; 57658931100; 55270484900; 58587779600; 58413437000; 58515336300","Smart explainable artificial intelligence for sustainable secure healthcare application based on quantum optical neural network","2023","Optical and Quantum Electronics","55","10","887","","","","2","10.1007/s11082-023-05155-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165542749&doi=10.1007%2fs11082-023-05155-3&partnerID=40&md5=221a383e11e558384725c47c1935fb8f","Managing expanding urbanisation, energy use, environmental preservation, citizen economic and living standards, and people's ability to effectively use and adopt modern information and communication technology (ICT) are all objectives of smart cities. A branch of machine intelligence engineering known as explaining artificial intelligence (XAI) makes complex techniques approachable and adaptable for efficient decision-making in the sciences and technologies. The quantum uncertainty problem may be applied to the network state, which consists of several states and dimensions and requires real-time information. Specifically pertinent are the linkages between the emerging paradigms of machine learning (ML), quantum computing (QC), and quantum machine learning (QML), and traditional communication networks. This study provides a new method in explainable deep learning for analysing healthcare data in multimedia for long-term quantum photonic applications. Input has been collected from multimedia healthcare data and processed for noise removal and normalization. Processed data features have been extracted using a gradient quantum neural network, and classification is carried out using an attention-based graph neural network (GNN). The experimental analysis is carried out in terms of accuracy, precision, recall, F-1 score, NSE (normalized square error), MAP (mean average precision),and Jaccard index.The proposed technique attained an accuracy of 93%, precision of 81%, recall of 79%, F-1 score of 71%, NSE of 65%, MAP of 58%, and Jaccard index of 53%. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Article","Final","","Scopus","2-s2.0-85165542749"
"Dong Z.; Wang J.; Li Y.; Deng Y.; Zhou W.; Zeng X.; Gong D.; Liu J.; Pan J.; Shang R.; Xu Y.; Xu M.; Zhang L.; Zhang M.; Tao X.; Zhu Y.; Du H.; Lu Z.; Yao L.; Wu L.; Yu H.","Dong, Zehua (57219774258); Wang, Junxiao (58018190800); Li, Yanxia (56392436400); Deng, Yunchao (57109635900); Zhou, Wei (57198624097); Zeng, Xiaoquan (58017087100); Gong, Dexin (57207781179); Liu, Jun (57196295810); Pan, Jie (57207304123); Shang, Renduo (57219979201); Xu, Youming (57215496299); Xu, Ming (57796247000); Zhang, Lihui (57215025612); Zhang, Mengjiao (57207770546); Tao, Xiao (57277238800); Zhu, Yijie (57213267205); Du, Hongliu (57372542600); Lu, Zihua (57218632638); Yao, Liwen (57209986966); Wu, Lianlian (57201900744); Yu, Honggang (56208754900)","57219774258; 58018190800; 56392436400; 57109635900; 57198624097; 58017087100; 57207781179; 57196295810; 57207304123; 57219979201; 57215496299; 57796247000; 57215025612; 57207770546; 57277238800; 57213267205; 57372542600; 57218632638; 57209986966; 57201900744; 56208754900","Explainable artificial intelligence incorporated with domain knowledge diagnosing early gastric neoplasms under white light endoscopy","2023","npj Digital Medicine","6","1","64","","","","5","10.1038/s41746-023-00813-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153111547&doi=10.1038%2fs41746-023-00813-y&partnerID=40&md5=79a2693a082bc8ed942c21b82b437df8","White light endoscopy is the most pivotal tool for detecting early gastric neoplasms. Previous artificial intelligence (AI) systems were primarily unexplainable, affecting their clinical credibility and acceptability. We aimed to develop an explainable AI named ENDOANGEL-ED (explainable diagnosis) to solve this problem. A total of 4482 images and 296 videos with focal lesions from 3279 patients from eight hospitals were used for training, validating, and testing ENDOANGEL-ED. A traditional sole deep learning (DL) model was trained using the same dataset. The performance of ENDOANGEL-ED and sole DL was evaluated on six levels: internal and external images, internal and external videos, consecutive videos, and man–machine comparison with 77 endoscopists in videos. Furthermore, a multi-reader, multi-case study was conducted to evaluate the ENDOANGEL-ED’s effectiveness. A scale was used to compare the overall acceptance of endoscopists to traditional and explainable AI systems. The ENDOANGEL-ED showed high performance in the image and video tests. In man–machine comparison, the accuracy of ENDOANGEL-ED was significantly higher than that of all endoscopists in internal (81.10% vs. 70.61%, p < 0.001) and external videos (88.24% vs. 78.49%, p < 0.001). With ENDOANGEL-ED’s assistance, the accuracy of endoscopists significantly improved (70.61% vs. 79.63%, p < 0.001). Compared with the traditional AI, the explainable AI increased the endoscopists’ trust and acceptance (4.42 vs. 3.74, p < 0.001; 4.52 vs. 4.00, p < 0.001). In conclusion, we developed a real-time explainable AI that showed high performance, higher clinical credibility, and acceptance than traditional DL models and greatly improved the diagnostic ability of endoscopists. © 2023, The Author(s).","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85153111547"
"Van Den Berg M.; Kuiper O.; Van Der Haas Y.; Gerlings J.; Sent D.; Leijnen S.","Van Den Berg, Martin (7201697066); Kuiper, Ouren (57197795607); Van Der Haas, Yvette (58590030000); Gerlings, Julie (57221693279); Sent, Danielle (6505796861); Leijnen, Stefan (57218950781)","7201697066; 57197795607; 58590030000; 57221693279; 6505796861; 57218950781","A Conceptual Model for Implementing Explainable AI by Design: Results of an Empirical Study","2023","Frontiers in Artificial Intelligence and Applications","368","","","60","73","13","0","10.3233/FAIA230075","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171485972&doi=10.3233%2fFAIA230075&partnerID=40&md5=335b81ee2d78bda1de6e36f3b2e1e9c7","Artificial Intelligence (AI) offers organizations unprecedented opportunities. However, one of the risks of using AI is that its outcomes and inner workings are not intelligible. In industries where trust is critical, such as healthcare and finance, explainable AI (XAI) is a necessity. However, the implementation of XAI is not straightforward, as it requires addressing both technical and social aspects. Previous studies on XAI primarily focused on either technical or social aspects and lacked a practical perspective. This study aims to empirically examine the XAI related aspects faced by developers, users, and managers of AI systems during the development process of the AI system. To this end, a multiple case study was conducted in two Dutch financial services companies using four use cases. Our findings reveal a wide range of aspects that must be considered during XAI implementation, which we grouped and integrated into a conceptual model. This model helps practitioners to make informed decisions when developing XAI. We argue that the diversity of aspects to consider necessitates an XAI 'by design' approach, especially in high-risk use cases in industries where the stakes are high such as finance, public services, and healthcare. As such, the conceptual model offers a taxonomy for method engineering of XAI related methods, techniques, and tools. © 2023 The Authors.","Conference paper","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85171485972"
"Guldogan E.; Yagin F.H.; Pinar A.; Colak C.; Kadry S.; Kim J.","Guldogan, Emek (57193501488); Yagin, Fatma Hilal (57211715604); Pinar, Abdulvahap (58763503200); Colak, Cemil (11738942300); Kadry, Seifedine (55906598300); Kim, Jungeun (56600264800)","57193501488; 57211715604; 58763503200; 11738942300; 55906598300; 56600264800","A proposed tree-based explainable artificial intelligence approach for the prediction of angina pectoris","2023","Scientific Reports","13","1","22189","","","","2","10.1038/s41598-023-49673-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179667134&doi=10.1038%2fs41598-023-49673-2&partnerID=40&md5=b6f92b39f7cc4c2d59c88c4cf197179b","Cardiovascular diseases (CVDs) are a serious public health issue that affects and is responsible for numerous fatalities and impairments. Ischemic heart disease (IHD) is one of the most prevalent and deadliest types of CVDs and is responsible for 45% of all CVD-related fatalities. IHD occurs when the blood supply to the heart is reduced due to narrowed or blocked arteries, which causes angina pectoris (AP) chest pain. AP is a common symptom of IHD and can indicate a higher risk of heart attack or sudden cardiac death. Therefore, it is important to diagnose and treat AP promptly and effectively. To forecast AP in women, we constructed a novel artificial intelligence (AI) method employing the tree-based algorithm known as an Explainable Boosting Machine (EBM). EBM is a machine learning (ML) technique that combines the interpretability of linear models with the flexibility and accuracy of gradient boosting. We applied EBM to a dataset of 200 female patients, 100 with AP and 100 without AP, and extracted the most relevant features for AP prediction. We then evaluated the performance of EBM against other AI methods, such as Logistic Regression (LR), Categorical Boosting (CatBoost), eXtreme Gradient Boosting (XGBoost), Adaptive Boosting (AdaBoost), and Light Gradient Boosting Machine (LightGBM). We found that EBM was the most accurate and well-balanced technique for forecasting AP, with accuracy (0.925) and Youden's index (0.960). We also looked at the global and local explanations provided by EBM to better understand how each feature affected the prediction and how each patient was classified. Our research showed that EBM is a useful AI method for predicting AP in women and identifying the risk factors related to it. This can help clinicians to provide personalized and evidence-based care for female patients with AP. © 2023, The Author(s).","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85179667134"
"Chan M.; Gazi A.H.; Aydemir V.B.; Soliman M.; Ozmen G.C.; Richardson K.L.; Abdallah C.A.; Nikbakht M.; Nichols C.; Inan O.T.","Chan, Michael (57221910722); Gazi, Asim H. (57216520583); Aydemir, Varol B. (57201498220); Soliman, Moamen (57214916021); Ozmen, Goktug C. (57221458569); Richardson, Kristine L. (58252836200); Abdallah, Calvin A. (57403470600); Nikbakht, Mohammad (57986449800); Nichols, Christopher (57406192600); Inan, Omer T. (15753969900)","57221910722; 57216520583; 57201498220; 57214916021; 57221458569; 58252836200; 57403470600; 57986449800; 57406192600; 15753969900","Respiratory Rate Estimation during Walking Using a Wearable Patch with Modality Attentive Fusion","2023","IEEE Sensors Journal","23","23","","29831","29843","12","0","10.1109/JSEN.2023.3324931","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176330630&doi=10.1109%2fJSEN.2023.3324931&partnerID=40&md5=62c9e4317962d14d7544ce7ef43bbb8d","Respiratory rate (RR) is an important vital sign to monitor outside the clinic, particularly during physiological challenges such as exercise; unfortunately, ambulatory measurement devices for RR are typically obtrusive and inaccurate. The objective of this work is to allow for accurate and robust RR monitoring with a convenient and small chest-worn wearable patch during walking and exercise recovery periods. Methods: To estimate RR from the wearable patch, respiratory signals were first extracted from electrocardiogram (ECG), photoplethysmogram (PPG), and seismocardiogram (SCG) signals. The optimal channel in each signal was adaptively selected using the respiratory quality index based on fast Fourier transform (RQIFFT). Next, we proposed modality attentive (MA) fusion-which merged spectral-Temporal information from different modalities-to address motion artifacts during walking. The fused output was subsequently denoised using a U-Net-based deep learning model and used for final estimation. A dataset of {N} = 17 subjects was collected to validate the RR estimated during three types of activities: stationary activities, walking (including 6-minute walk test), and running. Major results: Combining and denoising ECG and PPG data using MA fusion and the U-Net achieved the lowest mean absolute error (MAE) (2.21 breaths per minute [brpm]) during walking. After rejecting a small portion of the data (coverage = 84.43%) using RQIFFT, this error was further reduced to 1.59 brpm, which was comparable to the state-of-The-Art methods. Conclusion: Applying adaptive channel selection, MA fusion, and U-Net denoising achieved accurate RR estimation from a small chest-worn wearable patch. Significance: This work can enable cardiopulmonary monitoring applications in less controlled settings.  © 2001-2012 IEEE.","Article","Final","","Scopus","2-s2.0-85176330630"
"Thomas A.W.; Ré C.; Poldrack R.A.","Thomas, Armin W. (57208344290); Ré, Christopher (10739281400); Poldrack, Russell A. (7004739390)","57208344290; 10739281400; 7004739390","Benchmarking explanation methods for mental state decoding with deep learning models","2023","NeuroImage","273","","120109","","","","2","10.1016/j.neuroimage.2023.120109","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152626798&doi=10.1016%2fj.neuroimage.2023.120109&partnerID=40&md5=408b8f4b7e2282edbf963a344f76e8ec","Deep learning (DL) models find increasing application in mental state decoding, where researchers seek to understand the mapping between mental states (e.g., experiencing anger or joy) and brain activity by identifying those spatial and temporal features of brain activity that allow to accurately identify (i.e., decode) these states. Once a DL model has been trained to accurately decode a set of mental states, neuroimaging researchers often make use of methods from explainable artificial intelligence research to understand the model's learned mappings between mental states and brain activity. Here, we benchmark prominent explanation methods in a mental state decoding analysis of multiple functional Magnetic Resonance Imaging (fMRI) datasets. Our findings demonstrate a gradient between two key characteristics of an explanation in mental state decoding, namely, its faithfulness and its alignment with other empirical evidence on the mapping between brain activity and decoded mental state: explanation methods with high explanation faithfulness, which capture the model's decision process well, generally provide explanations that align less well with other empirical evidence than the explanations of methods with less faithfulness. Based on our findings, we provide guidance for neuroimaging researchers on how to choose an explanation method to gain insight into the mental state decoding decisions of DL models. © 2023","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85152626798"
"Gupta S.; Patanwadia R.; Kalkotwar P.; Mangrulkar R.","Gupta, Simrn (57226094650); Patanwadia, Rahul (57331644200); Kalkotwar, Parth (57348159900); Mangrulkar, Ramchandra (55401536500)","57226094650; 57331644200; 57348159900; 55401536500","Applications of artificial intelligence for employees' health and safety: Present and future","2023","Semantic Intelligent Computing and Applications","","","","65","86","21","0","10.1515/9783110781663-004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181406646&doi=10.1515%2f9783110781663-004&partnerID=40&md5=66514990c878234dc0cbdda3326665aa","The rapid development in Industry 4.0 era has led to an exponential increase in machines of high potency which are essential for economic growth, thereby increasing the risk for employees. An unsafe and unhealthy work environment can impede the innovation and productivity of employees, which may result in damaging the reputation of the industries. Therefore, there is a need to give a serious thought to these unaddressed issues which may be useful in the safety of our employees. Machine learning (ML) has been pervasively applied across disparate disciplines, especially in industries, and has been found to be significantly efficacious in improving the performance. The capabilities of ML and artificial intelligence (AI) can be exploited to offset the shortcomings of traditional approaches to employees' safety. This chapter tries to address this issue. Recent advances in this domain are assessed that use ML and AI to address, resolve, and ameliorate the safety concerns of employees. Various applications using supervised learning, semisupervised learning, unsupervised learning, and reinforcement learning methodologies have been delineated. Further, detailed reasoning on why semisupervised learning methodology is preferred currently, primarily due to its high throughput, and the advantage of generating warnings with minimal requirement of historical data has been discussed. Complex deep learning models have become feasible, owing to the easy access to fast modern computers. Of the diverse implementations of neural networks used, this comes at the cost of blackbox models generated in deep learning techniques that might reduce the trust to effectuate such technologies. This treatise proposes explainable artificial intelligence (XAI) as a potential solution for this problem, which can be used to augment employees' trust in these black-box models by providing a lucid explanation of the model's working. In totality, this exposition maintains that these ML and deep learning techniques are effective in enhancing workplace health, safety, and environment, as their competence to automate reasoning can culminate in better-informed decisions and effective accident prevention. © 2024 Walter de Gruyter GmbH, Berlin/Boston.","Book chapter","Final","","Scopus","2-s2.0-85181406646"
"Cordeiro J.R.; Mosca S.; Correia-Costa A.; Ferreira C.; Pimenta J.; Correia-Costa L.; Barros H.; Postolache O.","Cordeiro, João Rala (57205283529); Mosca, Sara (57221998211); Correia-Costa, Ana (57189440697); Ferreira, Cátia (58668383300); Pimenta, Joana (7005402243); Correia-Costa, Liane (56766098900); Barros, Henrique (7005434514); Postolache, Octavian (6701457335)","57205283529; 57221998211; 57189440697; 58668383300; 7005402243; 56766098900; 7005434514; 6701457335","The Association between Childhood Obesity and Cardiovascular Changes in 10 Years Using Special Data Science Analysis","2023","Children","10","10","1655","","","","0","10.3390/children10101655","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175083893&doi=10.3390%2fchildren10101655&partnerID=40&md5=a6d80c6d8ed9f1ccdb16c7edfcad7d61","The increasing prevalence of overweight and obesity is a worldwide problem, with several well-known consequences that might start to develop early in life during childhood. The present research based on data from children that have been followed since birth in a previously established cohort study (Generation XXI, Porto, Portugal), taking advantage of State-of-the-Art (SoA) data science techniques and methods, including Neural Architecture Search (NAS), explainable Artificial Intelligence (XAI), and Deep Learning (DL), aimed to explore the hidden value of data, namely on electrocardiogram (ECG) records performed during follow-up visits. The combination of these techniques allowed us to clarify subtle cardiovascular changes already present at 10 years of age, which are evident from ECG analysis and probably induced by the presence of obesity. The proposed novel combination of new methodologies and techniques is discussed, as well as their applicability in other health domains. © 2023 by the authors.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85175083893"
"Ledziński Ł.; Grześk G.","Ledziński, Łukasz (57224998793); Grześk, Grzegorz (6604017312)","57224998793; 6604017312","Artificial Intelligence Technologies in Cardiology","2023","Journal of Cardiovascular Development and Disease","10","5","202","","","","4","10.3390/jcdd10050202","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160219616&doi=10.3390%2fjcdd10050202&partnerID=40&md5=4e2a34d38d64960e572034a6bb37723d","As the world produces exabytes of data, there is a growing need to find new methods that are more suitable for dealing with complex datasets. Artificial intelligence (AI) has significant potential to impact the healthcare industry, which is already on the road to change with the digital transformation of vast quantities of information. The implementation of AI has already achieved success in the domains of molecular chemistry and drug discoveries. The reduction in costs and in the time needed for experiments to predict the pharmacological activities of new molecules is a milestone in science. These successful applications of AI algorithms provide hope for a revolution in healthcare systems. A significant part of artificial intelligence is machine learning (ML), of which there are three main types—supervised learning, unsupervised learning, and reinforcement learning. In this review, the full scope of the AI workflow is presented, with explanations of the most-often-used ML algorithms and descriptions of performance metrics for both regression and classification. A brief introduction to explainable artificial intelligence (XAI) is provided, with examples of technologies that have developed for XAI. We review important AI implementations in cardiology for supervised, unsupervised, and reinforcement learning and natural language processing, emphasizing the used algorithm. Finally, we discuss the need to establish legal, ethical, and methodical requirements for the deployment of AI models in medicine. © 2023 by the authors.","Review","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85160219616"
"Topp S.N.; Barclay J.; Diaz J.; Sun A.Y.; Jia X.; Lu D.; Sadler J.M.; Appling A.P.","Topp, Simon N. (57211094684); Barclay, Janet (57193822368); Diaz, Jeremy (57210132141); Sun, Alexander Y. (9279316500); Jia, Xiaowei (56589359200); Lu, Dan (36004878500); Sadler, Jeffrey M. (56425375100); Appling, Alison P. (56333545100)","57211094684; 57193822368; 57210132141; 9279316500; 56589359200; 36004878500; 56425375100; 56333545100","Stream Temperature Prediction in a Shifting Environment: Explaining the Influence of Deep Learning Architecture","2023","Water Resources Research","59","4","e2022WR033880","","","","4","10.1029/2022WR033880","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153854176&doi=10.1029%2f2022WR033880&partnerID=40&md5=64be16262b3a65b48aff0cae076f0397","Stream temperature is a fundamental control on ecosystem health. Recent efforts incorporating process guidance into deep learning models for predicting stream temperature have been shown to outperform existing statistical and physical models. This performance is in part because deep learning architectures can actively learn spatiotemporal relationships that govern how water and energy propagate through a river network. However, exploration of how spatiotemporal awareness and process guidance influence a model's generalizability under shifting environmental conditions such as climate change is limited. Here, we use Explainable Artificial Intelligence (XAI) to interrogate how differing deep learning architectures affect a model's learned spatial and temporal dependencies, and how those learned dependencies affect a model's ability to maintain high accuracy when applied to unseen environmental conditions. Using the Delaware River Basin in the northeastern United States as a test case, we compare two spatiotemporally aware process-guided deep learning models for predicting stream temperature (a recurrent graph convolution network—RGCN, and a temporal convolution graph model—Graph WaveNet). Both models achieve equally high predictive performance when testing data are well represented in the training data (test root mean squared errors of 1.64°C and 1.65°C); however, Graph WaveNet significantly outperforms RGCN in 4 out of 5 experiments where test partitions represent different types of unseen environmental conditions. XAI results show that the architecture of Graph WaveNet leads to learned spatial relationships with greater fidelity to physical processes, and that this fidelity improves the generalizability of the model when applied to shifting and/or unseen environmental conditions. © 2023 The Authors. This article has been contributed to by U.S. Government employees and their work is in the public domain in the USA.","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85153854176"
"Abba S.I.; Yassin M.A.; Mubarak A.S.; Shah S.M.H.; Usman J.; Oudah A.Y.; Naganna S.R.; Aljundi I.H.","Abba, Sani I. (57208942739); Yassin, Mohamed A. (56369582400); Mubarak, Auwalu Saleh (57218449342); Shah, Syed Muzzamil Hussain (55694711700); Usman, Jamilu (58530093700); Oudah, Atheer Y. (57210341575); Naganna, Sujay Raghavendra (57208242933); Aljundi, Isam H. (8612157800)","57208942739; 56369582400; 57218449342; 55694711700; 58530093700; 57210341575; 57208242933; 8612157800","Drinking Water Resources Suitability Assessment Based on Pollution Index of Groundwater Using Improved Explainable Artificial Intelligence","2023","Sustainability (Switzerland)","15","21","15655","","","","3","10.3390/su152115655","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178168974&doi=10.3390%2fsu152115655&partnerID=40&md5=3ee2a3567f2d418841a56ea7410e4a9d","The global significance of fluoride and nitrate contamination in coastal areas cannot be overstated, as these contaminants pose critical environmental and public health challenges across the world. Water quality is an essential component in sustaining environmental health. This integrated study aimed to assess indexical and spatial water quality, potential contamination sources, and health risks associated with groundwater resources in Al-Hassa, Saudi Arabia. Groundwater samples were tested using standard methods. The physiochemical results indicated overall groundwater pollution. This study addresses the critical issue of drinking water resource suitability assessment by introducing an innovative approach based on the pollution index of groundwater (PIG). Focusing on the eastern region of Saudi Arabia, where water resource management is of paramount importance, we employed advanced machine learning (ML) models to forecast groundwater suitability using several combinations (C1 = EC + Na + Mg + Cl, C2 = TDS + TA + HCO3 + K + Ca, and C3 = SO4 + pH + NO3 + F + Turb). Six ML models, including random forest (RF), decision trees (DT), XgBoost, CatBoost, linear regression, and support vector machines (SVM), were utilized to predict groundwater quality. These models, based on several performance criteria (MAPE, MAE, MSE, and DC), offer valuable insights into the complex relationships governing groundwater pollution with an accuracy of more than 90%. To enhance the transparency and interpretability of the ML models, we incorporated the local interpretable model-agnostic explanation method, SHapley Additive exPlanations (SHAP). SHAP allows us to interpret the prediction-making process of otherwise opaque black-box models. We believe that the integration of ML models and SHAP-based explainability offers a promising avenue for sustainable water resource management in Saudi Arabia and can serve as a model for addressing similar challenges worldwide. By bridging the gap between complex data-driven predictions and actionable insights, this study contributes to the advancement of environmental stewardship and water security in the region. © 2023 by the authors.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85178168974"
"Lee K.-S.; Song I.-S.; Kim E.S.; Kim J.; Jung S.; Nam S.; Ahn K.H.","Lee, Kwang-Sig (57221177656); Song, In-Seok (56727430400); Kim, Eun Sun (56308204100); Kim, Jisu (58794010700); Jung, Sohee (58794492500); Nam, Sunwoo (58794733100); Ahn, Ki Hoon (26031248300)","57221177656; 56727430400; 56308204100; 58794010700; 58794492500; 58794733100; 26031248300","Machine learning analysis with population data for the associations of preterm birth with temporomandibular disorder and gastrointestinal diseases","2024","PLoS ONE","19","1 January","e0296329","","","","0","10.1371/journal.pone.0296329","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181472218&doi=10.1371%2fjournal.pone.0296329&partnerID=40&md5=22e9a1a02c75793195075f65dbf4f354","This study employs machine learning analysis with population data for the associations of preterm birth (PTB) with temporomandibular disorder (TMD) and gastrointestinal diseases. The source of the population-based retrospective cohort was Korea National Health Insurance claims for 489,893 primiparous women with delivery at the age of 25–40 in 2017. The dependent variable was PTB in 2017. Twenty-one predictors were included, i.e., demographic, socioeconomic, disease and medication information during 2002–2016. Random forest variable importance was derived for finding important predictors of PTB and evaluating its associations with the predictors including TMD and gastroesophageal reflux disease (GERD). Shapley Additive Explanation (SHAP) values were calculated to analyze the directions of these associations. The random forest with oversampling registered a much higher area under the receiver-operating-characteristic curve compared to logistic regression with oversampling, i.e., 79.3% vs. 53.1%. According to random forest variable importance values and rankings, PTB has strong associations with low socioeconomic status, GERD, age, infertility, irritable bowel syndrome, diabetes, TMD, salivary gland disease, hypertension, tricyclic antidepressant and benzodiazepine. In terms of max SHAP values, these associations were positive, e.g., low socioeconomic status (0.29), age (0.21), GERD (0.27) and TMD (0.23). The inclusion of low socioeconomic status, age, GERD or TMD into the random forest will increase the probability of PTB by 0.29, 0.21, 0.27 or 0.23. A cutting-edge approach of explainable artificial intelligence highlights the strong associations of preterm birth with temporomandibular disorder, gastrointestinal diseases and antidepressant medication. Close surveillance is needed for pregnant women regarding these multiple risks at the same time. © 2024 Public Library of Science. All rights reserved.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85181472218"
"Wang Y.-C.; Chen T.-C.T.; Chiu M.-C.","Wang, Yu-Cheng (54415093100); Chen, Tin-Chih Toly (8298017700); Chiu, Min-Chi (37076950000)","54415093100; 8298017700; 37076950000","An improved explainable artificial intelligence tool in healthcare for hospital recommendation","2023","Healthcare Analytics","3","","100147","","","","10","10.1016/j.health.2023.100147","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149184092&doi=10.1016%2fj.health.2023.100147&partnerID=40&md5=f91ace99c8fc2c0eb7c75444f52e49a7","Artificial intelligence (AI) technologies have been widely applied in medicine and healthcare. Explainable AI (XAI) has been proposed to make AI applications more transparent and efficient. This study applies some simple cross-domain tools and techniques, including common expression (with linguistic terms), color management, traceable aggregation, and segmented distance diagrams, among others, to improve the explainability of AI applications in healthcare. Four applications of AI technologies in hospitals were used, and recommendations were studied to illustrate the applicability of the proposed methodology. The explainability of each AI application was evaluated before and after improvement for comparison. According to the experimental results, these AI-based hospital recommendation methods could be better explained by modifying their explanations using simple and cross-domain tools. © 2023 The Author(s)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85149184092"
"He C.; Ren J.; Liu W.","He, Cheng (57564868600); Ren, Jia (29567515500); Liu, Wenjian (57215504683)","57564868600; 29567515500; 57215504683","Dynamic Causal Modeling and Online Collaborative Forecasting of Air Quality in Hong Kong and Macao","2023","Entropy","25","9","1337","","","","1","10.3390/e25091337","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172195243&doi=10.3390%2fe25091337&partnerID=40&md5=6eecbf2e2294018fad5d8387f8530df5","The Hong Kong and Macao Special Administrative Regions, situated within China’s Guangdong–Hong Kong–Macao Greater Bay Area, significantly influence and are impacted by their air quality conditions. Rapid urbanization, high population density, and air pollution from diverse factors present challenges, making the health of the atmospheric environment in these regions a research focal point. This study offers three key contributions: (1) It applied an interpretable dynamic Bayesian network (DBN) to construct a dynamic causal model of air quality in Hong Kong and Macao, amidst complex, unstable, multi-dimensional, and uncertain factors over time. (2) It investigated the dynamic interaction between meteorology and air quality sub-networks, and both qualitatively and quantitatively identified, evaluated, and understood the causal relationships between air pollutants and their determinants. (3) It facilitated an online collaborative forecast of air pollutant concentrations, enabling pollution warnings. The findings proposed that a DBN-based dynamic causal model can effectively explain and manage complex atmospheric environmental systems in Hong Kong and Macao. This method offers crucial insights for decision-making and the management of atmospheric environments not only in these regions but also for neighboring cities and regions with similar geographical contexts. © 2023 by the authors.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85172195243"
"Ubi G.M.; Eyogor E.N.; Etta H.E.; Okon N.D.; Ekeng E.B.; Essien I.S.","Ubi, Godwin M. (57214818007); Eyogor, Edu N. (58452252700); Etta, Hannah E. (35098516200); Okon, Nkese D. (57226083237); Ekeng, Effiom B. (58454362400); Essien, Imabong S. (57695133200)","57214818007; 58452252700; 35098516200; 57226083237; 58454362400; 57695133200","Explainable artificial intelligence in drug discovery for biomedical applications","2023","Explainable Artificial Intelligence for Biomedical Applications","","","","309","335","26","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164020062&partnerID=40&md5=b8bc9099a76b6b03fac1e4aee5bcccfd","This chapter reveals the ease with which artificial intelligence (computer-based simulations and algorithms) can be applied in the discovery, identification, and explanation of carfentanil, rispiridine, naltrexone, buprenorphine, naloxone, and morphine as having the same biomedical applications and effects on CYP206 and OPRM2 major genes and CHRNA7, SLC5A4, and OPED7 minor genes as alternative drugs for Tramadol in biomedical applications. It also revealed that artificial intelligence can be used to discover and explain that omega-3-fatty acid, 1,1-dimethetyl, rimonabant, 1-naphthalenyl, HU-210, 2-arachidonyl and anandamide are alternative drugs with same biomedical applications and effects as Cannabinol drug on CD5 major and minor CNR1and CNR2 genes. It was also discovered that Verdanafil and Tadalafil have same and similar biological applications and effect on CYP25 OPRD1, and HTR7 major genes and PRKG2 and CAVI minor genes and are discovered as alternative drugs to Sildenafil drug. The study revealed that GDP-mannose, uridine phosphate, and guanosine diphosphate possess same and similar biological applications and effect on DNAI2 major gene and EPGN and ALGI1 minor genes and are discovered as alternative drugs to the Praziquantel drug used as anthelminthic drug in biomedical applications. Such discovered drugs can be properly harnessed, reconstituted, redesigned, refocused, and utilized as potential prophylactic or therapeutic drug (single or in combinations) for the prevention and treatment of diseases, which is a cardinal focus and target for the pharmaceutical industries. Hence, a computational-based algorithm that utilizes bioinformatics tools will therefore provide the much needed validation approach for the discovery and provides better explanations for the applications of small molecules as potential drugs for the prevention and cure of most health-related ailments and biomedical applications. © 2023 River Publishers. All rights reserved.","Book chapter","Final","","Scopus","2-s2.0-85164020062"
"Elmannai H.; El-Rashidy N.; Mashal I.; Alohali M.A.; Farag S.; El-Sappagh S.; Saleh H.","Elmannai, Hela (55366208300); El-Rashidy, Nora (57211502814); Mashal, Ibrahim (56166273200); Alohali, Manal Abdullah (57201747174); Farag, Sara (58203446100); El-Sappagh, Shaker (55233800700); Saleh, Hager (57218799328)","55366208300; 57211502814; 56166273200; 57201747174; 58203446100; 55233800700; 57218799328","Polycystic Ovary Syndrome Detection Machine Learning Model Based on Optimized Feature Selection and Explainable Artificial Intelligence","2023","Diagnostics","13","8","1506","","","","8","10.3390/diagnostics13081506","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153937687&doi=10.3390%2fdiagnostics13081506&partnerID=40&md5=1a4ef3253f8be035e791bd425846b640","Polycystic ovary syndrome (PCOS) has been classified as a severe health problem common among women globally. Early detection and treatment of PCOS reduce the possibility of long-term complications, such as increasing the chances of developing type 2 diabetes and gestational diabetes. Therefore, effective and early PCOS diagnosis will help the healthcare systems to reduce the disease’s problems and complications. Machine learning (ML) and ensemble learning have recently shown promising results in medical diagnostics. The main goal of our research is to provide model explanations to ensure efficiency, effectiveness, and trust in the developed model through local and global explanations. Feature selection methods with different types of ML models (logistic regression (LR), random forest (RF), decision tree (DT), naive Bayes (NB), support vector machine (SVM), k-nearest neighbor (KNN), xgboost, and Adaboost algorithm to get optimal feature selection and best model. Stacking ML models that combine the best base ML models with meta-learner are proposed to improve performance. Bayesian optimization is used to optimize ML models. Combining SMOTE (Synthetic Minority Oversampling Techniques) and ENN (Edited Nearest Neighbour) solves the class imbalance. The experimental results were made using a benchmark PCOS dataset with two ratios splitting 70:30 and 80:20. The result showed that the Stacking ML with REF feature selection recorded the highest accuracy at 100 compared to other models. © 2023 by the authors.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85153937687"
"Biedma-Rdguez C.; Gacto M.J.; Anguita-Ruiz A.; Alcalá R.; Aguilera C.M.; Alcalá-Fdez J.","Biedma-Rdguez, Carmen (57254099200); Gacto, María José (14028243200); Anguita-Ruiz, Augusto (57202949623); Alcalá, Rafael (35239878000); Aguilera, Concepción María (7005448693); Alcalá-Fdez, Jesús (8696969400)","57254099200; 14028243200; 57202949623; 35239878000; 7005448693; 8696969400","Learning positive-negative rule-based fuzzy associative classifiers with a good trade-off between complexity and accuracy","2023","Fuzzy Sets and Systems","465","","108511","","","","0","10.1016/j.fss.2023.03.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151650017&doi=10.1016%2fj.fss.2023.03.014&partnerID=40&md5=fa8850cab6db483bd55277bf6691a8d4","Nowadays, the call for transparency in Artificial Intelligence models is growing due to the need to understand how decisions derived from the methods are made when they ultimately affect human life and health. Fuzzy Rule-Based Classification Systems have been used successfully as they are models that are easily understood by models themselves. However, complex search spaces hinder the learning process, and in most cases, lead to problems of complexity (coverage and specificity). This problem directly affects the intention to use them to enable the user to analyze and understand the model. Because of this, we propose a fuzzy associative classification method to learn classifiers with an improved trade-off between accuracy and complexity. This method learns the most appropriate granularity of each variable to generate a set of simple fuzzy association rules with a reduced number of associations that consider positive and negative dependencies to be able to classify an instance depending on the presence or absence of certain items. The proposal also chooses the most interesting rules based on several interesting measures and finally performs a genetic rule selection and adjustment to reach the most suitable context of the selected rule set. The quality of our proposal has been analyzed using 23 real-world datasets, comparing them with other proposals by applying statistical analysis. Moreover, the study carried out on a real biomedical research problem of childhood obesity shows the improved trade-off between the accuracy and complexity of the models generated by our proposal. © 2023 The Author(s)","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85151650017"
"Nkengue M.J.; Zeng X.; Koehl L.; Tao X.","Nkengue, Marc Junior (57227976100); Zeng, Xianyi (55512876800); Koehl, Ludovic (17434471300); Tao, Xuyuan (15840482500)","57227976100; 55512876800; 17434471300; 15840482500","X-RCRNet: An explainable deep-learning network for COVID-19 detection using ECG beat signals","2024","Biomedical Signal Processing and Control","87","","105424","","","","0","10.1016/j.bspc.2023.105424","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171610916&doi=10.1016%2fj.bspc.2023.105424&partnerID=40&md5=cafe57bf0dffb5afd8b27a4795a17834","Wearable systems measuring human physiological indicators with integrated sensors and supervised learning-based medical image analysis (e.g. ECG, X-ray, CT or ultrasound images for lung or the chest) have been considered relevant tools for COVID-19 monitoring and diagnosis. However, these two technical roadmaps have their respective advantages and drawbacks. The current wearable systems enable to realize real-time monitoring of COVID-19 but are limited to its basic symptoms only, neither allowing to distinguish it from other diseases nor performing deep analysis. Current medical image analysis can provide accurate decision support for diagnosis but rarely deals with real-time data processing. In this context, we propose a new wearable system by combining the advantages of these two technical roadmaps. Considering that electrocardiogram (ECG) has been proved relevant to evolution of COVID-19 symptoms, the proposed wearable system will integrate an explainable Deep Neural Network to realize online monitoring of COVID-19 gravity by using ECG beat signal. This paper will focus on the Deep Neural Network model named X-RCRNet. The network is based on ResNet18 but with few enhancements: 1) LSTM Layers for regenerating the backpropagation error and further extracting the involved time-varying features; 2) LeakyReLU for increasing the performances of the model. With an accuracy of 96.48 % after experiments, our model has not only outperformed the existing methods in terms of accuracy and robustness, but also originally identify the ST interval of the ECG pattern, as the most prominent key features affected by the virus. © 2023 The Authors","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85171610916"
"Spoladore D.; Sacco M.; Trombetta A.","Spoladore, Daniele (57194616131); Sacco, Marco (7103361721); Trombetta, Alberto (6603712206)","57194616131; 7103361721; 6603712206","A review of domain ontologies for disability representation","2023","Expert Systems with Applications","228","","120467","","","","7","10.1016/j.eswa.2023.120467","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160012645&doi=10.1016%2fj.eswa.2023.120467&partnerID=40&md5=7e8548ae8829f7ff7e15ebff42e6ea7f","Healthcare 5.0 is a research trend promoting a patient-centric approach leveraging Artificial Intelligence (AI)-based solutions. It aims to enhance care and quality of life for all patients, including those with a disability. However, when applied to the health sector, AI may be perceived as not transparent: the Explainable AI (xAI) paradigm attempts to solve this issue by providing more understandable, reliable, and human-interpretable AI-based applications. In a field such as disability – characterized by various impairments, limitations in performing activities, or other kinds of restrictions – the possibility to rely on computable representations of domain knowledge in the form of ontologies can support the development of xAI for healthcare. This work proposes a systematic literature review to identify which disabilities are currently represented in domain ontologies, examining which applicative contexts the ontologies were developed for. This review also investigates how the domain ontologies are modelled, underlining several relevant aspects that may foster their adoption in xAI systems. The review process results allow for shedding light on the main disabilities represented in ontologies, tracing the research trends that were at the basis of their development. Results also enable the identification of research lines that can support semantic interoperability – thus enabling ontologies to play a significant role in explaining decision processes performed by AI-based systems in healthcare. © 2023 Elsevier Ltd","Review","Final","","Scopus","2-s2.0-85160012645"
"Panigutti C.; Beretta A.; Fadda D.; Giannotti F.; Pedreschi D.; Perotti A.; Rinzivillo S.","Panigutti, Cecilia (57194345147); Beretta, Andrea (57357289400); Fadda, Daniele (57193424191); Giannotti, Fosca (7004495132); Pedreschi, Dino (6603935985); Perotti, Alan (58321306400); Rinzivillo, Salvatore (12760259500)","57194345147; 57357289400; 57193424191; 7004495132; 6603935985; 58321306400; 12760259500","Co-design of Human-centered, Explainable AI for Clinical Decision Support","2023","ACM Transactions on Interactive Intelligent Systems","13","4","21","","","","9","10.1145/3587271","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160005164&doi=10.1145%2f3587271&partnerID=40&md5=29f0bf9be77c55daadd1040a4a9bfe27","eXplainable AI (XAI) involves two intertwined but separate challenges: the development of techniques to extract explanations from black-box AI models and the way such explanations are presented to users, i.e., the explanation user interface. Despite its importance, the second aspect has received limited attention so far in the literature. Effective AI explanation interfaces are fundamental for allowing human decision-makers to take advantage and oversee high-risk AI systems effectively. Following an iterative design approach, we present the first cycle of prototyping-testing-redesigning of an explainable AI technique and its explanation user interface for clinical Decision Support Systems (DSS). We first present an XAI technique that meets the technical requirements of the healthcare domain: sequential, ontology-linked patient data, and multi-label classification tasks. We demonstrate its applicability to explain a clinical DSS, and we design a first prototype of an explanation user interface. Next, we test such a prototype with healthcare providers and collect their feedback with a two-fold outcome: First, we obtain evidence that explanations increase users’ trust in the XAI system, and second, we obtain useful insights on the perceived deficiencies of their interaction with the system, so we can re-design a better, more human-centered explanation interface. © 2023 Copyright held by the owner/author(s)","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85160005164"
"Kim M.; Kim S.; Kim J.; Song T.-J.; Kim Y.","Kim, Minjung (57199746490); Kim, Saebyeol (58644900000); Kim, Jinwoo (58105917500); Song, Tae-Jin (55507164200); Kim, Yuyoung (57900702600)","57199746490; 58644900000; 58105917500; 55507164200; 57900702600","Do stakeholder needs differ? - Designing stakeholder-tailored Explainable Artificial Intelligence (XAI) interfaces","2024","International Journal of Human Computer Studies","181","","103160","","","","0","10.1016/j.ijhcs.2023.103160","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174062754&doi=10.1016%2fj.ijhcs.2023.103160&partnerID=40&md5=4c4bbcfdaae62856b7ce1dafd6cd8976","Explainable AI (XAI) is increasingly being used in the healthcare domain. In health management, clinicians and patients are critical stakeholders, requiring tailored XAI explanations based on their unique needs. Our study investigates the differences in explanation needs between clinicians and patients and designs corresponding explanation interfaces for each group. Using a scenario-based approach, we assessed stakeholder-tailored needs, analyzed differences, and designed interfaces using theoretical frameworks. The results demonstrate diverse stakeholder motivations for seeking explanations, leading to varied requirements. The designed interfaces effectively address these requirements, as validated by the preference selection and qualitative feedback from clinicians and patients. Their suggestions provide design insights and highlight the divergent needs of these stakeholder groups. This study contributes practical and theoretical implications to XAI research, emphasizing the importance of understanding diverse stakeholder needs and incorporating relevant theoretical concepts into user-centered interface design. © 2023","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85174062754"
"Menegatti D.; Giuseppi A.; Delli Priscoli F.; Pietrabissa A.; Di Giorgio A.; Baldisseri F.; Mattioni M.; Monaco S.; Lanari L.; Panfili M.; Suraci V.","Menegatti, Danilo (57562235700); Giuseppi, Alessandro (56951042400); Delli Priscoli, Francesco (57208921813); Pietrabissa, Antonio (7006959510); Di Giorgio, Alessandro (24723969100); Baldisseri, Federico (57942810300); Mattioni, Mattia (57045864900); Monaco, Salvatore (57191474845); Lanari, Leonardo (6701585824); Panfili, Martina (55350948700); Suraci, Vincenzo (24587872600)","57562235700; 56951042400; 57208921813; 7006959510; 24723969100; 57942810300; 57045864900; 57191474845; 6701585824; 55350948700; 24587872600","CADUCEO: A Platform to Support Federated Healthcare Facilities through Artificial Intelligence","2023","Healthcare (Switzerland)","11","15","2199","","","","1","10.3390/healthcare11152199","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167798087&doi=10.3390%2fhealthcare11152199&partnerID=40&md5=8f756955a6db59656a07350df5add8e5","Data-driven algorithms have proven to be effective for a variety of medical tasks, including disease categorization and prediction, personalized medicine design, and imaging diagnostics. Although their performance is frequently on par with that of clinicians, their widespread use is constrained by a number of obstacles, including the requirement for high-quality data that are typical of the population, the difficulty of explaining how they operate, and ethical and regulatory concerns. The use of data augmentation and synthetic data generation methodologies, such as federated learning and explainable artificial intelligence ones, could provide a viable solution to the current issues, facilitating the widespread application of artificial intelligence algorithms in the clinical application domain and reducing the time needed for prevention, diagnosis, and prognosis by up to 70%. To this end, a novel AI-based functional framework is conceived and presented in this paper. © 2023 by the authors.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85167798087"
"Allgaier J.; Mulansky L.; Draelos R.L.; Pryss R.","Allgaier, Johannes (57212271599); Mulansky, Lena (57222057986); Draelos, Rachel Lea (57219625171); Pryss, Rüdiger (36142027900)","57212271599; 57222057986; 57219625171; 36142027900","How does the model make predictions? A systematic literature review on the explainability power of machine learning in healthcare","2023","Artificial Intelligence in Medicine","143","","102616","","","","12","10.1016/j.artmed.2023.102616","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163330389&doi=10.1016%2fj.artmed.2023.102616&partnerID=40&md5=789a5e5102058aafb01d3994302a990f","Background: Medical use cases for machine learning (ML) are growing exponentially. The first hospitals are already using ML systems as decision support systems in their daily routine. At the same time, most ML systems are still opaque and it is not clear how these systems arrive at their predictions. Methods: In this paper, we provide a brief overview of the taxonomy of explainability methods and review popular methods. In addition, we conduct a systematic literature search on PubMed to investigate which explainable artificial intelligence (XAI) methods are used in 450 specific medical supervised ML use cases, how the use of XAI methods has emerged recently, and how the precision of describing ML pipelines has evolved over the past 20 years. Results: A large fraction of publications with ML use cases do not use XAI methods at all to explain ML predictions. However, when XAI methods are used, open-source and model-agnostic explanation methods are more commonly used, with SHapley Additive exPlanations (SHAP) and Gradient Class Activation Mapping (Grad-CAM) for tabular and image data leading the way. ML pipelines have been described in increasing detail and uniformity in recent years. However, the willingness to share data and code has stagnated at about one-quarter. Conclusions: XAI methods are mainly used when their application requires little effort. The homogenization of reports in ML use cases facilitates the comparability of work and should be advanced in the coming years. Experts who can mediate between the worlds of informatics and medicine will become more and more in demand when using ML systems due to the high complexity of the domain. © 2023","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85163330389"
"Nayak T.; Chadaga K.; Sampathila N.; Mayrose H.; Gokulkrishnan N.; Bairy G M.; Prabhu S.; S S.K.; Umakanth S.","Nayak, Tushar (58307214400); Chadaga, Krishnaraj (57226665664); Sampathila, Niranjana (56584740000); Mayrose, Hilda (57356755200); Gokulkrishnan, Nitila (58307214500); Bairy G, Muralidhar (58307214600); Prabhu, Srikanth (57197645702); S, Swathi K. (58880972700); Umakanth, Shashikiran (17347367000)","58307214400; 57226665664; 56584740000; 57356755200; 58307214500; 58307214600; 57197645702; 58880972700; 17347367000","Deep learning based detection of monkeypox virus using skin lesion images","2023","Medicine in Novel Technology and Devices","18","","100243","","","","9","10.1016/j.medntd.2023.100243","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161340363&doi=10.1016%2fj.medntd.2023.100243&partnerID=40&md5=eb835a61788a36dec46bebb0f996e837","As we set into the second half of 2022, the world is still recovering from the two-year COVID-19 pandemic. However, over the past three months, the outbreak of the Monkeypox Virus (MPV) has led to fifty-two thousand confirmed cases and over one hundred deaths. This caused the World Health Organisation to declare the outbreak a Public Health Emergency of International Concern (PHEIC). If this outbreak worsens, we could be looking at the Monkeypox virus causing the next global pandemic. As Monkeypox affects the human skin, the symptoms can be captured with regular imaging. Large samples of these images can be used as a training dataset for machine learning-based detection tools. Using a regular camera to capture the skin image of the infected person and running it against computer vision models is beneficial. In this research, we use deep learning to diagnose monkeypox from skin lesion images. Using a publicly available dataset, we tested the dataset on five pre-trained deep neural networks: GoogLeNet, Places365-GoogLeNet, SqueezeNet, AlexNet and ResNet-18. Hyperparameter was done to choose the best parameters. Performance metrics such as accuracy, precision, recall, f1-score and AUC were considered. Among the above models, ResNet18 was able to obtain the highest accuracy of 99.49%. The modified models obtained validation accuracies above 95%. The results prove that deep learning models such as the proposed model based on ResNet-18 can be deployed and can be crucial in battling the monkeypox virus. Since the used networks are optimized for efficiency, they can be used on performance limited devices such as smartphones with cameras. The addition of explainable artificial intelligence techniques LIME and GradCAM enables visual interpretation of the prediction made, helping health professionals using the model. © 2023 The Authors","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85161340363"
"Koyyada S.P.; Singh T.P.","Koyyada, Shiva prasad (58317706900); Singh, Thipendra P. (58310574300)","58317706900; 58310574300","An AI Decision System to Predict Lung Nodules through Localization from Chest X-ray Images","2023","2023 9th International Conference on Signal Processing and Communication, ICSC 2023","","","","214","220","6","0","10.1109/ICSC60394.2023.10441301","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187232488&doi=10.1109%2fICSC60394.2023.10441301&partnerID=40&md5=bef6e5ecf676f5af209ab14acebdc2d6","The objective of this research is to design an Artificial Intelligence (AI) decision system that helps medical practitioners diagnose lung disease with more probability. Between the 1970s and 1990s, clinical image analysis was carried out by fusing low-level pixel processing (edge and line detectors, region expansion) with mathematical modeling (fitting lines, circles, and ellipses) to develop compound rule-based systems that dealt with particular tasks. In the 1990s the handwritten recognition task adapted Convolutional neural networks(CNN) and achieved the remarkable success that has laid the road for CNNs. The Chinese of Wuhan, in the Hubei Province, saw an epidemic of the coronavirus illness 2019 (COVID-19), which was unleashed by the SARS-CoV-2. The World Health Organization declared the pandemic a public health emergency of worldwide significance on January 30, 2020. Over three years have passed since the worldwide struggle against the COVID-19 epidemic began. One of the quickest methods to diagnose patients is to use computer-aided diagnosis (CAD) and deep learning techniques on radiography and radiology images. With this, everyone's focus has turned to lung disease detection such as tuberculosis and lung cancer through Chest X-ray images with state-of-the-art methods such as CNNs. However, these fail to explain the reason for making such a decision. This work focuses on this aspect of determining region of interest features with a process of localization while predicting lung nodules in two stages with the help of an explainable artificial method. © 2023 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85187232488"
"Spoladore D.; Stella F.; Tosi M.; Lorenzini E.C.","Spoladore, Daniele (57194616131); Stella, Francesco (58613632700); Tosi, Martina (58191123800); Lorenzini, Erna C. (57789430000)","57194616131; 58613632700; 58191123800; 57789430000","Towards a Knowledge-Based Decision Support System for the Management of Type 2 Diabetic Patients","2023","Lecture Notes in Networks and Systems","745 LNNS","","","309","320","11","2","10.1007/978-3-031-38274-1_26","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172021285&doi=10.1007%2f978-3-031-38274-1_26&partnerID=40&md5=a4c635db12de7e68d6a35da4a5ef1cd6","The rise of Healthcare 5.0 paradigm calls for personalization of care and management of patients’ conditions. Though promising, data-driven techniques may raise some concerns as they are perceived as scarcely transparent and reliable by clinical personnel. With the emergence of Explainable Artificial Intelligence (AI), these limitations could significantly be overcome. In this regard, the exploitation of domain knowledge (properly formalized) can support explainable AI and foster the delivery of Decision Support Systems (DSS) for tailored treatment of many diseases. This work aims to present a knowledge-based DSS for managing patients with Type 2-Diabetes Mellitus (T2D), a non communicable disease that can take advantage of tailored medical nutrition therapies, taking into account patient’s specific health condition and comorbidities. The DSS leverages ontological representation of domain knowledge to automatically classify the patients’ phenotype and identify the potential comorbidities, then, it exploits a set of rules to provide tailored nutrition recommendations that can be adopted by general practice doctors and family clinicians to provide tailored dietary plans. In this way, the proposed DSS can support physicians and dieticians (who may lack specialized training in T2D management) in the management of diabetic patients through personalized medical nutrition therapies. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Conference paper","Final","","Scopus","2-s2.0-85172021285"
"Kumbhar U.T.; Phursule R.; Patil V.C.; Moje R.K.; Shete O.R.; Tayal M.A.","Kumbhar, Uddhav T. (57217688461); Phursule, Rajesh (57835559400); Patil, V.C. (37102294600); Moje, Ravindra K (58135143500); Shete, Omkar R. (57225974783); Tayal, Madhuri A. (34881105600)","57217688461; 57835559400; 37102294600; 58135143500; 57225974783; 34881105600","Explainable AI-Powered IoT Systems for Predictive and Preventive Healthcare - A Framework for Personalized Health Management and Wellness Optimization","2023","Journal of Electrical Systems","19","3","","23","31","8","0","10.52783/jes.648","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185202005&doi=10.52783%2fjes.648&partnerID=40&md5=f45c72026b0abae90cfe351515a59132","With the growing integration of Internet of Things (IoT) technologies and Artificial Intelligence (AI) in healthcare, it is crucial to prioritize transparency and interpretability in the decision-making process. This paper presents a novel framework that utilizes Explainable AI (XAI) to improve the interpretability of predictive healthcare models. The proposed system integrates feature importance-based methodologies with the Local Interpretable Model-agnostic Explanations (LIME) technique to offer a comprehensive comprehension of the predictive and preventive healthcare recommendations. The framework commences by conducting an in-depth examination of the present condition of Internet of Things (IoT) in the healthcare sector, as well as the importance of predictive and preventive healthcare. The literature review examines the difficulties related to the comprehensibility of artificial intelligence (AI) in the healthcare field and presents feature importance-based approaches and LIME as potential remedies. The focus is on the hybrid approach that combines these techniques, as it has the potential to offer precise predictions while also ensuring a strong level of interpretability. The methodology section delineates the procedure for gathering healthcare data and IoT sensor data, subsequently followed by preprocessing measures such as data cleansing and feature engineering. The predictive models undergo a process of selection, training, and evaluation, with the primary objective of attaining a notable accuracy level of 0.961. This text provides a detailed explanation of how the combination of feature importance-based approaches and LIME improves the transparency and interpretability of the model. An extensive case study is provided to illustrate the implementation of the suggested framework in an actual situation. The results and evaluation section showcases the exceptional precision of 0.961, as well as enhanced interpretability scores and decreased computational time in comparison to the baseline XAI models. The discussion section juxtaposes the suggested hybrid approach with conventional models, examines ethical considerations, and investigates the scalability and generalizability of the framework. To conclude, the paper provides a concise overview of the findings and implications of the Explainable AI-Powered IoT Systems for Predictive and Preventive Healthcare framework. This hybrid approach demonstrates high accuracy, improved interpretability, and efficient computational performance, making it a promising advancement in personalized health management and wellness optimization. This research adds to the expanding collection of literature on Explainable Artificial Intelligence (XAI) in the healthcare sector, thus opening up possibilities for future research avenues and practical applications in this domain. © JES 2023 on-line : journal.esrgroups.org","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85185202005"
"Raja M.A.; Loughran R.; Caffery F.M.","Raja, Muhammad Adil (57193863272); Loughran, Róisín (24825132300); Caffery, Fergal Mc (27967562500)","57193863272; 24825132300; 27967562500","A review of applications of artificial intelligence in cardiorespiratory rehabilitation","2023","Informatics in Medicine Unlocked","41","","101327","","","","0","10.1016/j.imu.2023.101327","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168996640&doi=10.1016%2fj.imu.2023.101327&partnerID=40&md5=7084fc3be5d9e5afdf31c29353a4ba6e","Implementations of artificial intelligence and machine learning are becoming commonplace in multiple application domains. This is in part due to advancements in computing hardware that have helped outsource the computation of resource-intensive mathematics related to artificial intelligence and machine learning to the chips of multi-core and parallel computing architectures. Partly it is due to the widespread appeal of machine learning as a suite of handy tools to fix practical issues. Many fields have become beneficiaries of artificial intelligence and machine learning and cardiorespiratory rehabilitation is no exception. The aim of this paper is to review the current state of the art of the applications of artificial intelligence and machine learning in cardiorespiratory rehabilitation. We have taken a multidimensional view to addressing the needs and utility of artificial intelligence and machine learning in cardiorespiratory rehabilitation. We start with the most primitive applications of machine learning reported in existing literature in making medical devices for analyzing heartbeats and respiratory functions. We then discuss more recent approaches including deep learning to analyze performance or suggest alternative choices for food or exercise. Applications and utility of most recent feats such as explainable artificial intelligence are also discussed and conclusions around the current state of the art and possible future directions are proposed. © 2023 The Author(s)","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85168996640"
"Ignesti G.; Deri C.; D'Angelo G.; Pratali L.; Bruno A.; Benassi A.; Salvetti O.; Moroni D.; Martinelli M.","Ignesti, Giacomo (58205482800); Deri, Chiara (58083782700); D'Angelo, Gennaro (57697276900); Pratali, Lorenza (6603105724); Bruno, Antonio (57764868400); Benassi, Antonio (23101920100); Salvetti, Ovidio (56234956100); Moroni, Davide (23397809400); Martinelli, Massimo (56678326500)","58205482800; 58083782700; 57697276900; 6603105724; 57764868400; 23101920100; 56234956100; 23397809400; 56678326500","Deep learning methods for point-of-care ultrasound examination","2023","Proceedings - 17th International Conference on Signal-Image Technology and Internet-Based Systems, SITIS 2023","","","","435","440","5","0","10.1109/SITIS61268.2023.00078","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190110533&doi=10.1109%2fSITIS61268.2023.00078&partnerID=40&md5=abc6f9299937aeec7e00ca0c6dc8cf9f","Point-of-care Test (POCT) is the delivery of medical care at or near the patient's bedside. Primarily employed in emergencies, where rapid diagnosis and treatment are critical, POCT is now being used in domestic telehealth solutions, as in the TiAssisto project, thanks to technological advances such as the development of portable and affordable devices, high-speed Internet connections, video conferencing, and Artificial Intelligence (AI). Ultrasound (US) images of internal organs and structures are valuable tools in POCT medicine since this examination is portable, quick, and cost-effective. USs can help diagnose different conditions, including heart problems, abdominal pain, and pneumonia. Deep learning algorithms have proven to be highly effective in image recognition, enabling physicians to make informed decisions on-site. This article presents a pipeline approach providing remarkable and reliable results to handle point-of-care ultrasound examinations, making use of methods for: a) automating text cleaning for privacy based on an Optical Character Recognition (OCR) algorithm; b) scrolling through the video frames and annotating them using an ad hoc implemented tool; c) classifying various signs in US using a state of the art deep learning algorithm, that is an adaptive efficient method ensembling two EfficientNet-b0 weak models; d) benchmarking medical plausibility to address transparency and human in the loop setting using a post hoc explanation visual explanation method, i.e. Grad-CAM.The involved physician's feedback remarks that this system can detect important signs in pulmonary US imaging. However, the dataset is not yet the final one since the TiAssisto project is still ongoing, with a planned conclusion in February 2024. Our ultimate goal is not merely to develop a classification system but to create an effective healthcare support system that can be used beyond primary healthcare facilities.  © 2023 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85190110533"
"Hall K.; Jayne C.; Chang V.","Hall, Karl (57561506700); Jayne, Chrisina (36608214500); Chang, Victor (57695131300)","57561506700; 36608214500; 57695131300","A Transformer-Based Framework for Biomedical Information Retrieval Systems","2023","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","14259 LNCS","","","317","331","14","1","10.1007/978-3-031-44223-0_26","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174580581&doi=10.1007%2f978-3-031-44223-0_26&partnerID=40&md5=3494a2f7162905f9258fd81188b6c847","With the increasing amount of electronic biomedical research available, it is becoming difficult for scientists and researchers to effectively access and process this information. In this study, a framework is proposed for biomedical information retrieval using state-of-the-art transformer-based natural language processing methods, focusing on discovering genetic relationships and mechanisms that contribute to diseases and other health conditions. Data processing, model evaluation and algorithm transparency through the use explainable artificial intelligence are proposed as the key components of the methodology. The framework is explained and demonstrated in the context of COVID-19 in a topical and systematic manner, and is designed so that it can be adapted for use in other biomedical settings. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Conference paper","Final","","Scopus","2-s2.0-85174580581"
"de Benito Fernández M.; Martínez D.L.; González-Briones A.; Chamoso P.; Corchado E.S.","de Benito Fernández, Marco (58617128500); Martínez, Daniel López (58616865600); González-Briones, Alfonso (57200569138); Chamoso, Pablo (55835429500); Corchado, Emilio S. (6602872701)","58617128500; 58616865600; 57200569138; 55835429500; 6602872701","Evaluation of XAI Models for Interpretation of Deep Learning Techniques’ Results in Automated Plant Disease Diagnosis","2023","Lecture Notes in Networks and Systems","732 LNNS","","","417","428","11","1","10.1007/978-3-031-36957-5_36","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172175909&doi=10.1007%2f978-3-031-36957-5_36&partnerID=40&md5=98b12f35c5fefbd26a3c49b1bf554e40","Automated disease diagnosis in plants is crucial for the agriculture industry to maintain crop health and increase yields, which has significant implications for global food security and the economy. The use of convolutional neural networks (CNNs) for disease diagnosis has gained much attention due to their ability to detect patterns in the images of plant leaves, allowing for the accurate diagnosis of diseases. However, one of the major challenges in using CNNs is their limited interpretability, which makes it difficult to understand the reasoning behind the model’s output. In this work, we explore the potential of CNNs for plant disease diagnosis and propose the use of explainable artificial intelligence (XAI) methods to improve the interpretability of the CNN’s output. We first discuss the state of the art in plant disease detection, including conventional methods such as Polymerase Chain Reaction (PCR) and Isothermal Amplification based diagnosis, and the rise of CNNs in the field. We then introduce the architecture and the principles of CNNs, highlighting their ability to classify images and their use in plant disease diagnosis. However, due to their black-box nature, the interpretation of CNN outputs remains challenging. To address this, we propose the use of post-hoc XAI methods, specifically LIME, SHAP, and Grad-CAM, to provide insights into CNN’s decision-making process. Our study aims to demonstrate the potential of CNNs for plant disease diagnosis and the importance of interpretability in deep learning models. We hope our work will contribute to the development of more accurate and interpretable CNN models for disease diagnosis in plants, ultimately leading to more efficient and sustainable agriculture practices. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Conference paper","Final","","Scopus","2-s2.0-85172175909"
"Khambampati S.; Dondapati S.; Kattamuri T.V.; Pathinarupothi R.K.","Khambampati, Subhash (58811713700); Dondapati, Sushanth (58811376200); Kattamuri, Tejo Vardhan (58934928400); Pathinarupothi, Rahul Krishnan (55645309700)","58811713700; 58811376200; 58934928400; 55645309700","CureNet: Improving Explainability of AI Diagnosis Using Custom Large Language Models","2023","2023 3rd International Conference on Smart Generation Computing, Communication and Networking, SMART GENCON 2023","","","","","","","0","10.1109/SMARTGENCON60755.2023.10442356","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187577462&doi=10.1109%2fSMARTGENCON60755.2023.10442356&partnerID=40&md5=a860482e5a6e6613811678a1ecf879f1","Cardiac abnormalities are a leading global cause of fatalities, necessitating precise diagnostic decisions, particularly in the context of cardiac health. Explainable Artificial Intelligence (XAI) has emerged as a valuable tool to interpret and clarify opaque algorithms used in Machine Learning (ML) and Deep Learning (DL). In our research, we emphasize the need for DL tools that are both precise and explainable, especially in domains where human expertise is crucial. We introduce a novel clinician-in-the-loop, prompt-based XAI tool that integrates multiple DL models with different XAI techniques, offering both accuracy and transparency. Our DL model, trained on a comprehensive ECG dataset, achieves high precision. We also employ XAI techniques, including Integrated Gradients, Layer-wise Relevance Propagation (LRP), and DeepLift, to generate heatmaps that highlight the model's decision-making regions within ECG signals. These visual outputs enhance the interpretability of the diagnostic process. Additionally, we present a chatbot that uses a pre-trained language model and an innovative image retrieval feature, offering an intelligent conversational agent for medical inquiries. © 2023 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85187577462"
"Alabdulhafith M.; Saleh H.; Elmannai H.; Ali Z.H.; El-Sappagh S.; Hu J.-W.; El-Rashidy N.","Alabdulhafith, Maali (55650519900); Saleh, Hager (57218799328); Elmannai, Hela (55366208300); Ali, Zainab Hassan (57194720250); El-Sappagh, Shaker (55233800700); Hu, Jong-Wan (55589006600); El-Rashidy, Nora (57211502814)","55650519900; 57218799328; 55366208300; 57194720250; 55233800700; 55589006600; 57211502814","A Clinical Decision Support System for Edge/Cloud ICU Readmission Model Based on Particle Swarm Optimization, Ensemble Machine Learning, and Explainable Artificial Intelligence","2023","IEEE Access","11","","","100604","100621","17","0","10.1109/ACCESS.2023.3312343","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171584043&doi=10.1109%2fACCESS.2023.3312343&partnerID=40&md5=6be2478afde13a1821687b9fb9b0538d","ICU readmission is usually associated with an increased number of hospital death. Predicting readmission helps to reduce such risks by avoiding early discharge, providing appropriate intervention, and planning for patient placement after ICU discharge. Unfortunately, ICU scores such as the simplified acute physiology score (SAPS) and Acute Physiology and Chronic Health (APACHE) could help predict mortality or evaluate illness severity. Still, it is ineffective in predicting ICU readmission. This study introduces a clinical monitoring fog-computing-based system for remote prognosis and monitoring of intensive care patients. This proposed monitoring system uses the advantages of machine learning (ML) approaches for generating a real-time alert signal to doctors for supplying e-healthcare, accelerating decision-making, and monitoring and controlling health systems. The proposed system includes three main layers. First, the data acquisition layer, in which we collect the vital signs and lab tests of the patient's health conditions in real-time. Then, the fog computing layer processes. The results are then sent to the cloud layer, which offers sizable storage space for patient healthcare. Demographic data, lab tests, and vital signs are aggregated from the MIMIC III dataset for 10,465 patients. Feature selection methods: Genetic algorithm (GA) and practical swarm optimization (PSO) are used to choose the optimal feature subset from detests. Moreover, Different traditional ML models, ensemble learning models, and the proposed stacking models are applied to full features and selected features to predict readmission after 30 days of ICU discharge. The proposed stacking models recorded the highest performance compared to other models. The proposed stacking ensemble model with selected features by POS achieved promising results (accuracy = 98.42, precision = 98.42, recall = 98.42, and F1-Score = 98.42), compared to full features and selected features. We also, provide model explanations to ensure efficiency, effectiveness, and trust in the developed model through local and global explanations.  © 2013 IEEE.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85171584043"
"Costa B.; Georgieva P.","Costa, Beatriz (57204896503); Georgieva, Petia (6701326860)","57204896503; 6701326860","Explainable Artificial Intelligence in Healthcare Applications: A Systematic Review","2023","2023 11th International Scientific Conference on Computer Science, COMSCI 2023 - Proceedings","","","","","","","0","10.1109/COMSCI59259.2023.10315829","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186385089&doi=10.1109%2fCOMSCI59259.2023.10315829&partnerID=40&md5=81c52b0304c68fa0928692329e50db40","Current artificial intelligence (AI) advances and progress in medicine created a new challenge for medical AI. The' black-box' nature of AI methods has created a discussion on the use of explainability techniques to build trust and provide transparency in the AI decision-making process. A study of current state-of-the-art approaches in Explainable Artificial Intelligence (XAI) was conducted using Preferred Reporting Items on Systematic Reviews and Meta-analysis (PRISMA) research technology. In this systematic review, we provide an overview of current XAI techniques based on different taxonomies. Finally, we discuss the applications and challenges that come with the application of explainability methods in the healthcare industry. © 2023 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85186385089"
"Donoso-Guzmá I.","Donoso-Guzmá, Ivania (58745643700)","58745643700","Designing an evaluation framework for eXplainable AI in the Healthcare domain","2023","CEUR Workshop Proceedings","3554","","","201","208","7","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178607051&partnerID=40&md5=c75c5b5847eebee17c12aea25c60a718","The rapid adoption of Artificial Intelligence (AI) has brought automation and problem-solving capabilities in various fields, including healthcare. However, a significant challenge lies in the lack of explanation for AI predictions, particularly in healthcare, where transparency is crucial. This issue has led to eXplainable AI (XAI) development, focusing on constructing explanations for AI systems. However, the evaluation of these explanations lacks a standardized user-centric approach. This research proposes an evaluation framework for XAI methods to address this gap. The project involves four stages: conducting a systematic review of current evaluation methods, assessing the appropriateness of automatic evaluation of explanations, and conducting user studies to gauge the framework's effectiveness in capturing the user experience complexity. The desired outcome is a user-centric evaluation framework and guidelines, enhancing the scalability of XAI research and fostering confidence in adopting AI systems in the healthcare domain. © 2023 CEUR-WS. All rights reserved.","Conference paper","Final","","Scopus","2-s2.0-85178607051"
"Corbucci L.; Monreale A.; Panigutti C.; Natilli M.; Smiraglio S.; Pedreschi D.","Corbucci, Luca (58651281000); Monreale, Anna (35113703300); Panigutti, Cecilia (57194345147); Natilli, Michela (36010971000); Smiraglio, Simona (58651963800); Pedreschi, Dino (6603935985)","58651281000; 35113703300; 57194345147; 36010971000; 58651963800; 6603935985","Semantic Enrichment of Explanations of AI Models for Healthcare","2023","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","14276 LNAI","","","216","229","13","0","10.1007/978-3-031-45275-8_15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174292824&doi=10.1007%2f978-3-031-45275-8_15&partnerID=40&md5=1edd4cad48b1a2466ca599bcc2685745","Explaining AI-based clinical decision support systems is crucial to enhancing clinician trust in those powerful systems. Unfortunately, current explanations provided by eXplainable Artificial Intelligence techniques are not easily understandable by experts outside of AI. As a consequence, the enrichment of explanations with relevant clinical information concerning the health status of a patient is fundamental to increasing human experts’ ability to assess the reliability of AI decisions. Therefore, in this paper, we propose a methodology to enable clinical reasoning by semantically enriching AI explanations. Starting with a medical AI explanation based only on the input features provided to the algorithm, our methodology leverages medical ontologies and NLP embedding techniques to link relevant information present in the patient’s clinical notes to the original explanation. Our experiments, involving a human expert, highlight promising performance in correctly identifying relevant information about the diseases of the patients. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Conference paper","Final","","Scopus","2-s2.0-85174292824"
"Raval M.S.; Roy M.; Kaya T.; Kapdi R.","Raval, Mehul S. (6701357003); Roy, Mohendra (56113062800); Kaya, Tolga (21834230200); Kapdi, Rupal (57189216764)","6701357003; 56113062800; 21834230200; 57189216764","Explainable AI in Healthcare: Unboxing Machine Learning for Biomedicine","2023","Explainable AI in Healthcare: Unboxing Machine Learning for Biomedicine","","","","1","304","303","1","10.1201/9781003333425","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168885875&doi=10.1201%2f9781003333425&partnerID=40&md5=92927eb87cb3bb9a451c176b3a034728","This book combines technology and the medical domain. It covers advances in computer vision (CV) and machine learning (ML) that facilitate automation in diagnostics and therapeutic and preventive health care. The special focus on explainable Artificial Intelligence (XAI) uncovers the black box of ML and bridges the semantic gap between the technologists and the medical fraternity. Explainable AI in Healthcare: Unboxing Machine Learning for Biomedicine intends to be a premier reference for practitioners, researchers, and students at basic, intermediary levels and expert levels in computer science, electronics and communications, information technology, instrumentation and control, and electrical engineering. This book will benefit readers in the following ways: Explores state of art in computer vision and deep learning in tandem to develop autonomous or semi-autonomous algorithms for diagnosis in health care Investigates bridges between computer scientists and physicians being built with XAI Focuses on how data analysis provides the rationale to deal with the challenges of healthcare and making decision-making more transparent Initiates discussions on human-AI relationships in health care Unites learning for privacy preservation in health care © 2024 selection and editorial matter, Mehul S Raval, Mohendra Roy, Tolga Kaya, Rupal Kapdi; individual chapters, the contributors.","Book","Final","","Scopus","2-s2.0-85168885875"
"Faust O.; De Michele S.; Koh J.E.W.; Jahmunah V.; Lih O.S.; Kamath A.P.; Barua P.D.; Ciaccio E.J.; Lewis S.K.; Green P.H.; Bhagat G.; Acharya U.R.","Faust, Oliver (14830975900); De Michele, Simona (57216556696); Koh, Joel EW (55905807100); Jahmunah, V. (57208643008); Lih, Oh Shu (57191645630); Kamath, Aditya P (57271228000); Barua, Prabal Datta (36993665100); Ciaccio, Edward J. (7004846604); Lewis, Suzanne K. (14628960100); Green, Peter H. (8629716200); Bhagat, Govind (57204522114); Acharya, U. Rajendra (7004510847)","14830975900; 57216556696; 55905807100; 57208643008; 57191645630; 57271228000; 36993665100; 7004846604; 14628960100; 8629716200; 57204522114; 7004510847","Automated analysis of small intestinal lamina propria to distinguish normal, Celiac Disease, and Non-Celiac Duodenitis biopsy images","2023","Computer Methods and Programs in Biomedicine","230","","107320","","","","3","10.1016/j.cmpb.2022.107320","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145781814&doi=10.1016%2fj.cmpb.2022.107320&partnerID=40&md5=c9556c52e6d0365091a07f97da41b56b","Background and objective: Celiac Disease (CD) is characterized by gluten intolerance in genetically predisposed individuals. High disease prevalence, absence of a cure, and low diagnosis rates make this disease a public health problem. The diagnosis of CD predominantly relies on recognizing characteristic mucosal alterations of the small intestine, such as villous atrophy, crypt hyperplasia, and intraepithelial lymphocytosis. However, these changes are not entirely specific to CD and overlap with Non-Celiac Duodenitis (NCD) due to various etiologies. We investigated whether Artificial Intelligence (AI) models could assist in distinguishing normal, CD, and NCD (and unaffected individuals) based on the characteristics of small intestinal lamina propria (LP). Methods: Our method was developed using a dataset comprising high magnification biopsy images of the duodenal LP compartment of CD patients with different clinical stages of CD, those with NCD, and individuals lacking an intestinal inflammatory disorder (controls). A pre-processing step was used to standardize and enhance the acquired images. Results: For the normal controls versus CD use case, a Support Vector Machine (SVM) achieved an Accuracy (ACC) of 98.53%. For a second use case, we investigated the ability of the classification algorithm to differentiate between normal controls and NCD. In this use case, the SVM algorithm with linear kernel outperformed all the tested classifiers by achieving 98.55% ACC. Conclusions: To the best of our knowledge, this is the first study that documents automated differentiation between normal, NCD, and CD biopsy images. These findings are a stepping stone toward automated biopsy image analysis that can significantly benefit patients and healthcare providers. © 2022","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85145781814"
"Dutta J.; Puthal D.; Yeun C.Y.","Dutta, Joy (57193125252); Puthal, Deepak (56151401500); Yeun, Chan Yeob (6508380997)","57193125252; 56151401500; 6508380997","Next Generation Healthcare with Explainable AI: IoMT-Edge-Cloud Based Advanced eHealth","2023","Proceedings - IEEE Global Communications Conference, GLOBECOM","","","","7327","7332","5","0","10.1109/GLOBECOM54140.2023.10436967","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187329894&doi=10.1109%2fGLOBECOM54140.2023.10436967&partnerID=40&md5=b729d55682aff8cc7f2154e5f905cd7b","This article provides in-depth experimental studies of XAI (EXplainable Artificial Intelligence) in the IoT-Edge-Cloud continuum. Within the different available XAI frameworks, such as Local Interpretable Model-Agnostic Explanations (LIME) and SHapley Additive exPlanations (SHAP) frameworks are utilized here as they are the most suitable feature map-based, model-agnostic, posthoc frameworks that match our requirements for getting real-time prediction explanations in the healthcare domain. In order to evaluate LIME and SHAP in this continuum and to make black box AI (BBAI)-based decisions interpretable, we have considered the real-world electronic health record (EHR)-based large cloud database (which could be a very large database-VLDB) and IoMT based real-time streams as edge databases for the prediction of cardiac arrest in the real-world. We have also verified the effectiveness of automated counterfactual explanations in this context for taking remedial actions. Thus, our proposed model is capable of making significant advancements in the healthcare industry by offering conscious healthcare monitoring automation along with an AI-based self-explanatory system that serves as a personalized health assistant for individuals, paving the way for the next major upgrade in healthcare. © 2023 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85187329894"
"Banerjee J.S.; Chakraborty A.; Mahmud M.; Kar U.; Lahby M.; Saha G.","Banerjee, Jyoti Sekhar (56016851400); Chakraborty, Arpita (57192713269); Mahmud, Mufti (35173453700); Kar, Ujjwal (58535469100); Lahby, Mohamed (37041689600); Saha, Gautam (59091080000)","56016851400; 57192713269; 35173453700; 58535469100; 37041689600; 59091080000","Explainable Artificial Intelligence (XAI) Based Analysis of Stress Among Tech Workers Amidst COVID-19 Pandemic","2023","Internet of Things","Part F1201","","","151","174","23","6","10.1007/978-3-031-28631-5_8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167865377&doi=10.1007%2f978-3-031-28631-5_8&partnerID=40&md5=ad766c9d77d267d91e1b4427b4c7c4bf","Work stress is now a global concern. Particularly, as a result of the extended lockdown during COVID-19, many industries, particularly those in the tech industry, were obliged to adopt remote working. In this chapter, we analyze the prevalence of mental health illnesses among tech professionals and assess attitudes toward mental health in the workplace. Question-answer based stress detection is one of the most widely used methods nowadays since it preserves social distance and is effective. Algorithms used in artificial intelligence (AI) are frequently referred to as “black boxes,” due to their lack of explainability nature. Again, an AI paradigm called Explainable AI (XAI) aims to make end users aware of the goals, choices, and rationale behind the system. End users may be anybody whose decisions are affected by an AI model, including consumers, data scientists, regulatory authorities, domain experts, executive board members, and managers who employ AI with or without knowledge. The goal of this research is to make a system that is very flexible and uses XAI to find the stressors at work that hurt the mental health of tech professionals. As per the best belief of the authors, till now, no researcher has reported in this domain. The findings point to both qualitative and quantitative visual representations that might provide doctors additional in-depth information about the results provided by the learned XAI models, enhancing their understanding and decision-making. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Book chapter","Final","","Scopus","2-s2.0-85167865377"
"Alahmadi A.; Alansari A.; Alsheikh N.; Alshammasi S.; Alshamery M.; Al-abdulmohsin R.; Al Rabia L.; Al Nass F.; Alghamdi M.; Almustafa S.; Aljamea Z.; Kurdi S.; Islam M.A.; Hussein D.","Alahmadi, Alaa (57213670305); Alansari, Aisha (57416168100); Alsheikh, Nawal (58079824700); Alshammasi, Salam (58079824800); Alshamery, Mona (58564512600); Al-abdulmohsin, Rand (58564008600); Al Rabia, Laila (57226314064); Al Nass, Fatimah (58564008700); Alghamdi, Manar (58022675400); Almustafa, Sarah (58564008800); Aljamea, Zainab (57193225573); Kurdi, Sawsan (57193310602); Islam, Md. Ashraful (58995497500); Hussein, Dania (55383086100)","57213670305; 57416168100; 58079824700; 58079824800; 58564512600; 58564008600; 57226314064; 58564008700; 58022675400; 58564008800; 57193225573; 57193310602; 58995497500; 55383086100","Beta blockers may be protective in COVID-19; findings of a study to develop an interpretable machine learning model to assess COVID-19 disease severity in light of clinical findings, medication history, and patient comorbidities","2023","Informatics in Medicine Unlocked","42","","101341","","","","0","10.1016/j.imu.2023.101341","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169927452&doi=10.1016%2fj.imu.2023.101341&partnerID=40&md5=20832a1f2c00d86673433ba91e1994f2","The coronavirus disease 2019 (COVID-19) has overwhelmed healthcare systems and continues to pose a significant threat worldwide. Predicting disease severity would enhance treatment provision and resource allocation. Although multiple studies were conducted to assess COVID-19's severity using machine learning (ML) models, few studies focus on patient medication history and comorbidities. In this study, ML algorithms were trained using a comprehensive dataset comprising medication history, comorbidities, and clinical findings. Patient data was gathered from King Fahad University Hospital (KFUH) in Saudi Arabia (IRB#: 2021-05-480). The dataset comprised 622 positive COVID-19 with 49 features. Three experiments were conducted to train four ML algorithms, including random forest (RF), gradient boosting machine (GMB), extreme gradient boost (XGBoost), and extra trees (ET). Findings revealed that GBM outperformed other models with 96.30% accuracy, 95.80% precision, 97.64% recall, and 96.69% F-score, with 23 features. Moreover, the permutation feature importance technique suggested that the five most influential features for forecasting disease severity were “CRP level”, “CO2 level”, “SrCr”, “Tocilizumab”, and “Age”. In addition, the shapley additive explanation (SHAP) recommended that the “D-Dimer level”, “CrCl”, and “Hypertension” were also influential. The development of an effective GBM model has the potential to aid medical specialists in the assessment of disease severity. While several models take into account patient presentation and laboratory findings, this study is unique in its scope, considering a far more comprehensive patient profile. The developed model was able to accurately predict features that have been clinically shown to correlate with disease severity. Of interest the model was able to identify a pattern of association between the use of certain medications such and disease severity. We report that the use of beta blockers may be associated with reduced severity, whereas the use of immune modulating drugs namely tocilizumab appeared to be associated with poor disease outcomes in this patient population. © 2023 The Authors","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85169927452"
"Martínez S.; Gómez C.; Herrera V.; Albusac J.A.; Castro-Schez J.J.; Vallejo D.","Martínez, Sergio (58759842700); Gómez, Cristian (57211939663); Herrera, Vanesa (36141514500); Albusac, Javier A. (23089942600); Castro-Schez, José J. (6602954376); Vallejo, David (23010740800)","58759842700; 57211939663; 36141514500; 23089942600; 6602954376; 23010740800","Decision Support System for Automatic Adjustment of Rehabilitation Routines for Stroke Patients","2023","CEUR Workshop Proceedings","3578","","","99","121","22","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179587822&partnerID=40&md5=4f88a8ea5adb426305949355e4dc9d7b","The ability of new Artificial Intelligence (AI) models to assist humans in performing tasks is creating new business models and transforming existing ones at breakneck speed. One of the application areas benefiting from this technology is healthcare. The work presented in this article falls within this domain. In this sense, our work focuses on how AI can be used to facilitate the work of therapists responsible for the physical rehabilitation of stroke patients. In particular, we present a decision support system integrated in a global remote rehabilitation system composed of two interconnected applications: the one used by the therapist to define routines and monitor patients and the one used by the patient to perform rehabilitation exercises autonomously. The decision support system is based on the use of fuzzy logic, which significantly increases its scalability and interpretability. The proposed system is capable of automatically suggesting personalised modifications to the rehabilitation routine assigned to a patient by the therapist, based on the patient’s performance. In addition, this system integrates aspects of Explainable Artificial Intelligence (XAI) by being able to justify why it suggests such modifications, so that the therapist has more information when validating or not validating the modifications proposed by the artificial system. The paper discusses a case study describing how a stroke patient’s routine is automatically adjusted by the system. © 2023 CEUR-WS. All rights reserved.","Conference paper","Final","","Scopus","2-s2.0-85179587822"
"Vittoria Togo M.; Mastrolorito F.; Orfino A.; Graps E.A.; Tondo A.R.; Altomare C.D.; Ciriaco F.; Trisciuzzi D.; Nicolotti O.; Amoroso N.","Vittoria Togo, Maria (58790447800); Mastrolorito, Fabrizio (58023576900); Orfino, Angelica (58790540700); Graps, Elisabetta Anna (57202235687); Tondo, Anna Rita (57209312574); Altomare, Cosimo Damiano (7005856522); Ciriaco, Fulvio (6507486891); Trisciuzzi, Daniela (56925495300); Nicolotti, Orazio (6603050960); Amoroso, Nicola (55419832300)","58790447800; 58023576900; 58790540700; 57202235687; 57209312574; 7005856522; 6507486891; 56925495300; 6603050960; 55419832300","Where developmental toxicity meets explainable artificial intelligence: state-of-the-art and perspectives","2023","Expert Opinion on Drug Metabolism and Toxicology","","","","","","","2","10.1080/17425255.2023.2298827","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181204244&doi=10.1080%2f17425255.2023.2298827&partnerID=40&md5=69e7268f8a8ef9bff392be5107414ee1","Introduction: The application of Artificial Intelligence (AI) to predictive toxicology is rapidly increasing, particularly aiming to develop non-testing methods that effectively address ethical concerns and reduce economic costs. In this context, Developmental Toxicity (Dev Tox) stands as a key human health endpoint, especially significant for safeguarding maternal and child well-being. Areas covered: This review outlines the existing methods employed in Dev Tox predictions and underscores the benefits of utilizing New Approach Methodologies (NAMs), specifically focusing on eXplainable Artificial Intelligence (XAI), which proves highly efficient in constructing reliable and transparent models aligned with recommendations from international regulatory bodies. Expert opinion: The limited availability of high-quality data and the absence of dependable Dev Tox methodologies render XAI an appealing avenue for systematically developing interpretable and transparent models, which hold immense potential for both scientific evaluations and regulatory decision-making. © 2023 Informa UK Limited, trading as Taylor & Francis Group.","Review","Article in press","","Scopus","2-s2.0-85181204244"
"D'Angelo G.; Della-Morte D.; Pastore D.; Donadel G.; De Stefano A.; Palmieri F.","D'Angelo, Gianni (7102881236); Della-Morte, David (23008189700); Pastore, Donatella (8753787600); Donadel, Giulia (6701730462); De Stefano, Alessandro (57023457900); Palmieri, Francesco (7103111550)","7102881236; 23008189700; 8753787600; 6701730462; 57023457900; 7103111550","Identifying patterns in multiple biomarkers to diagnose diabetic foot using an explainable genetic programming-based approach","2023","Future Generation Computer Systems","140","","","138","150","12","18","10.1016/j.future.2022.10.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141235542&doi=10.1016%2fj.future.2022.10.019&partnerID=40&md5=9eeaa711d8325b6017ec841f4daed4dc","Diabetes mellitus is a global health problem, recognized as the seventh cause of death in the world. One of the most debilitating complications of diabetes mellitus is the diabetic foot (DF), resulting in an increased risk of hospitalization and significant morbidity and mortality. Amputation above or below the knee is a feared complication and the mortality in these patients is higher than for most forms of cancer. Identifying and interpreting relationships existing among the factors involved in DF diagnosis is still challenging. Although machine learning approaches have proven to achieve great accuracy in DF prediction, few advances have been performed in understanding how they make such predictions, resulting in mistrust of their use in real contexts. In this study, we present an approach based on Genetic Programming to build a simple global explainable classifier, named X-GPC, which, unlike existing tools such as LIME and SHAP, provides a global interpretation of the DFU diagnosis through a mathematical model. Also, an easy consultable 3d graph is provided, which could be used by the medical staff to figure out the patients’ situation and take decisions for patients’ healing. Experimental results obtained by using a real-world dataset have shown the ability of the proposal to diagnose DF with an accuracy of 100% outperforming other techniques of the state-of-the-art. © 2022 Elsevier B.V.","Article","Final","","Scopus","2-s2.0-85141235542"
"Tsarapatsani K.-H.; Sakellarios A.; Tsakanikas V.D.; Kleber M.; Marz W.; Fotiadis D.I.","Tsarapatsani, Konstantina-Helen (57406228700); Sakellarios, Antonis (36476633700); Tsakanikas, Vasilis D. (36718299600); Kleber, Marcus (57207897615); Marz, Winfried (57220877383); Fotiadis, Dimitrios I. (55938920100)","57406228700; 36476633700; 36718299600; 57207897615; 57220877383; 55938920100","Machine Learning Models Predict Fatal Myocardial Infarction Within 10-years Followup Utilizing Explainable AI","2023","Proceedings - 2023 IEEE 23rd International Conference on Bioinformatics and Bioengineering, BIBE 2023","","","","320","324","4","0","10.1109/BIBE60311.2023.00059","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186503075&doi=10.1109%2fBIBE60311.2023.00059&partnerID=40&md5=7d15787b44a84b51d4ecf1875525ffde","Fatal myocardial infarction (MI) is one of the most common types of cardiovascular diseases that often presents in the emergency department. The prediction of death caused by myocardial infarction within 10-years follow-up is addressed in this study, using comorbidities, daily habits, clinical and laboratory data in binary and continuous data respectively. The used data are included in a cohort of the Ludwigshafen Risk and Cardiovascular Health (LURIC) study. The target feature, namely death caused by MI, contained 106 deceased patients and 2,321 alive patients in the final used dataset. The analysis was based on machine learning models (ML), such as support vector machine (SVM), light gradient-boosting machine (LGBM), Random Forest (RF), Decision Tree (DT). Their performance was estimated by Area Under Receiver Operating Characteristic Curve (AUC), Sensitivity, Specificity, Precision and Accuracy. Results show that LGBM was the most suitable of the aforementioned models to predict death caused by myocardial infarction within 10-years follow-up, achieving area under the curve (AUC) value equal to 77.03 %, accuracy 69.42 %, sensitivity 69.75 %, specificity 69.40% and precision 53.76 %. In addition, explainable artificial intelligence (xAI) was utilized and especially SHapley Additive exPlanations (SHAP) was the selected method. SHAP was utilized in order to shed light on the results, applying the LGBM model as the best predictive model. The provided SHAP plots contribute to the interpretation of how each independent feature aid to the final prediction. © 2023 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85186503075"
"Makubhai S.S.; Pathak G.R.; Chandre P.R.","Makubhai, Shahin S. (58484704100); Pathak, Ganesh R. (36873479200); Chandre, Pankaj R. (57188728117)","58484704100; 36873479200; 57188728117","Exploring the Trade-Offs Between Blackbox and Explainable AI: A Comparative Study","2023","2023 7th International Conference On Computing, Communication, Control And Automation, ICCUBEA 2023","","","","","","","0","10.1109/ICCUBEA58933.2023.10391996","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187312382&doi=10.1109%2fICCUBEA58933.2023.10391996&partnerID=40&md5=73eb6d6f9176075bb85f4457d5181cc7","Healthcare, finance, and transportation are just a few of the sectors where artificial intelligence (AI) is now a necessity. AI algorithms come in two flavors: explainable and blackbox. Explainable models are intended to be more transparent and interpretable than blackbox models, which are complicated and challenging to understand. Through a comparative analysis, this article investigates the trade-offs between explainable AI and blackbox AI. It specifically looks into how explainability affects accuracy, user perception of AI models, the efficiency of explainability methods, domain-specific effects, and ethical ramifications. The study seeks to offer useful insights into the advantages and disadvantages of these models and to assist in guiding decisions regarding the use of explainable and blackbox AI in various applications.  © 2023 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85187312382"
"Moreno-Sánchez P.A.","Moreno-Sánchez, Pedro A. (57219909354)","57219909354","Improvement of a prediction model for heart failure survival through explainable artificial intelligence","2023","Frontiers in Cardiovascular Medicine","10","","1219586","","","","5","10.3389/fcvm.2023.1219586","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168299575&doi=10.3389%2ffcvm.2023.1219586&partnerID=40&md5=695f8a3009c8f5c52f1802b4f441bff7","Cardiovascular diseases and their associated disorder of heart failure (HF) are major causes of death globally, making it a priority for doctors to detect and predict their onset and medical consequences. Artificial Intelligence (AI) allows doctors to discover clinical indicators and enhance their diagnoses and treatments. Specifically, “eXplainable AI” (XAI) offers tools to improve the clinical prediction models that experience poor interpretability of their results. This work presents an explainability analysis and evaluation of two HF survival prediction models using a dataset that includes 299 patients who have experienced HF. The first model utilizes survival analysis, considering death events and time as target features, while the second model approaches the problem as a classification task to predict death. The model employs an optimization data workflow pipeline capable of selecting the best machine learning algorithm as well as the optimal collection of features. Moreover, different post hoc techniques have been used for the explainability analysis of the model. The main contribution of this paper is an explainability-driven approach to select the best HF survival prediction model balancing prediction performance and explainability. Therefore, the most balanced explainable prediction models are Survival Gradient Boosting model for the survival analysis and Random Forest for the classification approach with a c-index of 0.714 and balanced accuracy of 0.74 (std 0.03) respectively. The selection of features by the SCI-XAI in the two models is similar where “serum_creatinine”, “ejection_fraction”, and “sex” are selected in both approaches, with the addition of “diabetes” for the survival analysis model. Moreover, the application of post hoc XAI techniques also confirm common findings from both approaches by placing the “serum_creatinine” as the most relevant feature for the predicted outcome, followed by “ejection_fraction”. The explainable prediction models for HF survival presented in this paper would improve the further adoption of clinical prediction models by providing doctors with insights to better understand the reasoning behind usually “black-box” AI clinical solutions and make more reasonable and data-driven decisions. 2023 Moreno-Sánchez.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85168299575"
"Rasmy L.; Xie Z.; Zhi D.","Rasmy, Laila (57202643832); Xie, Ziqian (57219767546); Zhi, Degui (57211105610)","57202643832; 57219767546; 57211105610","Training Recurrent Neural Network-Based Model to Predict COVID-19 Patient Risk for PASC using Pytorch-EHR - Hands-on Tutorial on N3C","2023","Proceedings - 2023 IEEE 11th International Conference on Healthcare Informatics, ICHI 2023","","","","529","530","1","0","10.1109/ICHI57859.2023.00094","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181559211&doi=10.1109%2fICHI57859.2023.00094&partnerID=40&md5=c4b62d415075b1b8806e1cab221e63df","Pytorch-EHR is a codebase enabling fast prototyping of deep learning-based predictive models using electronic health records structured data. Rather than a collection of vertical pipelines implementing methods from papers claiming state-of the-art results, Pytorch-EHR offers efficient implementations of data flow, padding, embeddings, choices of popular recurrent neural networks (RNN) cells and layers structures, prediction heads, and also predictions explanation and visualization. This tutorial will provide participants with a clear understanding of deep learning theoretical concepts behind Pytorch-EHR different components, as well as hands-on experience in building an explainable RNN-based model to solve a real-world clinical problem on a cloud-based platform hosting the National COVID Cohort Collaborative (N3C) data. URL: https://github.com/ZhiGroup/pytorch-ehr © 2023 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85181559211"
"Becker M.; Toprak E.; Beyerer J.","Becker, Maximilian (57226749189); Toprak, Emrah (58909449300); Beyerer, Jurgen (6603794653)","57226749189; 58909449300; 6603794653","Explainable Artificial Intelligence for Interpretable Data Minimization","2023","IEEE International Conference on Data Mining Workshops, ICDMW","","","","885","893","8","0","10.1109/ICDMW60847.2023.00119","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186138821&doi=10.1109%2fICDMW60847.2023.00119&partnerID=40&md5=84017d83a4dd518be8615f1f4b70b654","Black box models such as deep neural networks are increasingly being deployed in high-stakes fields, including justice, health, and finance. Furthermore, they require a huge amount of data, and such data often contains personal information. However, the principle of data minimization in the European Union's General Data Protection Regulation requires collecting only the data that is essential to fulfilling a particular purpose. Implementing data minimization for black box models can be difficult because it involves identifying the minimum set of variables that are relevant to the model's prediction, which may not be apparent without access to the model's inner workings. In addition, users are often reluctant to share all their personal information. We propose an interactive system to reduce the amount of personal data by determining the minimal set of features required for a correct prediction using explainable artificial intelligence techniques. Our proposed method can inform the user whether the provided variables contain enough information for the model to make accurate predictions or if additional variables are necessary. This human-centered approach can enable providers to minimize the amount of personal data collected for analysis and may increase the user's trust and acceptance of the system. © 2023 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85186138821"
"Li Z.; Li R.; Zhou Y.; Rasmy L.; Zhi D.; Zhu P.; Dono A.; Jiang X.; Xu H.; Esquenazi Y.; Zheng W.J.","Li, Zhao (57883273600); Li, Rongbin (58172515200); Zhou, Yujia (57208207129); Rasmy, Laila (57202643832); Zhi, Degui (57211105610); Zhu, Ping (57194719478); Dono, Antonio (57216658501); Jiang, Xiaoqian (24479530900); Xu, Hua (57215023364); Esquenazi, Yoshua (22937705000); Zheng, W Jim (57205446444)","57883273600; 58172515200; 57208207129; 57202643832; 57211105610; 57194719478; 57216658501; 24479530900; 57215023364; 22937705000; 57205446444","Prediction of Brain Metastases Development in Patients With Lung Cancer by Explainable Artificial Intelligence From Electronic Health Records","2023","JCO clinical cancer informatics","7","","","e2200141","","","0","10.1200/CCI.22.00141","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151803514&doi=10.1200%2fCCI.22.00141&partnerID=40&md5=7ea203f73a3fbbe3a1c634bc5711ee39","PURPOSE: Early detection of brain metastases (BMs) is critical for prompt treatment and optimal control of the disease. In this study, we seek to predict the risk of developing BM among patients diagnosed with lung cancer on the basis of electronic health record (EHR) data and to understand what factors are important for the model to predict BM development through explainable artificial intelligence approaches accurately. MATERIALS AND METHODS: We trained a recurrent neural network model, REverse Time AttentIoN (RETAIN), to predict the risk of developing BM using structured EHR data. To interpret the model's decision process, we analyzed the attention weights in the RETAIN model and the SHAP values from a feature attribution method, Kernel SHAP, to identify the factors contributing to BM prediction. RESULTS: We developed a high-quality cohort with 4,466 patients with BM from the Cerner Health Fact database, which contains over 70 million patients from more than 600 hospitals. RETAIN uses this data set to achieve the best area under the receiver operating characteristic curve at 0.825, a significant improvement over the baseline model. We also extended a feature attribution method, Kernel SHAP, to structured EHR data for model interpretation. Both RETAIN and Kernel SHAP can identify important features related to BM prediction. CONCLUSION: To the best of our knowledge, this is the first study to predict BM using structured EHR data. We achieved decent prediction performance for BM prediction and identified factors highly relevant to BM development. The sensitivity analysis demonstrated that both RETAIN and Kernel SHAP could discriminate unrelated features and put more weight on the features important to BM. Our study explored the potential of applying explainable artificial intelligence for future clinical applications.","Article","Final","","Scopus","2-s2.0-85151803514"
"Otiya S.; Faldu P.; Goel P.","Otiya, Samarth (58839113400); Faldu, Pavan (58839113500); Goel, Parth (57204771167)","58839113400; 58839113500; 57204771167","Cotton Leaf Disease Classification using Deep Convolution Neural Network with Explainable AI","2023","International Conference on Sustainable Communication Networks and Application, ICSCNA 2023 - Proceedings","","","","1417","1424","7","0","10.1109/ICSCNA58489.2023.10370214","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183046690&doi=10.1109%2fICSCNA58489.2023.10370214&partnerID=40&md5=e56b57eb5d6008374ced0372f01c7f87","The cotton leaf disease detection model combines Convolutional Neural Networks (CNN) with Explainable artificial intelligence (XAI) technology for the goal of diagnosing diseases in cotton leaves. This diagnostic procedure incorporates the classifying of leaves into four unique classes: healthy, curl virus-infected, bacterial blight-affected, and fusarium wilt afflicted. In contemporary times, agriculture and farming have witnessed a significant evolution, with disease detection techniques playing a pivotal role in this transformation. This technology proves invaluable to farmers by facilitating to detection of diseases in crops after that farmer can empowering them to take proactive measures to manage and mitigate the impact of these diseases on their agricultural yields and overall crop health. The primary objective of this research endeavor is to classify diseases within a set of four distinct categories. This categorization challenge is done by the employment of Convolutional Neural Networks (CNN). The research leverages transfer learning models, including Xception, ResNet50, DenseNet121, and EfficientNet, to achieve high classification accuracy while also employing Grad-CAM for disease detection. The projected accuracies for each model are as follows: 85% for Xception, 99.1% for ResNet50, 93.3% for DenseNet121, and 98.5% for EfficientNet. © 2023 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85183046690"
"Ihalapathirana A.; Chalkou K.; Siirtola P.; Tamminen S.; Chandra G.; Benkert P.; Kuhle J.; Salanti G.; Röning J.","Ihalapathirana, Anusha (57195401480); Chalkou, Konstantina (57217171718); Siirtola, Pekka (23393881200); Tamminen, Satu (6507476370); Chandra, Gunjan (57224207135); Benkert, Pascal (56617808400); Kuhle, Jens (8937520800); Salanti, Georgia (57219575707); Röning, Juha (6701703474)","57195401480; 57217171718; 23393881200; 6507476370; 57224207135; 56617808400; 8937520800; 57219575707; 6701703474","Explainable Artificial Intelligence to predict clinical outcomes in type 1 diabetes and relapsing-remitting multiple sclerosis adult patients","2023","Informatics in Medicine Unlocked","42","","101349","","","","0","10.1016/j.imu.2023.101349","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171391059&doi=10.1016%2fj.imu.2023.101349&partnerID=40&md5=323c192b5127eefeb7059752965ac872","Artificial intelligence (AI) is increasingly being used to improve patient care and management. In this paper, we propose explainable AI (XAI) models for predicting severe hypoglycemia (SH) and diabetic ketoacidosis (DKA) episodes in adults with type 1 diabetes (T1D) and relapses in adults with relapsing-remitting multiple sclerosis (RRMS). We follow a three-step process in this study: (1) develop baseline machine learning (ML) models, (2) improve the models using ReliefF feature selection technique, and develop sex-stratified models, (3) explain the models and their results using SHapley Additive exPlanations (SHAP). We built six ML models (XGBoost, LightGBM, CatBoost, AdaBoost, random forest, and linear regression) for all scenarios. Applying the ReliefF feature selection led to improved model performance in predicting all outcomes compared to the baseline models. Additionally, sex-stratified models further improved the prediction of SH episodes and relapses. The F1 scores for predicting SH episodes in male and female patients were 84.07% and 84.95%, respectively, and the DKA prediction model achieved an F1 score of 78.67%. The proposed relapse prediction models outperformed existing models with F1 scores of 84.55% (males) and 76.11% (females), and ROCs of 70.26% (males) and 69.05% (females). Our results highlight the importance of considering sex differences, socioeconomic factors, and physical and mental health in medical outcome prediction. Boosting ML algorithms were found to be effective in detecting SH and DKA in T1D patients and relapses in RRMS patients compared to conventional tree-based ML and statistical models. © 2023 The Authors","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85171391059"
"Khater T.; Hussain A.; Bendardaf R.; Talaat I.M.; Tawfik H.; Ansari S.; Mahmoud S.","Khater, Tarek (58547921500); Hussain, Abir (56212648400); Bendardaf, Riyad (8656432000); Talaat, Iman M. (7801465176); Tawfik, Hissam (10039993800); Ansari, Sam (57225445996); Mahmoud, Soliman (57200564959)","58547921500; 56212648400; 8656432000; 7801465176; 10039993800; 57225445996; 57200564959","An Explainable Artificial Intelligence Model for the Classification of Breast Cancer","2023","IEEE Access","","","","1","1","0","3","10.1109/ACCESS.2023.3308446","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168693210&doi=10.1109%2fACCESS.2023.3308446&partnerID=40&md5=9144a79cec1acaa614060470eba9addf","Breast cancer is the most common cancer among women and globally affects both genders. The disease arises due to abnormal growth of tissue formed of malignant cells. Early detection of breast cancer is crucial for enhancing the survival rate. Therefore, artificial intelligence has revolutionized healthcare and can serve as a promising tool for early diagnosis. The present study aims to develop a machine-learning model to classify breast cancer and to provide explanations for the model results. This could improve the understanding of the diagnosis and treatment of breast cancer by identifying the most important features of breast cancer tumors and the way they affect the classification task. The best-performing machine-learning model has achieved an accuracy of 97.7% using k-nearest neighbors and a precision of 98.2% based on the Wisconsin breast cancer dataset and an accuracy of 98.6% using the artificial neural network with 94.4% precision based on the Wisconsin diagnostic breast cancer dataset. Hence, this asserts the importance and effectiveness of the proposed approach. The present research explains the model behavior using model-agnostic methods, demonstrating that the bare nuclei feature in the Wisconsin breast cancer dataset and the area&#x2019;s worst feature Wisconsin diagnostic breast cancer dataset are the most important factors in determining breast cancer malignancy. The work provides extensive insights into the particular characteristics of the diagnosis of breast cancer and suggests possible directions for expected investigation in the future into the fundamental biological mechanisms that underlie the disease&#x2019;s onset. The findings underline the potential of machine learning to enhance breast cancer diagnosis and therapy planning while emphasizing the importance of interpretability and transparency in artificial intelligence-based healthcare systems. Author","Article","Article in press","All Open Access; Gold Open Access","Scopus","2-s2.0-85168693210"
"Annuzzi G.; Arpaia P.; Bozzetto L.; Criscuolo S.; Giugliano S.; Pesola M.","Annuzzi, Giovanni (6701900487); Arpaia, Pasquale (7006199525); Bozzetto, Lutgarda (23666268600); Criscuolo, Sabatina (57419367000); Giugliano, Salvatore (57222982677); Pesola, Marisa (57919703900)","6701900487; 7006199525; 23666268600; 57419367000; 57222982677; 57919703900","Assessing the Features on Blood Glucose Level Prediction in Type 1 Diabetes Patients Through Explainable Artificial Intelligence","2023","2023 IEEE International Conference on Metrology for eXtended Reality, Artificial Intelligence and Neural Engineering, MetroXRAINE 2023 - Proceedings","","","","278","283","5","0","10.1109/MetroXRAINE58569.2023.10405831","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185762493&doi=10.1109%2fMetroXRAINE58569.2023.10405831&partnerID=40&md5=69b44cc01ecdc0fe19127766c5d5b291","Type 1 diabetes mellitus (T1DM) is an autoimmune disease causing insulin deficiency and impaired blood sugar control. Managing postprandial glucose response (PGR) is a major challenge in T1DM, as it necessitates precise insulin dosing prior to meals. The potential of the artificial pancreas lies in its integration of insulin delivery and glucose monitoring, but accurate incorporation of meal information remains an open challenge. While machine learning methodologies could help in addressing this issue, most existing models lack interpretability and comprehension of the output. This study aims to assess the influence of various metabolic features on the prediction of postprandial Blood Glucose Levels (BGLs) using eXplainable Artificial Intelligence (XAI) methodologies. Two distinct prediction models are developed, considering varying prediction horizons following the meal (15 min and 60 min). Specifically, BGLs prediction models incorporate preprandial blood glucose values, insulin, meal carbohydrate intake, and glycemic index and load as input variable. To evaluate their impact, SHapley Additive exPlanations (SHAP), a method that provides insights into the contribution of each feature to the model predictions, was employed. By leveraging XAI methodologies, the study aims to improve the interpretability and transparency of BGLs prediction models. The findings of this research can contribute to the development of decision-support tools for patients affected by T1DM, helping the management of PGR and reducing the risks of adverse events. © 2023 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85185762493"
"Till T.; Tschauner S.; Singer G.; Lichtenegger K.; Till H.","Till, Tristan (58794575100); Tschauner, Sebastian (56183931400); Singer, Georg (9233306300); Lichtenegger, Klaus (24765049800); Till, Holger (55334722700)","58794575100; 56183931400; 9233306300; 24765049800; 55334722700","Development and optimization of AI algorithms for wrist fracture detection in children using a freely available dataset","2023","Frontiers in Pediatrics","11","","1291804","","","","1","10.3389/fped.2023.1291804","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181438315&doi=10.3389%2ffped.2023.1291804&partnerID=40&md5=5acac40476fa9fed8417be64c29051db","Introduction: In the field of pediatric trauma computer-aided detection (CADe) and computer-aided diagnosis (CADx) systems have emerged offering a promising avenue for improved patient care. Especially children with wrist fractures may benefit from machine learning (ML) solutions, since some of these lesions may be overlooked on conventional X-ray due to minimal compression without dislocation or mistaken for cartilaginous growth plates. In this article, we describe the development and optimization of AI algorithms for wrist fracture detection in children. Methods: A team of IT-specialists, pediatric radiologists and pediatric surgeons used the freely available GRAZPEDWRI-DX dataset containing annotated pediatric trauma wrist radiographs of 6,091 patients, a total number of 10,643 studies (20,327 images). First, a basic object detection model, a You Only Look Once object detector of the seventh generation (YOLOv7) was trained and tested on these data. Then, team decisions were taken to adjust data preparation, image sizes used for training and testing, and configuration of the detection model. Furthermore, we investigated each of these models using an Explainable Artificial Intelligence (XAI) method called Gradient Class Activation Mapping (Grad-CAM). This method visualizes where a model directs its attention to before classifying and regressing a certain class through saliency maps. Results: Mean average precision (mAP) improved when applying optimizations pre-processing the dataset images (maximum increases of (Formula presented.) 25.51% mAP@0.5 and (Formula presented.) 39.78% mAP@[0.5:0.95]), as well as the object detection model itself (maximum increases of (Formula presented.) 13.36% mAP@0.5 and (Formula presented.) 27.01% mAP@[0.5:0.95]). Generally, when analyzing the resulting models using XAI methods, higher scoring model variations in terms of mAP paid more attention to broader regions of the image, prioritizing detection accuracy over precision compared to the less accurate models. Discussion: This paper supports the implementation of ML solutions for pediatric trauma care. Optimization of a large X-ray dataset and the YOLOv7 model improve the model’s ability to detect objects and provide valid diagnostic support to health care specialists. Such optimization protocols must be understood and advocated, before comparing ML performances against health care specialists. 2023 Till, Tschauner, Singer, Lichtenegger and Till.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85181438315"
"Chaudhary S.; Joshi P.; Bhattacharya P.; Prasad V.K.; Shah R.; Tanwar S.","Chaudhary, Sachi (57810081300); Joshi, Pooja (58588018900); Bhattacharya, Pronaya (57200306370); Prasad, Vivek Kumar (57196721184); Shah, Rushabh (57190723413); Tanwar, Sudeep (56576145100)","57810081300; 58588018900; 57200306370; 57196721184; 57190723413; 56576145100","Untangling Explainable AI in Applicative Domains: Taxonomy, Tools, and Open Challenges","2023","Lecture Notes in Networks and Systems","664 LNNS","","","857","872","15","0","10.1007/978-981-99-1479-1_63","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168757527&doi=10.1007%2f978-981-99-1479-1_63&partnerID=40&md5=e4010b381c15e949a2480e9e2e626340","Recently, a paradigm shift is observed toward Industry 5.0, where tasks (processes) are automated at massive scales. This shift has initiated modern developments in artificial intelligence (AI) to support a plethora of applications like manufacturing, health care, vehicular net- works, and others. However, owing to the black-box nature of AI models, the research has shifted toward the proposal of novel techniques that aim toward the explainability and validity of these AI models. Thus, explainable AI (XAI) has become a norm in modern applicative domains, and the study of its frameworks and tools has become the buzzword among researchers. Thus, the paper intends to present the key concepts of XAI and aims at improving the model transparency. The survey systematically untangles the key concepts of XAI and presents a solution taxonomy in different applications. Modern XAI techniques are classified as self-explanatory, visual-based-model-agnostic, global surrogate, and local surrogate-model-agnostic. We also cover the tools and frameworks of XAI and discuss the open issues and challenges in practical realization. Thus, the survey intends to arm AI practitioners to design optimal solutions to realize XAI in practical use-case setups. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Conference paper","Final","","Scopus","2-s2.0-85168757527"
"Johnson M.; Albizri A.; Harfouche A.; Tutun S.","Johnson, Marina (57210189342); Albizri, Abdullah (56104317500); Harfouche, Antoine (36462416800); Tutun, Salih (57113588100)","57210189342; 56104317500; 36462416800; 57113588100","Digital transformation to mitigate emergency situations: increasing opioid overdose survival rates through explainable artificial intelligence","2023","Industrial Management and Data Systems","123","1","","324","344","20","19","10.1108/IMDS-04-2021-0248","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116791655&doi=10.1108%2fIMDS-04-2021-0248&partnerID=40&md5=a5775e494082c2203d66215fd4bf21c3","Purpose: The global health crisis represents an unprecedented opportunity for the development of artificial intelligence (AI) solutions. This paper aims to integrate explainable AI into the decision-making process in emergency scenarios to help mitigate the high levels of complexity and uncertainty associated with these situations. An AI solution is designed to extract insights into opioid overdose (OD) that can help government agencies to improve their medical emergency response and reduce opioid-related deaths. Design/methodology/approach: This paper employs the design science research paradigm as an overarching framework. Open-access digital data and AI, two essential components within the digital transformation domain, are used to accurately predict OD survival rates. Findings: The proposed AI solution has two primary implications for the advancement of informed emergency management. Results show that it can help not only local agencies plan their resources for timely response to OD incidents, thus improving survival rates, but also governments to identify geographical areas with lower survival rates and their primary contributing factor; hence, they can plan and allocate long-term resources to increase survival rates and help in developing effective emergency-related policies. Originality/value: This paper illustrates that digital transformation, particularly open-access digital data and AI, can improve the emergency management framework (EMF). It also demonstrates that the AI models developed in this study can identify opioid OD trends and determine the significant factors improving survival rates. © 2021, Emerald Publishing Limited.","Article","Final","","Scopus","2-s2.0-85116791655"
"Aelgani V.; Vadlakonda D.","Aelgani, Vivekanand (58184026800); Vadlakonda, Dhanalaxmi (57226178402)","58184026800; 57226178402","Explainable Artificial Intelligence based Ensemble Machine Learning for Ovarian Cancer Stratification using Electronic Health Records","2023","International Journal on Recent and Innovation Trends in Computing and Communication","11","7","","78","84","6","0","10.17762/ijritcc.v11i7.7832","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172905510&doi=10.17762%2fijritcc.v11i7.7832&partnerID=40&md5=bfb1ec6565e8f6c58f21ace5703a4f25","The purpose of this study is to show how ensemble learning-driven machine learning algorithms outperform individual machine learning algorithms at predicting ovarian cancer on a biomarker dataset. Additionally, this study provides model explanations using explainable Artificial Intelligence methods, The method involved gathering and combining 49 risk factors from 349 patients. We hypothesize that ensemble machine learning systems are superior to individual Machine Learning systems in predicting ovarian cancer. The Machine Learning system consists of five individual Machine Learning and five ensemble Machine Learning systems were trained using K-10 cross validation protocols. These training models were then used to predict the development of benign ovarian tumors and ovarian cancer tumors patients. The AUC and Accuracy metrics for ensemble machine learning increased by 19% and 16%. The MCC and Kappa scores for ensemble Machine Learning also increased over individual machine learning by 29% and 33%, respectively. As a result, we draw the conclusion that ensembled-based algorithms outperform individual machine learning in terms of ovarian carcinoma prediction. © 2023 International Journal on Recent and Innovation Trends in Computing and Communication. All rights reserved.","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85172905510"
"Ganguly R.; Singh D.","Ganguly, Rita (54395296400); Singh, Dharmpal (55541656800)","54395296400; 55541656800","Explainable Artificial Intelligence (XAI) for the Prediction of Diabetes Management: An Ensemble Approach","2023","International Journal of Advanced Computer Science and Applications","14","7","","158","163","5","2","10.14569/IJACSA.2023.0140717","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168808138&doi=10.14569%2fIJACSA.2023.0140717&partnerID=40&md5=876d01980dcb7bd612409a08a4cfe2bc","Machine learning determines patterns from data to expedite the process of decision making. Fact-based decisions and data-driven decisions are specified by the industry specialist. Due to the continuous growth of machine language models in healthcare, they are breeding continuous complexity and black boxes in ML models. To make the ML model crystal clear and authentically explainable, AI accession came in prevalence. This research scrutinizes the explainable AI and capabilities in the Indian healthcare system to detect diabetes. LIME and SHAP are two libraries and packages that are used to implement explainable AI. The intimated base amalgamates the local and global interpretable methods, which enhances the crystallinity of the complex model and obtains intuition into the equity from the complex model. Moreover, the obtained intuition could also boost clinical data scientists to plan a more felicitous composition of computer-aided diagnosis. Importance of XAI to forecast stubborn disease. In this case, of stubborn diabetes, the correlation between plasma versus insulin, age versus pregnancies, class (diabetic and nondiabetic) versus plasma glucose persisted with a strong relationship. The PIDD (PIMA Indian Diabetic Data set) with the SHAP value is used for concise dependency, and LIME is applicable when anchors and importance of features are both required simultaneously. Dependency plots help physicians visualize independent relationships with predicted disease. To identify dependencies of different attributes, a correlation heatmap is used. From an academic perspective, XAI is very indispensable to mature in the near future. To estimate the presentation of other applicable data set correspondence studies are very much apprenticed. © 2023, Science and Information Organization. All Rights Reserved.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85168808138"
"Tanwar R.; Singh G.; Pal P.K.","Tanwar, Ritu (58096176000); Singh, Ghanapriya (57189236272); Pal, Pankaj Kumar (55602418800)","58096176000; 57189236272; 55602418800","FuSeR: Fusion of wearables data for StrEss Recognition using explainable artificial intelligence models","2023","2023 14th International Conference on Computing Communication and Networking Technologies, ICCCNT 2023","","","","","","","0","10.1109/ICCCNT56998.2023.10307589","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179837190&doi=10.1109%2fICCCNT56998.2023.10307589&partnerID=40&md5=2b40e03f4460ffff6bbd3443a90fcf19","Wearables such as smartwatches have become increasingly popular and have many applications such as health monitoring and chronic disease management. As wearables are becoming more widely available and affordable, so personal data that these devices provide also increases. Additionally, many biological signal sensors are integrated into wearables and can be used for collecting personalised physiological data. Following data collection, classification techniques can aid in analysing the data insights. In this study, the effectiveness of a stress recognition approach using wearables' physiological data fusion was evaluated. Specifically, four data level fusion methods (Fusion 2, 3, 4 and 5), consisting of two, three, four and five physiological modalities have been used. Different combinations from electrocardiogram (ECG), electrodermal activity (EDA), electromyography (EMG), accelerometer (Acc.), temperature (Temp.) and respiration (Resp.) signals have been considered for each fusion method to recognize stress. Extra gradient boost (XGBoost), light gradient boost (LGBoost) and CatBoost machine learning methods along with explainable artificial method (XAI) are implemented to classify stress. WESAD dataset is used for training and testing the models. Performance metrics for Fusion 1 (two physiological modalities), 2 (three physiological modalities), 3 (four physiological modalities) and 4 (five physiological modalities) are evaluated. Overall, the accuracy of the stress recognition model increases with increase in modalities. Fusion 4 consisting of five modalities provided the best performance with an accuracy from 95% to 99%. XAI explains more influence of EDA modality among other modalities on the model accuracy. The proposed stress recognition model with fusion of wearables data using XAI efficiently detects stress and contributes to mental health monitoring. © 2023 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85179837190"
"Zahoor K.; Bawany N.Z.; Ghani U.","Zahoor, Kanwal (57221643018); Bawany, Narmeen Zakaria (10044512100); Ghani, Usman (57194008542)","57221643018; 10044512100; 57194008542","Explainable AI for Healthcare: An Approach Towards Interpretable Healthcare Models","2023","2023 24th International Arab Conference on Information Technology, ACIT 2023","","","","","","","0","10.1109/ACIT58888.2023.10453740","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189137663&doi=10.1109%2fACIT58888.2023.10453740&partnerID=40&md5=d8e3298836485866f35884c1e03fa97b","Artificial intelligence (AI) along with deep learning techniques has become an integral part of almost all aspects of life. One of the domains significantly impacted by this technological revolution is healthcare. Deep learning-based AI systems assist clinicians and medical professionals in disease diagnosis, personalized treatment, and monitoring through wearables, among other applications. Despite its expedient integration in healthcare, the trustworthiness of deep learning models remains a concern, primarily due to a lack of understanding of their underlying processes. However, Explainable AI (XAI) offers explanations through various methods, including Local Interpretable Model-agnostic Explanations (LIME), Shapley Additive explanation (SHAP), and GRAD-CAM. XAI is utilized to enhance transparency, allowing users to understand and trust AI decisions. In this study, we present deep learning models for the classification of pneumonia disease in Chest X-ray Images followed by their explanations. Convolutional Neural Networks (CNNs) and other pre-trained models, including VGG16, MobileNetV3, and ResNet50, were used for classification of images as 'normal' or 'pneumonia'. The VGG16 model, known for its exceptional image understanding capabilities, achieved the highest accuracy, with an impressive 93% score. Further, we used XAI techniques including SHAP, LIME, and Grad-CAM for explanation of models. LIME and Grad-CAM provided more accurate results than SHAP in our experiments. This approach was taken to evaluate the fairness and transparency of the model. The insights gained from XAI can be used to refine and improve machine learning models by identifying areas of weakness or misinterpretation which increases overall model robustness.  © 2023 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85189137663"
"Dutta J.; Puthal D.","Dutta, Joy (57193125252); Puthal, Deepak (56151401500)","57193125252; 56151401500","Human-Centered Explainable AI at the Edge for eHealth","2023","Proceedings - IEEE International Conference on Edge Computing","2023-July","","","227","232","5","1","10.1109/EDGE60047.2023.00044","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173518563&doi=10.1109%2fEDGE60047.2023.00044&partnerID=40&md5=2bc4d3ce66e344f304bf113a07c6bb1a","Explainable Artificial Intelligence (XAI) is a new paradigm of Artificial Intelligence (AI) that is giving different AI/ Machine Learning (ML) models a boost to penetrate sectors where people are thinking about adopting AI. This work focuses on the adoption of XAI in the health sector. It portrays that careful integration of XAI in both cloud and edge could change the whole healthcare industry and make humans more aware of their present health conditions, which is the need of the hour. To demonstrate the same, we have done an experiment based on the prediction of a particular medical condition called ""cardiac arrest""in a specific subject group (patients who are 70 years old). Here, based on the explanation provided by the XAI model (e.g., SHAP, LIME) at Cloud and Edge, our system can predict the chances of a ""cardiac arrest""for the subject with a valid explanation. This type of model will be the next big upgrade in the healthcare industry in terms of automation and a self-explanatory system that works as a personal health assistant for individuals.  © 2023 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85173518563"
"Ponnusamy U.; Dhruva Darshan B.S.; Sampathila N.","Ponnusamy, Uhma (58833662500); Dhruva Darshan, B.S. (58833662600); Sampathila, Niranjana (56584740000)","58833662500; 58833662600; 56584740000","Approaching Explainable Artificial Intelligence Methods in the Diagnosis of Iron Deficiency Anemia Using Blood Parameters","2023","2023 International Conference on Recent Advances in Information Technology for Sustainable Development, ICRAIS 2023 - Proceedings","","","","201","206","5","0","10.1109/ICRAIS59684.2023.10367126","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182938687&doi=10.1109%2fICRAIS59684.2023.10367126&partnerID=40&md5=7a775357e51e59c36c9d0e6d9d59302f","Anemia is a global health disorder diagnosed by observing blood parameters. It is a tedious and time-consuming method for healthcare workers to analyze the data manually and may also lead to mistakes. This paper proposes a novel method to understand the impact of blood parameters in diagnosing anemia. Machine learning methods have been used to classify the data, and the impact of the attributes was explained using explainable AI tools to bring transparency and trust to the architectures. XAI helps in ensuring fairness, accountability, and transparency. The models show a high accuracy of 80-100%• The beeswarm plot explained the impact of the various attributes present in a complete blood count in the diagnosis of iron deficiency anemia. The methods introduced help in the quick diagnosis of anemia and save time for healthcare professionals. Improvement in the current technology in collaboration with healthcare workers will lead the medical domain to new heights. © 2023 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85182938687"
"Riis A.H.; Kristensen P.K.; Lauritsen S.M.; Thiesson B.; Jørgensen M.J.","Riis, Anders Hammerich (8675304100); Kristensen, Pia Kjær (56029876700); Lauritsen, Simon Meyer (57201018315); Thiesson, Bo (55892450200); Jørgensen, Marianne Johansson (55577646300)","8675304100; 56029876700; 57201018315; 55892450200; 55577646300","Using Explainable Artificial Intelligence to Predict Potentially Preventable Hospitalizations: A Population-Based Cohort Study in Denmark","2023","Medical Care","61","4","","226","236","10","2","10.1097/MLR.0000000000001830","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149619886&doi=10.1097%2fMLR.0000000000001830&partnerID=40&md5=3702afa5a78eddf22d3a80e018c0e089","Background: The increasing aging population and limited health care resources have placed new demands on the healthcare sector. Reducing the number of hospitalizations has become a political priority in many countries, and special focus has been directed at potentially preventable hospitalizations. Objectives: We aimed to develop an artificial intelligence (AI) prediction model for potentially preventable hospitalizations in the coming year, and to apply explainable AI to identify predictors of hospitalization and their interaction. Methods: We used the Danish CROSS-TRACKS cohort and included citizens in 2016-2017. We predicted potentially preventable hospitalizations within the following year using the citizens' sociodemographic characteristics, clinical characteristics, and health care utilization as predictors. Extreme gradient boosting was used to predict potentially preventable hospitalizations with Shapley additive explanations values serving to explain the impact of each predictor. We reported the area under the receiver operating characteristic curve, the area under the precision-recall curve, and 95% confidence intervals (CI) based on five-fold cross-validation. Results: The best performing prediction model showed an area under the receiver operating characteristic curve of 0.789 (CI: 0.782-0.795) and an area under the precision-recall curve of 0.232 (CI: 0.219-0.246). The predictors with the highest impact on the prediction model were age, prescription drugs for obstructive airway diseases, antibiotics, and use of municipality services. We found an interaction between age and use of municipality services, suggesting that citizens aged 75+ years receiving municipality services had a lower risk of potentially preventable hospitalization. Conclusion: AI is suitable for predicting potentially preventable hospitalizations. The municipality-based health services seem to have a preventive effect on potentially preventable hospitalizations. © 2023 Lippincott Williams and Wilkins. All rights reserved.","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85149619886"
"Moradi H.; Timothy Bunnell H.; Price B.S.; Khodaverdi M.; Vest M.T.; Porterfield J.Z.; Anzalone A.J.; Santangelo S.L.; Kimble W.; Harper J.; Hillegass W.B.; Hodder S.L.","Moradi, Hamidreza (57194046755); Timothy Bunnell, H. (6602451077); Price, Bradley S. (56684389800); Khodaverdi, Maryam (57221802799); Vest, Michael T. (36984468700); Porterfield, James Z. (55815840300); Anzalone, Alfred J. (57226382842); Santangelo, Susan L. (7003488082); Kimble, Wesley (57221505466); Harper, Jeremy (57348898500); Hillegass, William B. (6603945760); Hodder, Sally L. (35558534100)","57194046755; 6602451077; 56684389800; 57221802799; 36984468700; 55815840300; 57226382842; 7003488082; 57221505466; 57348898500; 6603945760; 35558534100","Assessing the effects of therapeutic combinations on SARS-CoV-2 infected patient outcomes: A big data approach","2023","PLoS ONE","18","3 March","e0282587","","","","0","10.1371/journal.pone.0282587","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149662351&doi=10.1371%2fjournal.pone.0282587&partnerID=40&md5=f4ebf852764f6f53eee42c962c76b806","Background The COVID-19 pandemic has demonstrated the need for efficient and comprehensive, simultaneous assessment of multiple combined novel therapies for viral infection across the range of illness severity. Randomized Controlled Trials (RCT) are the gold standard by which efficacy of therapeutic agents is demonstrated. However, they rarely are designed to assess treatment combinations across all relevant subgroups. A big data approach to analyzing real-world impacts of therapies may confirm or supplement RCT evidence to further assess effectiveness of therapeutic options for rapidly evolving diseases such as COVID-19. Methods Gradient Boosted Decision Tree, Deep and Convolutional Neural Network classifiers were implemented and trained on the National COVID Cohort Collaborative (N3C) data repository to predict the patients’ outcome of death or discharge. Models leveraged the patients’ characteristics, the severity of COVID-19 at diagnosis, and the calculated proportion of days on different treatment combinations after diagnosis as features to predict the outcome. Then, the most accurate model is utilized by eXplainable Artificial Intelligence (XAI) algorithms to provide insights about the learned treatment combination impacts on the model’s final outcome prediction. Results Gradient Boosted Decision Tree classifiers present the highest prediction accuracy in identifying patient outcomes with area under the receiver operator characteristic curve of 0.90 and accuracy of 0.81 for the outcomes of death or sufficient improvement to be discharged. The resulting model predicts the treatment combinations of anticoagulants and steroids are associated with the highest probability of improvement, followed by combined anticoagulants and targeted antivirals. In contrast, monotherapies of single drugs, including use of anticoagulants without steroid or antivirals are associated with poorer outcomes. Conclusions This machine learning model by accurately predicting the mortality provides insights about the treatment combinations associated with clinical improvement in COVID-19 patients. Analysis of the model’s components suggests benefit to treatment with combination of steroids, antivirals, and anticoagulant medication. The approach also provides a framework for simultaneously evaluating multiple real-world therapeutic combinations in future research studies. Copyright: © 2023 Moradi et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85149662351"
"Firza N.; Antonucci L.; Crocetta C.; d’Ovidio F.D.; Monaco A.","Firza, Najada (57929348200); Antonucci, Laura (57095763900); Crocetta, Corrado (35098361700); d’Ovidio, Francesco Domenico (13204259000); Monaco, Alfonso (7201639219)","57929348200; 57095763900; 35098361700; 13204259000; 7201639219","Spatial Analysis to Investigate the Relationship Between Tourism and Wellbeing in Italy","2023","Social Indicators Research","","","","","","","0","10.1007/s11205-023-03234-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175201944&doi=10.1007%2fs11205-023-03234-2&partnerID=40&md5=d33c564ca1fbe17fe5965dffa76e78fe","The level and variety of services offered by tourist destinations are intricately linked to the overall health and condition of its area. We would like to investigate the existence of a possible connection between tourism and the social, economic, and environmental well-being of a territory. The tourism industry can improve the general well-being of a specific area by promoting consumption, reducing the income gap, and improving infrastructures. However, the well-being of the territory through enhancing the specific features of the local context and its factors of excellence can also influence tourism. In this context, we applied Machine Learning methods to investigate the relationship between tourism and well-being in Italy. The analysis used Italian BES indicators at the provincial level, referred to a time window of 17 years (2004–2020). We developed a Machine Learning algorithm based on a hybrid (unsupervised and supervised) approach to study 51 well-being indexes and 9 tourism indicators. We found a close connection (80% of accuracy) between tourism and well-being. We also selected a group of tourism indicators that have a strong effect on this connection. Using eXplainable Artificial Intelligence (XAI) methods, we detected that tourism in low season periods ranks first for importance followed by the spread of farms business and urban green areas density. Our research suggests that improved social, economic, environmental, and health well-being can positively spill over the effect on tourism arrivals and revenues in the long period. © 2023, The Author(s).","Article","Article in press","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85175201944"
"Gurbuz E.; Turgut O.; Kok I.","Gurbuz, Ece (58572504700); Turgut, Ozlem (58572456300); Kok, Ibrahim (57200283688)","58572504700; 58572456300; 57200283688","Explainable AI-Based Malicious Traffic Detection and Monitoring System in Next-Gen IoT Healthcare","2023","2023 International Conference on Smart Applications, Communications and Networking, SmartNets 2023","","","","","","","0","10.1109/SmartNets58706.2023.10215896","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170638209&doi=10.1109%2fSmartNets58706.2023.10215896&partnerID=40&md5=a3fcda3b0c8dea945b68e13d873f15c9","In recent years, there has been a surge in IoT healthcare applications, ranging from wearable health monitors and remote patient monitoring systems to smart medical devices, telemedicine platforms, and personalized health tracking and management tools. The purpose of these applications is to improve treatment outcomes, streamline healthcare delivery, and enable data-driven decision-making. However, due to the sensitive nature of health data and the critical role that these applications play in people's lives, ensuring their security and privacy has become a paramount concern. To address this issue, we developed an explainable malicious traffic detection and monitoring system based on Machine Learning (ML) and Deep Learning (DL) models. The proposed system involves the use of Explainable Artificial Intelligence (XAI) methods such as LIME, SHAP, ELI5, and Integrated Gradients(IG) to ensure the interpretability and explainability of the developed models. Finally, we demonstrate the high accuracy of the developed models in detecting attacks on the intensive care patient dataset. Furthermore, we ensure the transparency and interpretability of the model outcomes by presenting them through the Shapash Monitor interface, which can be easily accessed by both experts and non-experts. © 2023 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85170638209"
"Pavicic M.; Walker A.M.; Sullivan K.A.; Lagergren J.; Cliff A.; Romero J.; Streich J.; Garvin M.R.; Pestian J.; McMahon B.; Oslin D.W.; Beckham J.C.; Kimbrel N.A.; Jacobson D.A.","Pavicic, Mirko (57193874019); Walker, Angelica M. (57211404457); Sullivan, Kyle A. (57200677258); Lagergren, John (57201419035); Cliff, Ashley (57212212927); Romero, Jonathon (57209345429); Streich, Jared (57204116291); Garvin, Michael R. (20433521400); Pestian, John (8058174500); McMahon, Benjamin (7102944152); Oslin, David W. (7005150966); Beckham, Jean C. (56713428900); Kimbrel, Nathan A. (15829507600); Jacobson, Daniel A. (56548186700)","57193874019; 57211404457; 57200677258; 57201419035; 57212212927; 57209345429; 57204116291; 20433521400; 8058174500; 7102944152; 7005150966; 56713428900; 15829507600; 56548186700","Using iterative random forest to find geospatial environmental and Sociodemographic predictors of suicide attempts","2023","Frontiers in Psychiatry","14","","1178633","","","","1","10.3389/fpsyt.2023.1178633","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168272825&doi=10.3389%2ffpsyt.2023.1178633&partnerID=40&md5=536f4e21919b331647c27e7352aa30fb","Introduction: Despite a recent global decrease in suicide rates, death by suicide has increased in the United States. It is therefore imperative to identify the risk factors associated with suicide attempts to combat this growing epidemic. In this study, we aim to identify potential risk factors of suicide attempt using geospatial features in an Artificial intelligence framework. Methods: We use iterative Random Forest, an explainable artificial intelligence method, to predict suicide attempts using data from the Million Veteran Program. This cohort incorporated 405,540 patients with 391,409 controls and 14,131 attempts. Our predictive model incorporates multiple climatic features at ZIP-code-level geospatial resolution. We additionally consider demographic features from the American Community Survey as well as the number of firearms and alcohol vendors per 10,000 people to assess the contributions of proximal environment, access to means, and restraint decrease to suicide attempts. In total 1,784 features were included in the predictive model. Results: Our results show that geographic areas with higher concentrations of married males living with spouses are predictive of lower rates of suicide attempts, whereas geographic areas where males are more likely to live alone and to rent housing are predictive of higher rates of suicide attempts. We also identified climatic features that were associated with suicide attempt risk by age group. Additionally, we observed that firearms and alcohol vendors were associated with increased risk for suicide attempts irrespective of the age group examined, but that their effects were small in comparison to the top features. Discussion: Taken together, our findings highlight the importance of social determinants and environmental factors in understanding suicide risk among veterans. 2023, At least a portion of this work is authored by David W. Oslin, Jean C. Beckham and Nathan A. Kimbrel on behalf of the U.S. Government and as regards Dr Oslin, Dr. Beckham, Dr. Kimbrel and the U.S. Government, is not subject to copyright protection in the United States. Foreign and other copyrights may apply.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85168272825"
"Murala D.K.; Panda S.K.; Dash S.P.","Murala, Dileep Kumar (57210106554); Panda, Sandeep Kumar (57217153128); Dash, Sujata Priyambada (57217993972)","57210106554; 57217153128; 57217993972","MedMetaverse: Medical Care of Chronic Disease Patients and Managing Data Using Artificial Intelligence, Blockchain, and Wearable Devices State-of-the-Art Methodology","2023","IEEE Access","11","","3340798","138954","138985","31","1","10.1109/ACCESS.2023.3340791","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179789290&doi=10.1109%2fACCESS.2023.3340791&partnerID=40&md5=af7ba27918170709ce53c90ef22b35d7","The Metaverse is an online universe that combines virtual reality and augmented reality, linked together via a network.It has generated novel experiences that are fully engaging and transpire in real time, facilitating interpersonal communication and dialogue. Virtual environments with 3D space and avatars can boost patient-facing platforms, operational utilisation, digital education, diagnostics, and treatment choices in medicine and ophthalmology. Globally, there is an increasing prevalence of chronic diseases, with an estimated 25 percent of individuals presently contending with multiple chronic health issues. The management of chronic diseases is currently being rethought in light of the development of technology known together as 'Smart Healthcare.' A prime example is state-of-the-art wearable technology that incentivizes people to embrace healthier lifestyles through the monitoring of physiological indicators and metabolic processes. With better data organisation and analysis, chronic disease patients may benefit from improved health, privacy, and quality of life. Through the examination of physiological data acquired from wearable devices on a patient, Artificial Intelligence (AI) has the capability to generate informed recommendations pertaining to the diagnosis and treatment of illness. These recommendations can be provided by AI. The adoption of blockchain technology (BC) has the potential to significantly advance healthcare in a variety of ways, including decentralised data sharing, user privacy, user empowerment, and dependability in data administration. The potential impact of Wearable Technologies (WT), Artificial Intelligence (AI), and Blockchain Technology (BC) on Chronic Disease Management (CDM) could be a transition in emphasis from the hospital to the patient. This article provides a patient-centered technical framework for controlling chronic diseases using artificial intelligence, blockchain, and wearable technologies. Our proposed architecture depends on Metaverse environment. In order to participate in the Metaverse, both patients and physicians need to sign up on the Blockchain network. After entering, customers will be accompanied by avatars throughout the experience. A comprehensive record of all information gathered during doctor-patient consultations, including text, videos, images, audio, and clinical data, will be compiled, uploaded to the blockchain, and stored in perpetuity. Explainable Artificial Intelligence (XAI) algorithms examine these particulars in order to diagnose and forecast the progression of diseases. We conclude with a discussion of the constraints of this novel paradigm and recommendations for future research.  © 2013 IEEE.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85179789290"
"Chen T.-C.T.","Chen, Tin-Chih Toly (8298017700)","8298017700","Enhancing the Sustainability of Smart Healthcare Applications with XAI","2023","SpringerBriefs in Applied Sciences and Technology","Part F1237","","","93","110","17","0","10.1007/978-3-031-37146-2_5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168682837&doi=10.1007%2f978-3-031-37146-2_5&partnerID=40&md5=c077d6699867540ccf48176130036cf9","This chapter first mentions three ways to enhance the sustainability of smart healthcare applications. Then, our experience from the COVID-19 pandemic tells us that the three goals can actually be achieved by applying explainable artificial intelligence (XAI). After introducing some basics of XAI, some representative cases of XAI applications in medicine and healthcare in the literature are discussed. Subsequently, two well-known applications of XAI in healthcare are detailed: an XAI application for enhancing the sustainability of a ubiquitous clinic recommendation system and another XAI application for enhancing the sustainability of an ANN-based diabetes diagnosis system. The two XAI applications enhanced both the understanding and trust of users, making them willing to use the system again and contributing to the sustainability of the smart healthcare applications. © 2023, The Author(s).","Book chapter","Final","","Scopus","2-s2.0-85168682837"
"Seetharaman T.; Sharma V.; Balamurugan B.; Grover V.; Agnihotri A.","Seetharaman, Tamizharasi (58802791700); Sharma, Vandana (58957195700); Balamurugan, B. (55050821600); Grover, Veena (58684938800); Agnihotri, Alka (57202075332)","58802791700; 58957195700; 55050821600; 58684938800; 57202075332","An Efficient and Robust Explainable Artificial Intelligence for Securing Smart Healthcare System","2023","2023 2nd International Conference on Smart Technologies for Smart Nation, SmartTechCon 2023","","","","1066","1071","5","0","10.1109/SmartTechCon57526.2023.10391664","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184799769&doi=10.1109%2fSmartTechCon57526.2023.10391664&partnerID=40&md5=c133e7ba5cc502b676dd4c3bda212643","The advent of IoT technologies has a tremendous impact on the healthcare sector enabling efficient monitoring of patients and utilizing the data for better analytics. Since every activity related to a patient’s health is monitored, the focus on smart healthcare applications has significantly transferred from service provision to a security perspective. As most healthcare applications are automated security plays a vital role. The technique of machine learning has been widely used in securing smart healthcare systems. The major challenge is that these applications require high-quality labeled images, which are difficult to acquire from real-time security applications. Further, it highly time-consuming and cost-expensive process. To address these constraints, in this paper, we define an efficient and robust explainable artificial intelligence technique that takes a small quantity of labeled data to train and de-ploy the security countermeasure for targeted healthcare applications. The proposed approach enhances the security measure through the detection of drifting samples with explainability. It is observed that the proposed approach improved accuracy, high fidelity, and explanation measures. Also, this approach is proven to be considerably resistant against numerous security threats. © 2023 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85184799769"
"Priya C.; Durai Raj Vincent P.M.","Priya, C. (57211184303); Durai Raj Vincent, P.M. (55808710700)","57211184303; 55808710700","An Efficient CSPK-FCM Explainable Artificial Intelligence Model on COVID-19 Data to Predict the Emotion Using Topic Modeling","2023","Journal of Advances in Information Technology","14","6","","1390","1402","12","0","10.12720/jait.14.6.1390-1402","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180525343&doi=10.12720%2fjait.14.6.1390-1402&partnerID=40&md5=89e8ac15a35745fc5aefea20c95c07a9","Incessant COVID-19 pandemic negatively impacts nations throughout the globe. It is necessary to determine how people react to public health interventions and understand their concerns. Twitter is a social media platform that has emerged as a tool for disseminating information, debating concepts, and reviewing or commenting on global issues. This study applies Explainable Artificial Intelligence (XAI) methods, like Cosine Similarity and Polynomial Kernel-centered Fuzzy C-Means (CSPK-FCM) centered topic modeling and Fuzzy Logic with Improved Long Short-Term Memory (FL-ILSTM) centered Sentiment Analysis to COVID-19 data on Twitter. The proposed model has five major steps: preprocessing, feature extraction, term weighting, topic modeling (clustering), and classification. Twitter comments relating to the COVID-19 pandemic are initially collected from publicly accessible websites. The collected data are then preprocessed to remove irrelevant information, namely, noises. The Feature Extraction phase is then performed by extracting emoticon and non-emoticon features. The extracted feature dataset is scored: the Term Frequency Inverse Document Frequency-Chi-Square (TFIDF-CHI) method is utilized for non-emoticon, and the score for the emoticon is assigned based on a few criteria. For Topic modeling, the TFIDF-CHI scores are provided to the CSPK-FCM clustering algorithm, which groups the most frequently discussed topics throughout COVID-19. FL-ILSTM executes the Sentiment analysis of clustered topics and emoticon features. It has extraordinary performance when compared to other methodologies. © 2023 by the authors.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85180525343"
"Islam N.; Kibria H.B.","Islam, Nazmin (58169966200); Kibria, Hafsa Binte (57222373843)","58169966200; 57222373843","Enhancing Stroke Prediction through Interpretable AI: Distinguishing Stroke Cases from Non-Stroke Cases","2023","2023 6th International Conference on Electrical Information and Communication Technology, EICT 2023","","","","","","","0","10.1109/EICT61409.2023.10427860","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186744953&doi=10.1109%2fEICT61409.2023.10427860&partnerID=40&md5=137f7e457dc8b60c1aa49e11ab2263b4","A sudden interruption of blood supply to part of the brain can result in a stroke, causing impairment and possible fatalities due to damage to brain cells. Rapid identification and intervention are crucial for preventing strokes and minimizing severe brain damage. The integration of Artificial Intelligence (AI) and machine learning (ML) shows promise in improving healthcare practitioners' ability to predict strokes swiftly and accurately. This study focuses on creating and assessing a stroke prediction model using a Random Forest (RF) classifier, supported by various performance metrics like AUC, precision, recall, F1-score, and accuracy, achieving a commendable accuracy rate of 94.6%. To address the class imbalance in the training set, a Random Over-Sampler was employed. Moreover, interpretability is enhanced through Explainable AI (XAI) techniques, specifically Shapley Additive Values (SHAP) and Local Interpretable Modelagnostic Explanations (LIME). By establishing an automated screening framework, this research aims to provide healthcare professionals with tools to offer more personalized and efficient care, potentially transforming stroke prevention and treatment. © 2023 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85186744953"
"Chen T.-C.T.","Chen, Tin-Chih Toly (8298017700)","8298017700","Smart Healthcare","2023","SpringerBriefs in Applied Sciences and Technology","Part F1237","","","1","18","17","0","10.1007/978-3-031-37146-2_1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168691664&doi=10.1007%2f978-3-031-37146-2_1&partnerID=40&md5=9bf189da951434d6827d3082c09f4107","This chapter first defines smart healthcare and smart healthcare technologies. After observing changes in the number of smart healthcare applications explored before, during, and after the COVID-19 pandemic, we have to question the sustainability of smart healthcare applications. Answering this question requires a reassessment of the sustainability of each smart healthcare application. To this end, some common smart healthcare applications, including smartphone applications (apps), smartwatch applications, location-aware services (LASs), telemedicine and telecare, Internet of Things (IoT) applications, machine learning (ML) and deep learning (DL) applications, are first introduced by reviewing examples from the literature. The sustainabilities of these smart healthcare applications are then subjectively estimated. However, the actual sustainability of each smart healthcare application will be determined on a case-by-case basis and should be compared with those of other applications. © 2023, The Author(s).","Book chapter","Final","","Scopus","2-s2.0-85168691664"
"Yilmaz R.; Yagin F.H.; Raza A.; Colak C.; Akinci T.C.","Yilmaz, Rustem (58582676500); Yagin, Fatma Hilal (57211715604); Raza, Ali (56072492500); Colak, Cemil (11738942300); Akinci, Tahir Cetin (16229256000)","58582676500; 57211715604; 56072492500; 11738942300; 16229256000","Assessment of Hematological Predictors via Explainable Artificial Intelligence in the Prediction of Acute Myocardial Infarction","2023","IEEE Access","11","","","108591","108602","11","2","10.1109/ACCESS.2023.3321509","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174805768&doi=10.1109%2fACCESS.2023.3321509&partnerID=40&md5=0c10e050a829ca6114f3de07a4b11a57","Acute myocardial infarction (AMI) is the main cause of death in developed and developing countries. AMI is a serious medical problem that necessitates hospitalization and sometimes results in death. Patients hospitalized in the emergency department (ED) should therefore receive an immediate diagnosis and treatment. Many studies have been conducted on the prognosis of AMI with hemogram parameters. However, no study has investigated potential hemogram parameters for the diagnosis of AMI using an interpretable artificial intelligence-based clinical approach. The purpose of this research is to implement the principles of explainable artificial intelligence (XAI) in the analysis of hematological predictors for AMI. In this retrospective analysis, 477 (48.6%) patients with AMI and 504 (51.4%) healthy individuals were enrolled and assessed in predicting AMI. Of the patients with AMI, 182 (38%) had an ST-segment elevation MI (STEMI), and 295 (62%) had a non-ST-segment elevation MI (NSTEMI). Demographic and hematological information of the patients was analyzed to determine AMI. The XAI approach combined with machine learning approaches (Extreme Gradient Boosting, XGB; Adaptive Boosting, AB; Light Gradient Boosting Machine, LGBM) was applied for the estimation of AMI and distinguishing subgroups of AMI (STEMI and NSTEMI). The SHAP approach was used to explain the predictions intuitively. After selecting the 10 most important hematological parameters for AMI, the LGBM model achieved 83% and 74% accuracy for prediction of AMI, and distinguishing subgroups of AMI (STEMI and NSTEMI), respectively. SHAP results showed that neutrophil (NEU), white blood cell (WBC), platelet width of distribution (PDW), and basophil (BA) were the most important for AMI prediction. Mean corpuscular volume (MCV), BA, monocytes (MO), and lymphocytes (LY) were the most important hematological parameters that distinguish STEMI from NSTEMI. The proposed model serves as a valuable tool for physicians, facilitating the diagnosis, treatment, and follow-up of patients with AMI and distinguishing subgroups of AMI (STEMI and NSTEMI). Analyzing readily accessible hemogram parameters empowers medical professionals to make informed decisions and provide enhanced care to a wide range of individuals.  © 2013 IEEE.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85174805768"
"Bhattacharya A.; Ooge J.; Stiglic G.; Verbert K.","Bhattacharya, Aditya (58119597200); Ooge, Jeroen (57220642214); Stiglic, Gregor (6506110486); Verbert, Katrien (13605498800)","58119597200; 57220642214; 6506110486; 13605498800","Directive Explanations for Monitoring the Risk of Diabetes Onset: Introducing Directive Data-Centric Explanations and Combinations to Support What-If Explorations","2023","International Conference on Intelligent User Interfaces, Proceedings IUI","","","","204","219","15","2","10.1145/3581641.3584075","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152123065&doi=10.1145%2f3581641.3584075&partnerID=40&md5=5547b7133261759a290492786ef4c24f","Explainable artificial intelligence is increasingly used in machine learning (ML) based decision-making systems in healthcare. However, little research has compared the utility of different explanation methods in guiding healthcare experts for patient care. Moreover, it is unclear how useful, understandable, actionable and trustworthy these methods are for healthcare experts, as they often require technical ML knowledge. This paper presents an explanation dashboard that predicts the risk of diabetes onset and explains those predictions with data-centric, feature-importance, and example-based explanations. We designed an interactive dashboard to assist healthcare experts, such as nurses and physicians, in monitoring the risk of diabetes onset and recommending measures to minimize risk. We conducted a qualitative study with 11 healthcare experts and a mixed-methods study with 45 healthcare experts and 51 diabetic patients to compare the different explanation methods in our dashboard in terms of understandability, usefulness, actionability, and trust. Results indicate that our participants preferred our representation of data-centric explanations that provide local explanations with a global overview over other methods. Therefore, this paper highlights the importance of visually directive data-centric explanation method for assisting healthcare experts to gain actionable insights from patient health records. Furthermore, we share our design implications for tailoring the visual representation of different explanation methods for healthcare experts.  © 2023 ACM.","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85152123065"
"Javed A.R.; Ahmed W.; Pandya S.; Maddikunta P.K.R.; Alazab M.; Gadekallu T.R.","Javed, Abdul Rehman (57215024733); Ahmed, Waqas (57224316161); Pandya, Sharnil (57200178916); Maddikunta, Praveen Kumar Reddy (57219858463); Alazab, Mamoun (36661792200); Gadekallu, Thippa Reddy (57217062630)","57215024733; 57224316161; 57200178916; 57219858463; 36661792200; 57217062630","A Survey of Explainable Artificial Intelligence for Smart Cities","2023","Electronics (Switzerland)","12","4","1020","","","","50","10.3390/electronics12041020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148854883&doi=10.3390%2felectronics12041020&partnerID=40&md5=51b118092941b9d0e3e5fd03fd8f357d","The emergence of Explainable Artificial Intelligence (XAI) has enhanced the lives of humans and envisioned the concept of smart cities using informed actions, enhanced user interpretations and explanations, and firm decision-making processes. The XAI systems can unbox the potential of black-box AI models and describe them explicitly. The study comprehensively surveys the current and future developments in XAI technologies for smart cities. It also highlights the societal, industrial, and technological trends that initiate the drive towards XAI for smart cities. It presents the key to enabling XAI technologies for smart cities in detail. The paper also discusses the concept of XAI for smart cities, various XAI technology use cases, challenges, applications, possible alternative solutions, and current and future research enhancements. Research projects and activities, including standardization efforts toward developing XAI for smart cities, are outlined in detail. The lessons learned from state-of-the-art research are summarized, and various technical challenges are discussed to shed new light on future research possibilities. The presented study on XAI for smart cities is a first-of-its-kind, rigorous, and detailed study to assist future researchers in implementing XAI-driven systems, architectures, and applications for smart cities. © 2023 by the authors.","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85148854883"
"Khalasi D.; Bathwar D.; Bhatia J.; Kumhar M.; Thumar V.","Khalasi, Darshi (58654297100); Bathwar, Devang (58654297200); Bhatia, Jitendra (55062577500); Kumhar, Malaram (57205209368); Thumar, Vinodray (57163483100)","58654297100; 58654297200; 55062577500; 57205209368; 57163483100","Secure and Explainable Artificial Intelligence (XAI) in Cloud Ecosystems: Challenges and Opportunities","2023","Lecture Notes in Networks and Systems","754 LNNS","","","553","567","14","0","10.1007/978-981-99-4932-8_51","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174437114&doi=10.1007%2f978-981-99-4932-8_51&partnerID=40&md5=e392b192391358465b71ce6e3e789f0d","The widespread adoption of artificial intelligence (AI) in cloud ecosystems has revolutionized various domains, offering significant advantages such as improved efficiency, scalability, and cost-effectiveness. However, the use of AI also poses significant security risks, and the need for more transparency in AI models makes it difficult to understand how decisions are made. This paper reviews recent research efforts toward developing secure and explainable AI (XAI) for cloud ecosystems, highlights the critical need for secure XAI in cloud ecosystems, and explores the challenges and potential solutions. The paper also discusses a case study involving the security of cloud-based health care with intrusion detection systems (IDS). Finally, the paper concludes with open research directions in this domain and their potential impact on cloud ecosystems. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd 2023.","Conference paper","Final","","Scopus","2-s2.0-85174437114"
"Togo M.V.; Mastrolorito F.; Ciriaco F.; Trisciuzzi D.; Tondo A.R.; Gambacorta N.; Bellantuono L.; Monaco A.; Leonetti F.; Bellotti R.; Altomare C.D.; Amoroso N.; Nicolotti O.","Togo, Maria Vittoria (58024578700); Mastrolorito, Fabrizio (58023576900); Ciriaco, Fulvio (6507486891); Trisciuzzi, Daniela (56925495300); Tondo, Anna Rita (57209312574); Gambacorta, Nicola (57211413049); Bellantuono, Loredana (56166549700); Monaco, Alfonso (7201639219); Leonetti, Francesco (57208994987); Bellotti, Roberto (8419904800); Altomare, Cosimo Damiano (7005856522); Amoroso, Nicola (55419832300); Nicolotti, Orazio (6603050960)","58024578700; 58023576900; 6507486891; 56925495300; 57209312574; 57211413049; 56166549700; 7201639219; 57208994987; 8419904800; 7005856522; 55419832300; 6603050960","TIRESIA: An eXplainable Artificial Intelligence Platform for Predicting Developmental Toxicity","2023","Journal of Chemical Information and Modeling","63","1","","56","66","10","13","10.1021/acs.jcim.2c01126","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144342381&doi=10.1021%2facs.jcim.2c01126&partnerID=40&md5=4d183974248f4f98848663ee4ed54d81","Herein, a robust and reproducible eXplainable Artificial Intelligence (XAI) approach is presented, which allows prediction of developmental toxicity, a challenging human-health endpoint in toxicology. The application of XAI as an alternative method is of the utmost importance with developmental toxicity being one of the most animal-intensive areas of regulatory toxicology. In this work, the established CAESAR (Computer Assisted Evaluation of industrial chemical Substances According to Regulations) training set made of 234 chemicals for model learning is employed. Two test sets, including as a whole 585 chemicals, were instead used for validation and generalization purposes. The proposed framework favorably compares with the state-of-the-art approaches in terms of accuracy, sensitivity, and specificity, thus resulting in a reliable support system for developmental toxicity ensuring informativeness, uncertainty estimation, generalization, and transparency. Based on the eXtreme Gradient Boosting (XGB) algorithm, our predictive model provides easy interpretative keys based on specific molecular descriptors and structural alerts enabling one to distinguish toxic and nontoxic chemicals. Inspired by the Organisation for Economic Co-operation and Development (OECD) principles for the validation of Quantitative Structure-Activity Relationships (QSARs) for regulatory purposes, the results are summarized in a standard report in portable document format, enclosing also details concerned with a density-based model applicability domain and SHAP (SHapley Additive exPlanations) explainability, the latter particularly useful to better understand the effective roles played by molecular features. Notably, our model has been implemented in TIRESIA (Toxicology Intelligence and Regulatory Evaluations for Scientific and Industry Applications), a free of charge web platform available at http://tiresia.uniba.it. © 2022 American Chemical Society.","Article","Final","","Scopus","2-s2.0-85144342381"
"GhoshRoy D.; Alvi P.A.; Santosh K.C.","GhoshRoy, Debasmita (57204946002); Alvi, Parvez Ahmad (12753225500); Santosh, K.C. (14831502300)","57204946002; 12753225500; 14831502300","Unboxing Industry-Standard AI Models for Male Fertility Prediction with SHAP","2023","Healthcare (Switzerland)","11","7","929","","","","4","10.3390/healthcare11070929","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152674771&doi=10.3390%2fhealthcare11070929&partnerID=40&md5=15f216e13c55dbbca86c1098f5d69310","Infertility is a social stigma for individuals, and male factors cause approximately 30% of infertility. Despite this, male infertility is underrecognized and underrepresented as a disease. According to the World Health Organization (WHO), changes in lifestyle and environmental factors are the prime reasons for the declining rate of male fertility. Artificial intelligence (AI)/machine learning (ML) models have become an effective solution for early fertility detection. Seven industry-standard ML models are used: support vector machine, random forest (RF), decision tree, logistic regression, naïve bayes, adaboost, and multi-layer perception to detect male fertility. Shapley additive explanations (SHAP) are vital tools that examine the feature’s impact on each model’s decision making. On these, we perform a comprehensive comparative study to identify good and poor classification models. While dealing with the all-above-mentioned models, the RF model achieves an optimal accuracy and area under curve (AUC) of 90.47% and 99.98%, respectively, by considering five-fold cross-validation (CV) with the balanced dataset. Furthermore, we provide the SHAP explanations of existing models that attain good and poor performance. The findings of this study show that decision making (based on ML models) with SHAP provides thorough explanations for detecting male fertility, as well as a reference for clinicians for further treatment planning. © 2023 by the authors.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85152674771"
"Hendawi R.; Li J.; Roy S.","Hendawi, Rasha (57223106417); Li, Juan (35069740100); Roy, Souradip (57277649300)","57223106417; 35069740100; 57277649300","A Mobile App That Addresses Interpretability Challenges in Machine Learning–Based Diabetes Predictions: Survey-Based User Study","2023","JMIR Formative Research","7","1","e50328","","","","1","10.2196/50328","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178111667&doi=10.2196%2f50328&partnerID=40&md5=a1f8d7929c693af278cfb2f1aceaf64a","Background: Machine learning approaches, including deep learning, have demonstrated remarkable effectiveness in the diagnosis and prediction of diabetes. However, these approaches often operate as opaque black boxes, leaving health care providers in the dark about the reasoning behind predictions. This opacity poses a barrier to the widespread adoption of machine learning in diabetes and health care, leading to confusion and eroding trust. Objective: This study aimed to address this critical issue by developing and evaluating an explainable artificial intelligence (AI) platform, XAI4Diabetes, designed to empower health care professionals with a clear understanding of AI-generated predictions and recommendations for diabetes care. XAI4Diabetes not only delivers diabetes risk predictions but also furnishes easily interpretable explanations for complex machine learning models and their outcomes. Methods: XAI4Diabetes features a versatile multimodule explanation framework that leverages machine learning, knowledge graphs, and ontologies. The platform comprises the following four essential modules: (1) knowledge base, (2) knowledge matching, (3) prediction, and (4) interpretation. By harnessing AI techniques, XAI4Diabetes forecasts diabetes risk and provides valuable insights into the prediction process and outcomes. A structured, survey-based user study assessed the app’s usability and influence on participants’ comprehension of machine learning predictions in real-world patient scenarios. Results: A prototype mobile app was meticulously developed and subjected to thorough usability studies and satisfaction surveys. The evaluation study findings underscore the substantial improvement in medical professionals’ comprehension of key aspects, including the (1) diabetes prediction process, (2) data sets used for model training, (3) data features used, and (4) relative significance of different features in prediction outcomes. Most participants reported heightened understanding of and trust in AI predictions following their use of XAI4Diabetes. The satisfaction survey results further revealed a high level of overall user satisfaction with the tool. Conclusions: This study introduces XAI4Diabetes, a versatile multi-model explainable prediction platform tailored to diabetes care. By enabling transparent diabetes risk predictions and delivering interpretable insights, XAI4Diabetes empowers health care professionals to comprehend the AI-driven decision-making process, thereby fostering transparency and trust. These advancements hold the potential to mitigate biases and facilitate the broader integration of AI in diabetes care. ©Rasha Hendawi, Juan Li, Souradip Roy.","Article","Final","","Scopus","2-s2.0-85178111667"
"Shi W.; Murakoso M.; Guo X.; Xiong L.; Chen M.; Wang M.D.","Shi, Wenqi (57220119218); Murakoso, Mio (58075341500); Guo, Xiaoyan (58884404600); Xiong, Linxi (58884704100); Chen, Matthew (58884912000); Wang, May D. (57700374400)","57220119218; 58075341500; 58884404600; 58884704100; 58884912000; 57700374400","Effective Surrogate Models for Docking Scores Prediction of Candidate Drug Molecules on SARS-CoV-2 Protein Targets","2023","Proceedings - 2023 2023 IEEE International Conference on Bioinformatics and Biomedicine, BIBM 2023","","","","4235","4242","7","0","10.1109/BIBM58861.2023.10385643","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184857713&doi=10.1109%2fBIBM58861.2023.10385643&partnerID=40&md5=81c4a8ff79224c43e9814765a649d50e","Emerging infectious diseases, such as coronavirus disease 2019 (COVID-19), pose a major threat to public health and present a critical challenge for drug discovery. Due to the cost- and time-consuming process of new drug development, virtual pre-screening methods such as protein-ligand docking prediction have become essential tools in enhancing drug refurbishment and repurposing. In this study, we propose a machine learning-based surrogate model for docking score prediction of drug candidates on SARS-CoV-2 protein targets via deep feature concatenation. We investigate 14 different combinations of rule-based and data-driven fingerprinting methods to identify the optimal representation of candidate drug molecules. Extensive experiments on docking scores of 270,000 molecules across 18 different SARS-CoV-2 protein targets demonstrate the effectiveness of the proposed surrogate models. In addition to unseen drugs, we further investigate the generalization of the proposed framework for unseen protein targets. This study may provide an instrumental and generalizable framework for exploring ligand-protein interaction, serving as a useful tool to facilitate rapid drug pre-screening during emerging public health crises.  © 2023 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85184857713"
"Loh H.W.; Ooi C.P.; Oh S.L.; Barua P.D.; Tan Y.R.; Acharya U.R.; Fung D.S.S.","Loh, Hui Wen (57220933535); Ooi, Chui Ping (55663773200); Oh, Shu Lih (57185991600); Barua, Prabal Datta (36993665100); Tan, Yi Ren (57202098810); Acharya, U. Rajendra (7004510847); Fung, Daniel Shuen Sheng (7103139903)","57220933535; 55663773200; 57185991600; 36993665100; 57202098810; 7004510847; 7103139903","ADHD/CD-NET: automated EEG-based characterization of ADHD and CD using explainable deep neural network technique","2023","Cognitive Neurodynamics","","","","","","","0","10.1007/s11571-023-10028-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177792709&doi=10.1007%2fs11571-023-10028-2&partnerID=40&md5=ffb1731b57c95d7417fd93ec7bed8a7d","In this study, attention deficit hyperactivity disorder (ADHD), a childhood neurodevelopmental disorder, is being studied alongside its comorbidity, conduct disorder (CD), a behavioral disorder. Because ADHD and CD share commonalities, distinguishing them is difficult, thus increasing the risk of misdiagnosis. It is crucial that these two conditions are not mistakenly identified as the same because the treatment plan varies depending on whether the patient has CD or ADHD. Hence, this study proposes an electroencephalogram (EEG)-based deep learning system known as ADHD/CD-NET that is capable of objectively distinguishing ADHD, ADHD + CD, and CD. The 12-channel EEG signals were first segmented and converted into channel-wise continuous wavelet transform (CWT) correlation matrices. The resulting matrices were then used to train the convolutional neural network (CNN) model, and the model’s performance was evaluated using 10-fold cross-validation. Gradient-weighted class activation mapping (Grad-CAM) was also used to provide explanations for the prediction result made by the ‘black box’ CNN model. Internal private dataset (45 ADHD, 62 ADHD + CD and 16 CD) and external public dataset (61 ADHD and 60 healthy controls) were used to evaluate ADHD/CD-NET. As a result, ADHD/CD-NET achieved classification accuracy, sensitivity, specificity, and precision of 93.70%, 90.83%, 95.35% and 91.85% for the internal evaluation, and 98.19%, 98.36%, 98.03% and 98.06% for the external evaluation. Grad-CAM also identified significant channels that contributed to the diagnosis outcome. Therefore, ADHD/CD-NET can perform temporal localization and choose significant EEG channels for diagnosis, thus providing objective analysis for mental health professionals and clinicians to consider when making a diagnosis. © 2023, The Author(s).","Article","Article in press","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85177792709"
"Sarp S.; Catak F.O.; Kuzlu M.; Cali U.; Kusetogullari H.; Zhao Y.; Ates G.; Guler O.","Sarp, Salih (57217103054); Catak, Ferhat Ozgur (55625667200); Kuzlu, Murat (35409671400); Cali, Umit (54974113000); Kusetogullari, Huseyin (22941197000); Zhao, Yanxiao (8339346500); Ates, Gungor (35241925700); Guler, Ozgur (36466376200)","57217103054; 55625667200; 35409671400; 54974113000; 22941197000; 8339346500; 35241925700; 36466376200","An XAI approach for COVID-19 detection using transfer learning with X-ray images","2023","Heliyon","9","4","e15137","","","","13","10.1016/j.heliyon.2023.e15137","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151627296&doi=10.1016%2fj.heliyon.2023.e15137&partnerID=40&md5=24f8dd069afae643f8c1a506c02a2677","The coronavirus disease (COVID-19) has continued to cause severe challenges during this unprecedented time, affecting every part of daily life in terms of health, economics, and social development. There is an increasing demand for chest X-ray (CXR) scans, as pneumonia is the primary and vital complication of COVID-19. CXR is widely used as a screening tool for lung-related diseases due to its simple and relatively inexpensive application. However, these scans require expert radiologists to interpret the results for clinical decisions, i.e., diagnosis, treatment, and prognosis. The digitalization of various sectors, including healthcare, has accelerated during the pandemic, with the use and importance of Artificial Intelligence (AI) dramatically increasing. This paper proposes a model using an Explainable Artificial Intelligence (XAI) technique to detect and interpret COVID-19 positive CXR images. We further analyze the impact of COVID-19 positive CXR images using heatmaps. The proposed model leverages transfer learning and data augmentation techniques for faster and more adequate model training. Lung segmentation is applied to enhance the model performance further. We conducted a pre-trained network comparison with the highest classification performance (F1-Score: 98%) using the ResNet model. © 2023 The Author(s)","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85151627296"
"Bhatia S.; Albarrak A.S.","Bhatia, Surbhi (56655934600); Albarrak, Abdulaziz Saad (57224776818)","56655934600; 57224776818","A Blockchain-Driven Food Supply Chain Management Using QR Code and XAI-Faster RCNN Architecture","2023","Sustainability (Switzerland)","15","3","2579","","","","11","10.3390/su15032579","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148003638&doi=10.3390%2fsu15032579&partnerID=40&md5=9629338c7945630eb895ed630092b14e","The availability of food in a country and the capacity of its citizens to access, acquire, and receive enough food are both referred to as having food security. A crucial component of food security is ensuring and maintaining safe and high-quality goods, which the supply chain process should take into due deliberation. To enhance the food supply chain, organic and wholesome food items should be encouraged. Although packaged goods are evaluated and approved by legal authorities, there is no mechanism in place for testing and assessing the market’s available supply on a regular basis. As a result, food manufacturers are compelled to provide nutritious and healthy products. In this research, we propose an explainable artificial intelligence-based faster regions with convolutional neural networks (XAI-based Faster RCNN) model to evaluate the contents of the food items through user-friendly web-based front-end design and QR code. To validate each communication token in the network, an elliptic curve integrated encrypted scheme (ECIES) based on blockchain technology is utilized. Additionally, artificial rabbit optimization (ARO) is used to register each user and assign him a key. The user will gain a deeper understanding of machine learning (ML) and AI applications using the XAI technique. An EAI-based Faster RCNN model is proposed to help digitize information about food products, rapidly retrieve the information, and discover any hidden information in the quick response (QR) code that could have impacted the safety and quality of the food. The results of the experiments indicated that the proposed method requires less response time than other existing methods with the increase of payload and users. The Shapley additive explanation is used to obtain a legal plea for the laboratory test based on the nutritional information present in the QR code. The benefits provided by ECIES-based blockchain technology assist policymakers, manufacturers, and merchants in efficient decision-making, minimizing public health hazards, and improving welfare. This paper also shows that the accuracy achieved by the proposed method reached 99.53%, with the lowest processing time. © 2023 by the authors.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85148003638"
"Nazir S.; Dickson D.M.; Akram M.U.","Nazir, Sajid (47061625500); Dickson, Diane M. (57075710900); Akram, Muhammad Usman (24474159700)","47061625500; 57075710900; 24474159700","Survey of explainable artificial intelligence techniques for biomedical imaging with deep neural networks","2023","Computers in Biology and Medicine","156","","106668","","","","52","10.1016/j.compbiomed.2023.106668","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149371270&doi=10.1016%2fj.compbiomed.2023.106668&partnerID=40&md5=805a0d649d8ba98c2502e4344fbff105","Artificial Intelligence (AI) techniques of deep learning have revolutionized the disease diagnosis with their outstanding image classification performance. In spite of the outstanding results, the widespread adoption of these techniques in clinical practice is still taking place at a moderate pace. One of the major hindrance is that a trained Deep Neural Networks (DNN) model provides a prediction, but questions about why and how that prediction was made remain unanswered. This linkage is of utmost importance for the regulated healthcare domain to increase the trust in the automated diagnosis system by the practitioners, patients and other stakeholders. The application of deep learning for medical imaging has to be interpreted with caution due to the health and safety concerns similar to blame attribution in the case of an accident involving autonomous cars. The consequences of both a false positive and false negative cases are far reaching for patients' welfare and cannot be ignored. This is exacerbated by the fact that the state-of-the-art deep learning algorithms comprise of complex interconnected structures, millions of parameters, and a ‘black box’ nature, offering little understanding of their inner working unlike the traditional machine learning algorithms. Explainable AI (XAI) techniques help to understand model predictions which help develop trust in the system, accelerate the disease diagnosis, and meet adherence to regulatory requirements. This survey provides a comprehensive review of the promising field of XAI for biomedical imaging diagnostics. We also provide a categorization of the XAI techniques, discuss the open challenges, and provide future directions for XAI which would be of interest to clinicians, regulators and model developers. © 2023 The Authors","Review","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85149371270"
"Turchioe M.R.; Hermann A.; Benda N.C.","Turchioe, Meghan Reading (57195221233); Hermann, Alison (57191965467); Benda, Natalie C. (56501780400)","57195221233; 57191965467; 56501780400","Recentering responsible and explainable artificial intelligence research on patients: implications in perinatal psychiatry","2023","Frontiers in Psychiatry","14","","1321265","","","","0","10.3389/fpsyt.2023.1321265","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183708394&doi=10.3389%2ffpsyt.2023.1321265&partnerID=40&md5=3aa78db2458ed2e9ac6caf41f1127985","In the setting of underdiagnosed and undertreated perinatal depression (PD), Artificial intelligence (AI) solutions are poised to help predict and treat PD. In the near future, perinatal patients may interact with AI during clinical decision-making, in their patient portals, or through AI-powered chatbots delivering psychotherapy. The increase in potential AI applications has led to discussions regarding responsible AI and explainable AI (XAI). Current discussions of RAI, however, are limited in their consideration of the patient as an active participant with AI. Therefore, we propose a patient-centered, rather than a patient-adjacent, approach to RAI and XAI, that identifies autonomy, beneficence, justice, trust, privacy, and transparency as core concepts to uphold for health professionals and patients. We present empirical evidence that these principles are strongly valued by patients. We further suggest possible design solutions that uphold these principles and acknowledge the pressing need for further research about practical applications to uphold these principles. Copyright © 2024 Turchioe, Hermann and Benda.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85183708394"
"Vakharia V.; Shah M.; Nair P.; Borade H.; Sahlot P.; Wankhede V.","Vakharia, Vinay (56511613400); Shah, Milind (57678442500); Nair, Pranav (58116923300); Borade, Himanshu (57214892940); Sahlot, Pankaj (57191405876); Wankhede, Vishal (57204053781)","56511613400; 57678442500; 58116923300; 57214892940; 57191405876; 57204053781","Estimation of Lithium-ion Battery Discharge Capacity by Integrating Optimized Explainable-AI and Stacked LSTM Model","2023","Batteries","9","2","125","","","","35","10.3390/batteries9020125","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148852212&doi=10.3390%2fbatteries9020125&partnerID=40&md5=28fee22eb6c5020d8c4f5cf617645a25","Accurate lithium-ion battery state of health evaluation is crucial for correctly operating and managing battery-based energy storage systems. Experimental determination is problematic in these applications since standard functioning is necessary. Machine learning techniques enable accurate and effective data-driven predictions in such situations. In the present paper, an optimized explainable artificial intelligence (Ex-AI) model is proposed to predict the discharge capacity of the battery. In the initial stage, three deep learning (DL) models, stacked long short-term memory networks (stacked LSTMs), gated recurrent unit (GRU) networks, and stacked recurrent neural networks (SRNNs) were developed based on the training of six input features. Ex-AI was applied to identify the relevant features and further optimize Ex-AI operating parameters, and the jellyfish metaheuristic optimization technique was considered. The results reveal that discharge capacity was better predicted when the jellyfish-Ex-AI model was applied. A very low RMSE of 0.04, MAE of 0.60, and MAPE of 0.03 were observed with the Stacked-LSTM model, demonstrating our proposed methodology’s utility. © 2023 by the authors.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85148852212"
"Saleh R.A.A.; Farea S.M.; Al-Huda Z.; Ertunc M.; Kvak D.; Al-Antari M.A.","Saleh, Radhwan A. A. (57223295367); Farea, Shawqi Mohammed (57240943700); Al-Huda, Zaid (57207762604); Ertunc, Metin (58995926200); Kvak, Daniel (57564860400); Al-Antari, Mugahed A. (57189003551)","57223295367; 57240943700; 57207762604; 58995926200; 57564860400; 57189003551","FXAI: Fusing XAI for Predicting COVID-19 Using Diverse Chest X - Ray Images","2023","ISKE 2023 - 18th International Conference on Intelligent Systems and Knowledge Engineering","","","","86","92","6","0","10.1109/ISKE60036.2023.10481318","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190945635&doi=10.1109%2fISKE60036.2023.10481318&partnerID=40&md5=496391e04b66f6cbb900a96cc7a00270","Fusing explainable artificial intelligence (FXAI) is currently a prominent research topic in medical imaging interpretations. The proposed FXAI has the capability to provide the following benefits. Firstly, it can extract strong and reliable high-level deep features by combining various standard AI networks. Secondly, it can simultaneously generate visual explainable saliency maps associated with each chest X-ray (CXR) scan. Such heat maps not only demonstrate the most relevant regions of the AI decision-making process but also offer advantages to radiologists and patients. Thirdly, it enhances prediction performance to deliver an optimal intelligent solution for communities worldwide. These advantages can support the development of an optimal treatment plan, reduce medical costs, and enhance the capabilities of health care systems. We have trained and evaluated the proposed FXAI using a diverse benchmark medical CXR dataset that has been collected from various public resources. Our findings encourage researchers and stakeholders in the medical industry to validate this proposed framework in a practical manner.  © 2023 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85190945635"
"Amin A.; Hasan K.; Zein-Sabatto S.; Chimba D.; Ahmed I.; Islam T.","Amin, Al (58144013300); Hasan, Kamrul (57200588757); Zein-Sabatto, Saleh (6701653211); Chimba, Deo (34978330600); Ahmed, Imtiaz (55866677500); Islam, Tariqul (57213772433)","58144013300; 57200588757; 6701653211; 34978330600; 55866677500; 57213772433","An Explainable AI Framework for Artificial Intelligence of Medical Things","2023","2023 IEEE Globecom Workshops, GC Wkshps 2023","","","","2097","2102","5","0","10.1109/GCWkshps58843.2023.10464798","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190272373&doi=10.1109%2fGCWkshps58843.2023.10464798&partnerID=40&md5=abfad014cc3f22932ec522055cdd2868","The healthcare industry has been revolutionized by the convergence of Artificial Intelligence of Medical Things (AIoMT), allowing advanced data-driven solutions to improve healthcare systems. With the increasing complexity of Artificial Intelligence (AI) models, the need for Explainable Artificial Intelligence (XAI) techniques become paramount, particularly in the medical domain, where transparent and interpretable decision-making becomes crucial. Therefore, in this work, we leverage a custom XAI framework, incorporating techniques such as Local Interpretable Model-Agnostic Explanations (LIME), SHapley Additive exPlanations (SHAP), and Gradient-weighted Class Activation Mapping (Grad-Cam), explicitly designed for the domain of AIoMT. The proposed framework enhances the effectiveness of strategic healthcare methods and aims to instill trust and promote understanding in AI-driven medical applications. Moreover, we utilize a majority voting technique that aggregates predictions from multiple convolutional neural networks (CNNs) and leverages their collective intelligence to make robust and accurate decisions in the healthcare system. Building upon this decision-making process, we apply the XAI framework to brain tumor detection as a use case demon strating accurate and transparent diagnosis. Evaluation results underscore the exceptional performance of the XAI framework, achieving high precision, recall, and F1 scores with a training accuracy of 99% and a validation accuracy of 98%. Combining advanced XAI techniques with ensemble-based deep-learning (DL) methodologies allows for precise and reliable brain tumor diagnoses as an application of AIoMT.  © 2023 IEEE.","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85190272373"
"Petrea D.; Stoleru G.-I.; Iftene A.","Petrea, Daniela (58759448200); Stoleru, Georgiana-Ingrid (57210115267); Iftene, Adrian (23397232600)","58759448200; 57210115267; 23397232600","Leveraging Convolutional Neural Networks for Malaria Detection from Red Blood Cell Images","2023","17th International Conference on INnovations in Intelligent SysTems and Applications, INISTA 2023 - Proceedings","","","","","","","0","10.1109/INISTA59065.2023.10310367","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179557317&doi=10.1109%2fINISTA59065.2023.10310367&partnerID=40&md5=56f70367bb50e34c2c2eec5ee9738883","In recent years, artificial intelligence (AI) has started to be used more and more in the medical field. This paper presents a study focused on malaria classification based on segmented blood cells from images collected from the National Institutes of Health (NIH) database. The research involved the development of a handcrafted convolutional neural network (CNN), as well as experimentation with various fine-tuning approaches using the VGG16 architecture. The conducted experiments have yielded promising results, providing empirical evidence for the potential effectiveness of these techniques in future applications. The augmented CNN achieved an impressive accuracy of 96.51%, while the VGG16 fully trainable model outperformed it with an accuracy of 96.69%. A problem that needs to be analyzed more carefully in the future concerns the explainability of the results so that they can be used with confidence by healthcare professionals. © 2023 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85179557317"
"Iftikhar S.; Karim A.M.; Karim A.M.; Karim M.A.; Aslam M.; Rubab F.; Malik S.K.; Kwon J.E.; Hussain I.; Azhar E.I.; Kang S.C.; Yasir M.","Iftikhar, Sara (57739072600); Karim, Asad Mustafa (56405119200); Karim, Aoun Murtaza (58018488800); Karim, Mujahid Aizaz (57980955900); Aslam, Muhammad (54415401200); Rubab, Fazila (58017828500); Malik, Sumera Kausar (57189336425); Kwon, Jeong Eun (57189625662); Hussain, Imran (57189469018); Azhar, Esam I. (23090055800); Kang, Se Chan (56718832700); Yasir, Muhammad (57212058563)","57739072600; 56405119200; 58018488800; 57980955900; 54415401200; 58017828500; 57189336425; 57189625662; 57189469018; 23090055800; 56718832700; 57212058563","Prediction and interpretation of antibiotic-resistance genes occurrence at recreational beaches using machine learning models","2023","Journal of Environmental Management","328","","116969","","","","5","10.1016/j.jenvman.2022.116969","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144053888&doi=10.1016%2fj.jenvman.2022.116969&partnerID=40&md5=c1a7a91787d0677c8701388f55cfd316","Antibiotic-resistant bacteria and antibiotic resistance genes (ARGs) are pollutants of worldwide concern that seriously threaten public health and ecosystems. Machine learning (ML) prediction models have been applied to predict ARGs in beach waters. However, the existing studies were conducted at a single location and had low prediction performance. Moreover, ML models are “black boxes” that do not reveal their predictions' internal nuances and mechanisms. This lack of transparency and trust can result in serious consequences when using these models in high-stakes decisions. In this study, we developed a gradient boosted regression tree based (GBRT) ML model and then described its behavior using six explainable artificial intelligence (XAI) model-agnostic explanation methods. We used hydro-meteorological and qPCR data from the beaches in South Korea and Pakistan and developed ML prediction models for aac (6′-lb-cr), sul1, and tetX with 10-fold time-blocked cross-validation performances of 4.9, 2.06 and 4.4 root mean squared logarithmic error, respectively. We then analyzed the local and global behavior of the developed ML model using four interpretation methods. The developed ML models showed that water temperature, precipitation and tide are the most important predictors for prediction of ARGs at recreational beaches. We show that the model-agnostic interpretation methods not only explain the behavior of the ML model but also provide insights into the behavior of the ML model under new unseen conditions. Moreover, these post-processing techniques can be a debugging tool for ML-based modeling. © 2022 Elsevier Ltd","Article","Final","","Scopus","2-s2.0-85144053888"
"Laios A.; Kalampokis E.; Mamalis M.E.; Tarabanis C.; Nugent D.; Thangavelu A.; Theophilou G.; De Jong D.","Laios, Alexandros (23569638700); Kalampokis, Evangelos (24766142200); Mamalis, Marios Evangelos (58510212300); Tarabanis, Constantine (55960009800); Nugent, David (8744073900); Thangavelu, Amudha (12752763700); Theophilou, Georgios (25629759200); De Jong, Diederick (57196969042)","23569638700; 24766142200; 58510212300; 55960009800; 8744073900; 12752763700; 25629759200; 57196969042","RoBERTa-Assisted Outcome Prediction in Ovarian Cancer Cytoreductive Surgery Using Operative Notes","2023","Cancer Control","30","","","","","","0","10.1177/10732748231209892","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175769972&doi=10.1177%2f10732748231209892&partnerID=40&md5=f2a8c9a3c3b0734f5356d8eee7181173","Introduction: Contemporary efforts to predict surgical outcomes focus on the associations between traditional discrete surgical risk factors. We aimed to determine whether natural language processing (NLP) of unstructured operative notes improves the prediction of residual disease in women with advanced epithelial ovarian cancer (EOC) following cytoreductive surgery. Methods: Electronic Health Records were queried to identify women with advanced EOC including their operative notes. The Term Frequency – Inverse Document Frequency (TF-IDF) score was used to quantify the discrimination capacity of sequences of words (n-grams) regarding the existence of residual disease. We employed the state-of-the-art RoBERTa-based classifier to process unstructured surgical notes. Discrimination was measured using standard performance metrics. An XGBoost model was then trained on the same dataset using both discrete and engineered clinical features along with the probabilities outputted by the RoBERTa classifier. Results: The cohort consisted of 555 cases of EOC cytoreduction performed by eight surgeons between January 2014 and December 2019. Discrete word clouds weighted by n-gram TF-IDF score difference between R0 and non-R0 resection were identified. The words ‘adherent’ and ‘miliary disease’ best discriminated between the two groups. The RoBERTa model reached high evaluation metrics (AUROC.86; AUPRC.87, precision, recall, and F1 score of.77 and accuracy of.81). Equally, it outperformed models that used discrete clinical and engineered features and outplayed the performance of other state-of-the-art NLP tools. When the probabilities from the RoBERTa classifier were combined with commonly used predictors in the XGBoost model, a marginal improvement in the overall model’s performance was observed (AUROC and AUPRC of.91, with all other metrics the same). Conclusion/Implications: We applied a sui generis approach to extract information from the abundant textual surgical data and demonstrated how it can be effectively used for classification prediction, outperforming models relying on conventional structured data. State-of-art NLP applications in biomedical texts can improve modern EOC care. © The Author(s) 2023.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85175769972"
"Liu F.; Li Y.; Zuo M.","Liu, Fei (57222252256); Li, Yibo (59070307800); Zuo, Meiyun (23391425500)","57222252256; 59070307800; 23391425500","KESHEM: Knowledge Enabled Short Health Misinformation Detection Framework","2023","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","14169 LNAI","","","372","388","16","0","10.1007/978-3-031-43412-9_22","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174447694&doi=10.1007%2f978-3-031-43412-9_22&partnerID=40&md5=1c94ab899392b529c9ad3b6dcbf5162a","Health misinformation detection is a challenging but urgent problem in the field of information governance. In recent years, some studies have utilized long-form text detection models for this task, producing some promising early results. However, we found that most health information online is a short text, especially knowledge-based information. Meanwhile, the explainability of detection results is as important as the detection accuracy. There is no appropriate explainable short health misinformation detection model currently. To address these issues, we propose a novel Knowledge Enabled Short HEalth Misinformation detection framework, called KESHEM. This method extracts abundant knowledge from multiple, multi-form, and dynamically updated knowledge graphs (KGs) as supplementary material and effectively represents semantic features of the information contents and the external knowledge by powerful language models. KG-attention is then applied to distinguish the effects of each external knowledge for the information credibility reasoning and enhance the model’s explainability. We build a credible Chinese short text dataset for better evaluation and future research. Extensive experiments demonstrate that KESHEM significantly outperforms competing methods and accurately identifies important knowledge that explains the veracity of short health information. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Conference paper","Final","","Scopus","2-s2.0-85174447694"
"Lolak S.; Attia J.; McKay G.J.; Thakkinstian A.","Lolak, Sermkiat (57221347798); Attia, John (7003299759); McKay, Gareth J. (7202215396); Thakkinstian, Ammarin (6602111727)","57221347798; 7003299759; 7202215396; 6602111727","Comparing Explainable Machine Learning Approaches With Traditional Statistical Methods for Evaluating Stroke Risk Models: Retrospective Cohort Study","2023","JMIR Cardio","7","","e4773","","","","1","10.2196/47736","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168012091&doi=10.2196%2f47736&partnerID=40&md5=3adba8623e16cc09d914a9aab27c4a5e","Background: Stroke has multiple modifiable and nonmodifiable risk factors and represents a leading cause of death globally. Understanding the complex interplay of stroke risk factors is thus not only a scientific necessity but a critical step toward improving global health outcomes. Objective: We aim to assess the performance of explainable machine learning models in predicting stroke risk factors using real-world cohort data by comparing explainable machine learning models with conventional statistical methods. Methods: This retrospective cohort included high-risk patients from Ramathibodi Hospital in Thailand between January 2010 and December 2020. We compared the performance and explainability of logistic regression (LR), Cox proportional hazard, Bayesian network (BN), tree-augmented Naïve Bayes (TAN), extreme gradient boosting (XGBoost), and explainable boosting machine (EBM) models. We used multiple imputation by chained equations for missing data and discretized continuous variables as needed. Models were evaluated using C-statistics and F1-scores. Results: Out of 275,247 high-risk patients, 9659 (3.5%) experienced a stroke. XGBoost demonstrated the highest performance with a C-statistic of 0.89 and an F1-score of 0.80 followed by EBM and TAN with C-statistics of 0.87 and 0.83, respectively; LR and BN had similar C-statistics of 0.80. Significant factors associated with stroke included atrial fibrillation (AF), hypertension (HT), antiplatelets, HDL, and age. AF, HT, and antihypertensive medication were common significant factors across most models, with AF being the strongest factor in LR, XGBoost, BN, and TAN models. Conclusions: Our study developed stroke prediction models to identify crucial predictive factors such as AF, HT, or systolic blood pressure or antihypertensive medication, anticoagulant medication, HDL, age, and statin use in high-risk patients. The explainable XGBoost was the best model in predicting stroke risk, followed by EBM. ©Sermkiat Lolak, John Attia, Gareth J McKay, Ammarin Thakkinstian.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85168012091"
"Kononov E.; Klyuev A.; Tashkinov M.","Kononov, Evgeniy (58107359300); Klyuev, Andrey (58119321400); Tashkinov, Mikhail (37114780700)","58107359300; 58119321400; 37114780700","Prediction of Technical State of Mechanical Systems Based on Interpretive Neural Network Model","2023","Sensors","23","4","1892","","","","4","10.3390/s23041892","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148974249&doi=10.3390%2fs23041892&partnerID=40&md5=932586346dec98fa3c3747822a8b34ee","A classic problem in prognostic and health management (PHM) is the prediction of the remaining useful life (RUL). However, until now, there has been no algorithm presented to achieve perfect performance in this challenge. This study implements a less explored approach: binary classification of the state of mechanical systems at a given forecast horizon. To prove the effectiveness of the proposed approach, tests were conducted on the C-MAPSS sample dataset. The obtained results demonstrate the achievement of an almost maximal performance threshold. The explainability of artificial intelligence (XAI) using the SHAP (Shapley Additive Explanations) feature contribution estimation method for classification models trained on data with and without a sliding window technique is also investigated. © 2023 by the authors.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85148974249"
"Omisade O.; Gegov A.; Zhou S.-M.; Good A.; Tryfona C.; Sengar S.S.; Prior A.-L.; Liu B.; Adedeji T.; Toptan C.","Omisade, Omobolanle (57210196750); Gegov, Alex (6602091156); Zhou, Shang-Ming (7404166600); Good, Alice (23396751200); Tryfona, Catherine (57190277092); Sengar, Sandeep Singh (57062849000); Prior, Amie-Louise (56020411000); Liu, Bangli (57193678111); Adedeji, Taiwo (57844082600); Toptan, Carrie (57224585696)","57210196750; 6602091156; 7404166600; 23396751200; 57190277092; 57062849000; 56020411000; 57193678111; 57844082600; 57224585696","Explainable Artificial Intelligence and Mobile Health for Treating Eating Disorders in Young Adults with Autism Spectrum Disorder Based on the Theory of Change: A Mixed Method Protocol","2023","Smart Innovation, Systems and Technologies","371","","","31","44","13","0","10.1007/978-981-99-6706-3_3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178576032&doi=10.1007%2f978-981-99-6706-3_3&partnerID=40&md5=8a6eab02a0171c92dc85c9026db88466","Autistic children often face difficulties eating well into their early adolescence, putting them at a greater risk of developing disordered eating habits during this developmental stage. Research suggests that mobile devices are easily accessible to young adults, and their widespread use can be leveraged to provide support and intervention for autistic young adults in preventing and self-managing eating disorders. By utilising Explainable Artificial Intelligence (XAI) and Machine Learning (ML) powered mobile devices, a progressive learning system can be developed that provides essential life skills for independent living and improved quality of life. In addition, XAI can enhance healthcare professionals’ decision-making abilities by utilising trained algorithms that can learn, providing a therapeutic benefit for preventing and mitigating the risk of eating disorders. This study will utilise the theory of change (ToC) approach to guide the investigation and analysis of the complex integration of autism, ED, XAI, ML, and mobile health. This approach will be complemented by user-centred design, Patient and Public Involvement and Engagement (PPIE) tasks, activities, and a mixed method approach to make the integration more rigorous, timely, and valuable. Ultimately, this study aims to provide essential life skills to autistic young adults to prevent and self-manage eating disorders using XAI-powered mobile devices. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Conference paper","Final","","Scopus","2-s2.0-85178576032"
"Patil T.; Arora S.","Patil, Tanaya (57215341571); Arora, Sandhya (11339983500)","57215341571; 11339983500","Survey of Explainable AI Techniques: A Case Study of Healthcare","2023","Lecture Notes in Networks and Systems","765 LNNS","","","335","346","11","0","10.1007/978-981-99-5652-4_30","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177060919&doi=10.1007%2f978-981-99-5652-4_30&partnerID=40&md5=11a5a2b25312da7683981197e61b2e97","With its unparalleled predictive capabilities and decision-making power, artificial intelligence (AI) has transformed countless disciplines. Nevertheless, as AI technology advances in complexity and ubiquity, so does the requirement for transparency and interpretability increase commensurately. The opaqueness of AI models makes them difficult to comprehend, which leads to mounting concerns regarding their application. To address this issue effectively, Explainable Artificial Intelligence (XAI) has emerged as an attempt to demystify how machine learning model decisions are made possible by offering comprehensible explanations for opacities caused by variations in input features or noise in data collection processes using methods such as LIME or SHAP especially vital in domains such as health care where ethical considerations outweigh those faced in other areas. This study provides an overview of XAI techniques, a case study that talks about the importance of using techniques such as LIME and SHAP in a healthcare organization to predict whether or not the patient is likely to get a heart disease or not, why and how the outcomes have been obtained. In the concluding section, the paper elaborates on the challenges facing XAI and outlines prospective future directions for research in this domain. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd 2023.","Conference paper","Final","","Scopus","2-s2.0-85177060919"
"Yang J.; Yue Z.; Yuan Y.","Yang, Jingyu (56953627200); Yue, Zuogong (57188644570); Yuan, Ye (56316070300)","56953627200; 57188644570; 56316070300","Noise-Aware Sparse Gaussian Processes and Application to Reliable Industrial Machinery Health Monitoring","2023","IEEE Transactions on Industrial Informatics","19","4","","5995","6005","10","5","10.1109/TII.2022.3200428","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137543179&doi=10.1109%2fTII.2022.3200428&partnerID=40&md5=62e1835503c196718c2095324008ed0d","Maintenance of machinery equipment in smart manufacturing requires real-time health monitoring, strongly supported by the rapid evolution of Artificial Intelligence (AI) technologies. Most AI-based health monitoring systems are powered by advanced modeling methods and intensive high-quality monitoring data. Such monitoring systems center on high-accuracy predictive performance but cannot necessarily convey reliability, such as satisfactory resistance to strong noises, credible uncertainty analysis, and model interpretability. This article novelly proposes noise-aware sparse Gaussian processes (NASGP) within the Bayesian inference framework. NASGP are capable of consistent high-performance and credible uncertainty assessment under strong noises. Based on NASGP, we then develop an explainable generalized additive model to bridge the gap between latent inference mechanism and domain expert knowledge. The efficacy of the proposed approach is corroborated through two case studies including remaining useful life prognosis and fault diagnosis for rolling bearings.  © 2005-2012 IEEE.","Article","Final","","Scopus","2-s2.0-85137543179"
"Chaddad A.; Lu Q.; Li J.; Katib Y.; Kateb R.; Tanougast C.; Bouridane A.; Abdulkadir A.","Chaddad, Ahmad (53982608400); Lu, Qizong (57330238000); Li, Jiali (57328889600); Katib, Yousef (55387812200); Kateb, Reem (57201012089); Tanougast, Camel (24480204100); Bouridane, Ahmed (6701348149); Abdulkadir, Ahmed (42760899800)","53982608400; 57330238000; 57328889600; 55387812200; 57201012089; 24480204100; 6701348149; 42760899800","Explainable, Domain-Adaptive, and Federated Artificial Intelligence in Medicine","2023","IEEE/CAA Journal of Automatica Sinica","10","4","","859","876","17","22","10.1109/JAS.2023.123123","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151636026&doi=10.1109%2fJAS.2023.123123&partnerID=40&md5=319b7a53bd8eb01e29e19eb694f940da","Artificial intelligence (AI) continues to transform data analysis in many domains. Progress in each domain is driven by a growing body of annotated data, increased computational resources, and technological innovations. In medicine, the sensitivity of the data, the complexity of the tasks, the potentially high stakes, and a requirement of accountability give rise to a particular set of challenges. In this review, we focus on three key methodological approaches that address some of the particular challenges in AI-driven medical decision making. 1) Explainable AI aims to produce a human-interpretable justification for each output. Such models increase confidence if the results appear plausible and match the clinicians expectations. However, the absence of a plausible explanation does not imply an inaccurate model. Especially in highly non-linear, complex models that are tuned to maximize accuracy, such interpretable representations only reflect a small portion of the justification. 2) Domain adaptation and transfer learning enable AI models to be trained and applied across multiple domains. For example, a classification task based on images acquired on different acquisition hardware. 3) Federated learning enables learning large-scale models without exposing sensitive personal health information. Unlike centralized AI learning, where the centralized learning machine has access to the entire training data, the federated learning process iteratively updates models across multiple sites by exchanging only parameter updates, not personal health data. This narrative review covers the basic concepts, highlights relevant corner-stone and state-of-the-art research in the field, and discusses perspectives.  © 2014 Chinese Association of Automation.","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85151636026"
"Castronuovo G.; Favia G.; Telesca V.; Vammacigno A.","Castronuovo, Gianfranco (58136117500); Favia, Gianfranco (7005631417); Telesca, Vito (6507849586); Vammacigno, Andrea (58763317900)","58136117500; 7005631417; 6507849586; 58763317900","Analyzing the Interactions between Environmental Parameters and Cardiovascular Diseases Using Random Forest and SHAP Algorithms","2023","Reviews in Cardiovascular Medicine","24","","330","","","","1","10.31083/j.rcm2411330","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179677901&doi=10.31083%2fj.rcm2411330&partnerID=40&md5=c431eaf40a7d77aac86c6a65a55410b6","Cardiovascular diseases (CVD) remain the predominant global cause of mortality, with both low and high temperatures increasing CVD-related mortalities. Climate change impacts human health directly through temperature fluctuations and indirectly via factors like disease vectors. Elevated and reduced temperatures have been linked to increases in CVD-related hospitalizations and mortality, with various studies worldwide confirming the significant health implications of temperature variations and air pollution on cardiovascular outcomes. Methods: A database of daily Emergency Room admissions at the Giovanni XIII Polyclinic in Bari (Southern Italy) was developed, spanning from 2013 to 2019, including weather and air quality data. A Random Forest (RF) supervised machine learning model was used to simulate the trend of hospital admissions for CVD. The Seasonal and Trend decomposition using Loess (STL) decomposition model separated the trend component, while cross-validation techniques were employed to prevent overfitting. Model performance was assessed using specific metrics and error analysis. Additionally, the SHapley Additive explanations (SHAP) method, a feature importance technique within the explainable Artificial Intelligence (XAI) framework, was used to identify the feature importance. Results: An R2 of 0.97 and a Mean Absolute Error of 0.36 admissions were achieved by the model. Atmospheric pressure, minimum temperature, and carbon monoxide were found to collectively contribute about 74% to the model's predictive power, with atmospheric pressure being the dominant factor at 37%. Conclusions: This research underscores the significant influence of weather-climate variables on cardiovascular diseases. The identified key climate factors provide a practical framework for policymakers and healthcare professionals to mitigate the adverse effects of climate change on CVD and devise preventive strategies. © 2023 IMR Press Limited. All rights reserved.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85179677901"
"Parziale M.; Lomazzi L.; Giglio M.; Cadini F.","Parziale, Marc (57843666800); Lomazzi, Luca (57223213927); Giglio, Marco (7004876674); Cadini, Francesco (8268636400)","57843666800; 57223213927; 7004876674; 8268636400","Transmissibility Functions-Based Structural Damage Assessment with the Use of Explainable Convolutional Neural Networks","2023","Lecture Notes in Civil Engineering","433 LNCE","","","540","549","9","1","10.1007/978-3-031-39117-0_55","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174812661&doi=10.1007%2f978-3-031-39117-0_55&partnerID=40&md5=a77756406ccefae161b69750f84add50","Damage assessment in the structural field is an important area of study and research. In this context, the use of vibration data has always created a lot of interest due to the great and cheap availability of sensors, as well as the strong relationship that the gathered signals have with the system damages. Among the different vibration-based features that have been exploited, transmissibility functions (TFs) have shown the potential to solely rely on the system outputs, making their use particularly convenient in practical engineering. However, most of the damage-related signals features are often hidden and to extract them, many pre-processing steps are typically required. With this aim, this paper proposes a new vibration-based structural diagnosis tool that exploits the capabilities of convolutional neural networks (CNNs) to directly extract subtle damage-related features from complex raw TFs spectra. The method is presented for an experimental case study of a steel structure made of beams connected by means of bolted joints. In particular, a CNN is used to accurately localize the damage in the structure, which is introduced by loosening the joint bolts. In addition, to then understand which are the most important input features exploited by the CNN in the damage characterization, an explainable artificial intelligence method, based on the layer-wise relevance propagation algorithm, is also used. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Conference paper","Final","","Scopus","2-s2.0-85174812661"
"López-García G.; Jerez J.M.; Ribelles N.; Alba E.; Veredas F.J.","López-García, Guillermo (57209347867); Jerez, José M. (57210258807); Ribelles, Nuria (6602001801); Alba, Emilio (56123629600); Veredas, Francisco J. (6508135908)","57209347867; 57210258807; 6602001801; 56123629600; 6508135908","Explainable clinical coding with in-domain adapted transformers","2023","Journal of Biomedical Informatics","139","","104323","","","","1","10.1016/j.jbi.2023.104323","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148858827&doi=10.1016%2fj.jbi.2023.104323&partnerID=40&md5=14312c45d455a719c365ed94c4310748","Background and Objective: Automatic clinical coding is a crucial task in the process of extracting relevant information from unstructured medical documents contained in Electronic Health Records (EHR). However, most of the existing computer-based methods for clinical coding act as “black boxes”, without giving a detailed description of the reasons for the clinical-coding assignments, which greatly limits their applicability to real-world medical scenarios. The objective of this study is to use transformer-based models to effectively tackle explainable clinical-coding. In this way, we require the models to perform the assignments of clinical codes to medical cases, but also to provide the reference in the text that justifies each coding assignment. Methods: We examine the performance of 3 transformer-based architectures on 3 different explainable clinical-coding tasks. For each transformer, we compare the performance of the original general-domain version with an in-domain version of the model adapted to the specificities of the medical domain. We address the explainable clinical-coding problem as a dual medical named entity recognition (MER) and medical named entity normalization (MEN) task. For this purpose, we have developed two different approaches, namely a multi-task and a hierarchical-task strategy. Results: For each analyzed transformer, the clinical-domain version significantly outperforms the corresponding general domain model across the 3 explainable clinical-coding tasks analyzed in this study. Furthermore, the hierarchical-task approach yields a significantly superior performance than the multi-task strategy. Specifically, the combination of the hierarchical-task strategy with an ensemble approach leveraging the predictive capabilities of the 3 distinct clinical-domain transformers, yields the best obtained results, with f1-score, precision and recall of 0.852, 0.847 and 0.849 on the Cantemist-Norm task and 0.718, 0.566 and 0.633 on the CodiEsp-X task, respectively. Conclusions: By separately addressing the MER and MEN tasks, as well as by following a context-aware text-classification approach to tackle the MEN task, the hierarchical-task approach effectively reduces the intrinsic complexity of explainable clinical-coding, leading the transformers to establish new SOTA performances for the predictive tasks considered in this study. In addition, the proposed methodology has the potential to be applied to other clinical tasks that require both the recognition and normalization of medical entities. © 2023 The Author(s)","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85148858827"
"Nnamdi M.C.; Shi W.; Tamo J.B.; Iwinski H.J.; Wattenbarger J.M.; Wang M.D.","Nnamdi, Micky C. (58682153500); Shi, Wenqi (57220119218); Tamo, J. Ben (58759288900); Iwinski, Henry J. (6507279186); Wattenbarger, J. Michael (6602434105); Wang, May D. (10739831700)","58682153500; 57220119218; 58759288900; 6507279186; 6602434105; 10739831700","Concept Bottleneck Model for Adolescent Idiopathic Scoliosis Patient Reported Outcomes Prediction","2023","BHI 2023 - IEEE-EMBS International Conference on Biomedical and Health Informatics, Proceedings","","","","","","","0","10.1109/BHI58575.2023.10313382","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179516064&doi=10.1109%2fBHI58575.2023.10313382&partnerID=40&md5=1778cb58dbf0e44107bcab4d737fcd0c","Post-surgical patient-reported outcomes (PROs) serve as a crucial subjective measure of surgical success for adolescent idiopathic scoliosis (AIS) patients. Leveraging pre-operative patient information to predict post-operative PROs is instrumental in improving pediatric patient care and providing invaluable insights for clinical decision-making. Recently, deep learning techniques have demonstrated encouraging results in developing predictive models for clinical decision support. However, the inherent black-box nature makes them non-interactive and challenging to troubleshoot during the training phase. To mitigate this issue, our study introduces an interactive concept bottleneck model to predict subjective rehabilitation outcomes for AIS patients. We assess three learning schemas - independent, sequential, and joint - to first comprehend the concepts, which are a set of post-operative radiographic data available during the training phase. Subsequently, these acquired concepts are employed to predict post-operative patient rehabilitation outcomes across five domains: pain, function, general satisfaction, self-image, and mental health. Our results demonstrated improvement compared to the existing baseline, with the joint learning schema yielding the highest F1 score in the function and pain domains, while sequential learning recorded the highest F1 score in the mental health and self-image domains. This proposed framework harbors the immense potential to aid pre-operative surgical planning and further enhance the transparency of AI models, thereby supporting real-world clinical decision-making. © 2023 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85179516064"
"Gutiérrez-Tobal G.C.; Kheirandish-Gozal L.; Gozal D.; Hornero R.","Gutiérrez-Tobal, Gonzalo C. (57205213387); Kheirandish-Gozal, Leila (14060157400); Gozal, David (34975080100); Hornero, Roberto (57211010342)","57205213387; 14060157400; 34975080100; 57211010342","Editorial: Unraveling sleep and its disorders using novel analytical approaches, volume II","2023","Frontiers in Neuroscience","17","","1332749","","","","0","10.3389/fnins.2023.1332749","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180415547&doi=10.3389%2ffnins.2023.1332749&partnerID=40&md5=18b7c3e75d3a526c1ec07ccc9405bb94","[No abstract available]","Editorial","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85180415547"
"Hossain M.M.; Ali M.S.; Ahmed M.M.; Rakib M.R.H.; Kona M.A.; Afrin S.; Islam M.K.; Ahsan M.M.; Raj S.M.R.H.; Rahman M.H.","Hossain, Md Maruf (57224000918); Ali, Md Shahin (57225851312); Ahmed, Md Mahfuz (58730581400); Rakib, Md Rakibul Hasan (58633467400); Kona, Moutushi Akter (58633467500); Afrin, Sadia (56157702600); Islam, Md Khairul (57226522198); Ahsan, Md Manjurul (57218995955); Raj, Sheikh Md Razibul Hasan (58633094400); Rahman, Md Habibur (57220839063)","57224000918; 57225851312; 58730581400; 58633467400; 58633467500; 56157702600; 57226522198; 57218995955; 58633094400; 57220839063","Cardiovascular disease identification using a hybrid CNN-LSTM model with explainable AI","2023","Informatics in Medicine Unlocked","42","","101370","","","","4","10.1016/j.imu.2023.101370","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173425984&doi=10.1016%2fj.imu.2023.101370&partnerID=40&md5=20d16b3572e59a5de8a4ffa8e1cded2f","Cardiovascular disease (CVD) is a leading cause of death worldwide, with millions dying each year. The identification and early diagnosis of CVD are critical in preventing adverse health outcomes. Hence, this study proposes a hybrid deep learning (DL) model that combines a convolutional neural network (CNN) and long short-term memory (LSTM) to identify CVD from the clinical data. This study utilizes CNN to extract the relevant features from the input data and the LSTM network to process sequential data and capture dependencies and patterns over time. This study provides insights into the potential of a hybrid DL model combined with feature engineering and explainable AI to improve the accuracy and interpretability of CVD prediction. We evaluated our model on a publicly available dataset where the proposed CNN-LSTM achieved a high accuracy of 73.52% and 74.15% with and without feature engineering, respectively, in identifying individuals with CVD, which is the best result compared to the current state-of-the-art model. The results of this study demonstrate the potential of DL models for the early diagnosis of CVD. Our proposed CNN-LSTM model also incorporates explainable AI to identify the top features responsible for CVD. They could be used to develop more eﬀective screening tools in clinical practice. © 2023 The Authors","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85173425984"
"He Z.; Zhang R.; Diallo G.; Huang Z.; Glicksberg B.S.","He, Zhe (55320918000); Zhang, Rui (55613242636); Diallo, Gayo (24398462100); Huang, Zhengxing (8593282000); Glicksberg, Benjamin S. (55845627200)","55320918000; 55613242636; 24398462100; 8593282000; 55845627200","Editorial: Explainable artificial intelligence for critical healthcare applications","2023","Frontiers in Artificial Intelligence","6","","1282800","","","","1","10.3389/frai.2023.1282800","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172168502&doi=10.3389%2ffrai.2023.1282800&partnerID=40&md5=822180a970bb7baa291054eb71fe37bc","[No abstract available]","Editorial","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85172168502"
"Das P.P.; Wiese L.","Das, Pronaya Prosun (57217126023); Wiese, Lena (8983772000)","57217126023; 8983772000","Explainability Based on Feature Importance for Better Comprehension of Machine Learning in Healthcare","2023","Communications in Computer and Information Science","1850 CCIS","","","324","335","11","1","10.1007/978-3-031-42941-5_28","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172016831&doi=10.1007%2f978-3-031-42941-5_28&partnerID=40&md5=429be9a1129f00a9df5d6fd6f7aa1d11","The use of Artificial Intelligence (AI) in healthcare is getting more prevalent, encompassing responsibilities like intelligent medical diagnoses and operative robots. The accuracy and performance of AI systems are prioritized by Machine Learning (ML) engineers while medical professionals are more interested in their applicability and usefulness in clinical settings. Unfortunately, medical practitioners often lack the necessary skills to interpret AI-based systems, limiting the usage of the tools that enhance healthcare solutions, automating routine analysis tasks and limiting expertise available for validation. Explainable Artificial Intelligence(XAI) is a field that focuses on methods to help understand and interpret ML models. However, most XAI research has been from a viewpoint of Computer Science (CS), with little focus on supporting other domains like healthcare. In this work, a straightforward solution is presented to increase the explainability of ML models to professionals from non-CS domains like healthcare experts. The suggested method integrates feature importance that assesses the influence of distinct features on AI-based system outcomes into standard ML workflows. This could permit medical experts to better understand AI-based systems, improving their ability to comprehend the usefulness and applicability of ML models. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Conference paper","Final","","Scopus","2-s2.0-85172016831"
"Nakerakanti S.K.; Chirraiahgari G.K.R.; Athilakshmi R.","Nakerakanti, Sravan Kumar (58893907600); Chirraiahgari, Gokul Krishna Reddy (58894135600); Athilakshmi, R. (37090284500)","58893907600; 58894135600; 37090284500","Explainable SIR Forecasting Model for Herd Immunity Prediction in COVID-19","2023","2nd International Conference on Automation, Computing and Renewable Systems, ICACRS 2023 - Proceedings","","","","1008","1015","7","0","10.1109/ICACRS58579.2023.10404766","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185389552&doi=10.1109%2fICACRS58579.2023.10404766&partnerID=40&md5=fa7235238ee9e8ea1ac297ad0472aec8","The COVID-19 pandemic has had a profound impact on societies across the globe. Prioritizing the goal of achieving herd immunity is crucial for reducing the spread of the virus, and thus, this research places a strong emphasis on this objective. This study aims to emphasize the importance of understanding and predicting herd immunity levels for COVID-19 using advanced machine learning algorithms to ensure high-quality predictions that are easily accessible. By leveraging various data sources such as vaccination data, population statistics, and daily COVID-19 case counts, this research study has developed predictive models to estimate the threshold required for herd immunity. Our approach involves employing a range of machine-learning techniques, including regression analysis, and ensemble methods along with an Explainable and Susceptible-Infectious-Recovered (SIR) Forecasting model, to capture the complex interactions between infection rates, vaccination campaigns, and population demographics. The performance of our models has been rigorously evaluated and validated against COVID-19 data. The proposed model combines the interpretability of the SIR model with the predictive power of machine learning and Explainable-AI, providing a transparent and reliable framework for herd immunity estimation. The findings highlight the importance of explainable AI in public health decision-making and provide valuable insights into herd immunity dynamics for COVID-19. © 2023 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85185389552"
"Mishra R.; Satpathy R.; Pati B.","Mishra, Ritunsa (58667058200); Satpathy, Rabinarayan (57425440500); Pati, Bibudhendu (56439232500)","58667058200; 57425440500; 56439232500","Human Computer Interaction Applications in Healthcare: An Integrative Review","2023","EAI Endorsed Transactions on Pervasive Health and Technology","9","1","","","","","0","10.4108/eetpht.9.4186","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175025469&doi=10.4108%2feetpht.9.4186&partnerID=40&md5=27d0d527fdbd7c1e3a6d4dd89c2726a2","INTRODUCTION: Human computer interaction (HCI) interprets the design model and the uses of computer technology which focuses on the interface between the user and the computer. HCI is a very important factor in the design of software-oriented decision-making ideas in health-care organizations and also it assists in accurate detection of image, disease including safety of the patients. OBJECTIVES: There are some pitfalls arises over some previous works on cloud based HCI applications. For that reason, to masafety, patient’s safety we wanted to work on explainable artificial intelligence (x-AI) and human intelligence in conjunction with HCI in various fields and algorithms to pro-vide transparency to the user. This may also use some web-based technologies and digital platforms with HCI for development of quality, safety and usability of the patients. METHODS: The purpose of this study about the communication between the HCI design and healthcare system through client and apply that method to the information system of Healthcare department to analyse the functions, effects and outcomes. RESULTS: The integration of explainable artificial intelligence (x-AI) and human intelligence with Human-Computer Interaction (HCI) demonstrated promising potential in enhancing patient safety and optimizing healthcare processes. CONCLUSION: By leveraging web-based technologies and digital platforms, this study established a framework for improving the quality, safety, and usability of healthcare services through effective communication between HCI design and healthcare systems. © 2023 R. Mishra et al., licensed to EAI.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85175025469"
"Kim Y.K.; Koo J.H.; Lee S.J.; Song H.S.; Lee M.","Kim, Yun Kwan (57763048600); Koo, Ja Hyung (57193726061); Lee, Sun Jung (57210802822); Song, Hee Seok (57223135470); Lee, Minji (57196187332)","57763048600; 57193726061; 57210802822; 57223135470; 57196187332","Explainable Artificial Intelligence Warning Model Using an Ensemble Approach for In-Hospital Cardiac Arrest Prediction: Retrospective Cohort Study","2023","Journal of Medical Internet Research","25","1","e48244","","","","1","10.2196/48244","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181178831&doi=10.2196%2f48244&partnerID=40&md5=5bdaee29df332b9a65aacf90662cf31c","Background: Cardiac arrest (CA) is the leading cause of death in critically ill patients. Clinical research has shown that early identification of CA reduces mortality. Algorithms capable of predicting CA with high sensitivity have been developed using multivariate time series data. However, these algorithms suffer from a high rate of false alarms, and their results are not clinically interpretable. Objective: We propose an ensemble approach using multiresolution statistical features and cosine similarity-based features for the timely prediction of CA. Furthermore, this approach provides clinically interpretable results that can be adopted by clinicians. Methods: Patients were retrospectively analyzed using data from the Medical Information Mart for Intensive Care-IV database and the eICU Collaborative Research Database. Based on the multivariate vital signs of a 24-hour time window for adults diagnosed with heart failure, we extracted multiresolution statistical and cosine similarity-based features. These features were used to construct and develop gradient boosting decision trees. Therefore, we adopted cost-sensitive learning as a solution. Then, 10-fold cross-validation was performed to check the consistency of the model performance, and the Shapley additive explanation algorithm was used to capture the overall interpretability of the proposed model. Next, external validation using the eICU Collaborative Research Database was performed to check the generalization ability. Results: The proposed method yielded an overall area under the receiver operating characteristic curve (AUROC) of 0.86 and area under the precision-recall curve (AUPRC) of 0.58. In terms of the timely prediction of CA, the proposed model achieved an AUROC above 0.80 for predicting CA events up to 6 hours in advance. The proposed method simultaneously improved precision and sensitivity to increase the AUPRC, which reduced the number of false alarms while maintaining high sensitivity. This result indicates that the predictive performance of the proposed model is superior to the performances of the models reported in previous studies. Next, we demonstrated the effect of feature importance on the clinical interpretability of the proposed method and inferred the effect between the non-CA and CA groups. Finally, external validation was performed using the eICU Collaborative Research Database, and an AUROC of 0.74 and AUPRC of 0.44 were obtained in a general intensive care unit population. Conclusions: The proposed framework can provide clinicians with more accurate CA prediction results and reduce false alarm rates through internal and external validation. In addition, clinically interpretable prediction results can facilitate clinician understanding. Furthermore, the similarity of vital sign changes can provide insights into temporal pattern changes in CA prediction in patients with heart failure-related diagnoses. Therefore, our system is sufficiently feasible for routine clinical use. In addition, regarding the proposed CA prediction system, a clinically mature application has been developed and verified in the future digital health field. © 2023 Journal of Medical Internet Research. All rights reserved.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85181178831"
"Soliman A.; Agvall B.; Etminani K.; Hamed O.; Lingman M.","Soliman, Amira (57226431186); Agvall, Björn (12778763300); Etminani, Kobra (24491745100); Hamed, Omar (58265314300); Lingman, Markus (6504238326)","57226431186; 12778763300; 24491745100; 58265314300; 6504238326","The Price of Explainability in Machine Learning Models for 100-Day Readmission Prediction in Heart Failure: Retrospective, Comparative, Machine Learning Study","2023","Journal of Medical Internet Research","25","","e46934","","","","2","10.2196/46934","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175278273&doi=10.2196%2f46934&partnerID=40&md5=3c762d28fb9a429a208118207199f981","Background: Sensitive and interpretable machine learning (ML) models can provide valuable assistance to clinicians in managing patients with heart failure (HF) at discharge by identifying individual factors associated with a high risk of readmission. In this cohort study, we delve into the factors driving the potential utility of classification models as decision support tools for predicting readmissions in patients with HF. Objective: The primary objective of this study is to assess the trade-off between using deep learning (DL) and traditional ML models to identify the risk of 100-day readmissions in patients with HF. Additionally, the study aims to provide explanations for the model predictions by highlighting important features both on a global scale across the patient cohort and on a local level for individual patients. Methods: The retrospective data for this study were obtained from the Regional Health Care Information Platform in Region Halland, Sweden. The study cohort consisted of patients diagnosed with HF who were over 40 years old and had been hospitalized at least once between 2017 and 2019. Data analysis encompassed the period from January 1, 2017, to December 31, 2019. Two ML models were developed and validated to predict 100-day readmissions, with a focus on the explainability of the model’s decisions. These models were built based on decision trees and recurrent neural architecture. Model explainability was obtained using an ML explainer. The predictive performance of these models was compared against 2 risk assessment tools using multiple performance metrics. Results: The retrospective data set included a total of 15,612 admissions, and within these admissions, readmission occurred in 5597 cases, representing a readmission rate of 35.85%. It is noteworthy that a traditional and explainable model, informed by clinical knowledge, exhibited performance comparable to the DL model and surpassed conventional scoring methods in predicting readmission among patients with HF. The evaluation of predictive model performance was based on commonly used metrics, with an area under the precision-recall curve of 66% for the deep model and 68% for the traditional model on the holdout data set. Importantly, the explanations provided by the traditional model offer actionable insights that have the potential to enhance care planning. Conclusions: This study found that a widely used deep prediction model did not outperform an explainable ML model when predicting readmissions among patients with HF. The results suggest that model transparency does not necessarily compromise performance, which could facilitate the clinical adoption of such models. © 2023 Journal of Medical Internet Research. All rights reserved.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85175278273"
"Delgado-Gallegos J.L.; Avilés-Rodriguez G.; Padilla-Rivas G.R.; De los Ángeles Cosío-León M.; Franco-Villareal H.; Nieto-Hipólito J.I.; de Dios Sánchez López J.; Zuñiga-Violante E.; Islas J.F.; Romo-Cardenas G.S.","Delgado-Gallegos, Juan Luis (57202086947); Avilés-Rodriguez, Gener (57203528734); Padilla-Rivas, Gerardo R. (53664238100); De los Ángeles Cosío-León, María (57221489405); Franco-Villareal, Héctor (57219468265); Nieto-Hipólito, Juan Iván (57206505615); de Dios Sánchez López, Juan (57195959017); Zuñiga-Violante, Erika (57221250641); Islas, Jose Francisco (15837178900); Romo-Cardenas, Gerardo Salvador (15832725000)","57202086947; 57203528734; 53664238100; 57221489405; 57219468265; 57206505615; 57195959017; 57221250641; 15837178900; 15832725000","Application of C5.0 Algorithm for the Assessment of Perceived Stress in Healthcare Professionals Attending COVID-19","2023","Brain Sciences","13","3","513","","","","5","10.3390/brainsci13030513","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151272184&doi=10.3390%2fbrainsci13030513&partnerID=40&md5=870be5237f1bd9989f2a2d4670a93083","Coronavirus disease (COVID-19) represents one of the greatest challenges to public health in modern history. As the disease continues to spread globally, medical and allied healthcare professionals have become one of the most affected sectors. Stress and anxiety are indirect effects of the COVID-19 pandemic. Therefore, it is paramount to understand and categorize their perceived levels of stress, as it can be a detonating factor leading to mental illness. Here, we propose a computer-based method to better understand stress in healthcare workers facing COVID-19 at the beginning of the pandemic. We based our study on a representative sample of healthcare professionals attending to COVID-19 patients in the northeast region of Mexico, at the beginning of the pandemic. We used a machine learning classification algorithm to obtain a visualization model to analyze perceived stress. The C5.0 decision tree algorithm was used to study datasets. We carried out an initial preprocessing statistical analysis for a group of 101 participants. We performed chi-square tests for all questions, individually, in order to validate stress level calculation (p < 0.05) and a calculated Cronbach’s alpha of 0.94 and McDonald’s omega of 0.95, demonstrating good internal consistency in the dataset. The obtained model failed to classify only 6 out of the 101, missing two cases for mild, three for moderate and one for severe (accuracy of 94.1%). We performed statistical correlation analysis to ensure integrity of the method. In addition, based on the decision tree model, we concluded that severe stress cases can be related mostly to high levels of xenophobia and compulsive stress. Thus, showing that applied machine learning algorithms represent valuable tools in the assessment of perceived stress, which can potentially be adapted to other areas of the medical field. © 2023 by the authors.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85151272184"
"Hu J.; Liang Y.; Zhao W.; McAreavey K.; Liu W.","Hu, Jingyu (57210110325); Liang, Yizhu (58698317800); Zhao, Weiyu (58698038500); McAreavey, Kevin (55767510900); Liu, Weiru (8945346000)","57210110325; 58698317800; 58698038500; 55767510900; 8945346000","An Interactive XAI Interface with Application in Healthcare for Non-experts","2023","Communications in Computer and Information Science","1901 CCIS","","","649","670","21","0","10.1007/978-3-031-44064-9_35","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176914622&doi=10.1007%2f978-3-031-44064-9_35&partnerID=40&md5=8f19bdf35a67a8ebcfbfbad7358c0e76","Explainable artificial intelligence (XAI) has gained increasing attention in the medical field, where understanding the reasons for predictions is crucial. In this paper we introduce an interactive and dynamic visual interface providing global, local and counterfactual explanations to end-users, with a use case in healthcare. The dataset used in the study is about predicting an individual’s coronary heart disease (CHD) within 10 years using the decision tree classification method. We evaluated our XAI system with 200 participants. Our results show that the participants reported an overall good assessment of the user interface, with non-expert users showing a higher satisfaction than users who have some degree of knoweldge of AI. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Conference paper","Final","","Scopus","2-s2.0-85176914622"
"Ramsey A.; Kale A.; Kassa Y.; Gandhi R.; Ricks B.","Ramsey, Ashley (58609306300); Kale, Akshay (57246942100); Kassa, Yonas (58608966300); Gandhi, Robin (34770427300); Ricks, Brian (55181944700)","58609306300; 57246942100; 58608966300; 34770427300; 55181944700","Toward Interactive Visualizations for Explaining Machine Learning Models","2023","Proceedings of the International ISCRAM Conference","2023-text","","","837","852","15","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171729100&partnerID=40&md5=76774d6baddd7ddaec11be73f4999be0","Researchers and end users generally demand more trust and transparency from Machine learning (ML) models due to the complexity of their learned rule spaces. The field of eXplainable Artificial Intelligence (XAI) seeks to rectify this problem by developing methods of explaining ML models and the attributes used in making inferences. In the area of structural health monitoring of bridges, machine learning can offer insight into the relation between a bridge’s conditions and its environment over time. In this paper, we describe three visualization techniques that explain decision tree (DT) ML models that identify which features of a bridge make it more likely to receive repairs. Each of these visualizations enable interpretation, exploration, and clarification of complex DT models. We outline the development of these visualizations, along with their validity by experts in AI and in bridge design and engineering. This work has inherent benefits in the field of XAI as a direction for future research and as a tool for interactive visual explanation of ML models. © 2023 Information Systems for Crisis Response and Management, ISCRAM. All rights reserved.","Conference paper","Final","","Scopus","2-s2.0-85171729100"
"Bárcena J.L.C.; Ducange P.; Marcelloni F.; Renda A.; Ruffini F.","Bárcena, José Luis Corcuera (57303922700); Ducange, Pietro (16425459700); Marcelloni, Francesco (7003309696); Renda, Alessandro (57203896405); Ruffini, Fabrizio (57985194000)","57303922700; 16425459700; 7003309696; 57203896405; 57985194000","Federated Learning of Explainable Artificial Intelligence Models for Predicting Parkinson’s Disease Progression","2023","Communications in Computer and Information Science","1901 CCIS","","","630","648","18","0","10.1007/978-3-031-44064-9_34","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176933643&doi=10.1007%2f978-3-031-44064-9_34&partnerID=40&md5=cfe7547ce34254080821242d6af70b66","Services based on Artificial Intelligence (AI) are becoming increasingly pervasive in our society. At the same time, however, we are also witnessing a growing awareness towards the ethical aspects and the trustworthiness of AI tools, especially in high stakes domains, such as the healthcare one. In this paper, we propose the adoption of AI techniques for predicting Parkinson’s Disease progression with the overarching aim of accommodating the urgent need for trustworthiness. We address two key requirements towards trustworthy AI, namely privacy preservation in learning AI models and their explainability. As for the former aspect, we consider the (rather common) case of medical data coming from different health institutions, assuming that they cannot be shared due to privacy concerns. To address this shortcoming, we leverage federated learning (FL) as a paradigm for collaborative model training among multiple parties without any disclosure of private raw data. As for the latter aspect, we focus on highly interpretable models, i.e., those for which humans are able to understand how decisions have been taken. An extensive experimental analysis carried out on a well-known Parkinson Telemonitoring dataset highlights how the proposed approach based on FL of fuzzy rule-based systems allows achieving, simultaneously, data privacy and interpretability. Results are reported for different data partitioning scenarios, also comparing the interpretable-by-design model with an opaque neural network model. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Conference paper","Final","","Scopus","2-s2.0-85176933643"
"Amin O.; Brown B.; Stephen B.; McArthur S.; Livina V.","Amin, Omnia (58745453100); Brown, Blair (57199485570); Stephen, Bruce (8375894400); McArthur, Stephen (7006668389); Livina, Valerie (19337595200)","58745453100; 57199485570; 8375894400; 7006668389; 19337595200","Machine learning explanations by design: A Case study explaining the predicted degradation of a roto-dynamic pump","2023","19th International Conference on Condition Monitoring and Asset Management, CM 2023","","","","","","","0","10.1784/cm2023.2d3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178623774&doi=10.1784%2fcm2023.2d3&partnerID=40&md5=0673b6406c4a6a408fb25e30681bae0f","The field of explainable Artificial Intelligence (AI) has gained growing attention over the last few years due to the potential for making accurate data-based predictions of asset health. One of the current research aims in AI is to address challenges associated with adopting machine learning (ML) (i.e., data-driven) AI – that is, understanding how and why ML predictions are made. Despite ML models successfully providing accurate predictions in many applications, such as condition monitoring, there are still concerns about the transparency of the prediction-making process. Therefore, ensuring that the models used are explainable to human users is essential to build trust in the approaches proposed. Consequently, AI and ML practitioners need to be able to evaluate any available eXplainable AI (XAI) tools’ suitability for their intended domain and end users, while simultaneously being aware of the tools’ limitations. This paper provides insight into various existing XAI approaches and their limitations to be considered by practitioners in condition monitoring applications during the design process for an ML-based prediction. The aim is to assist practitioners in engineering applications in building interpretable and explainable models intended for end users who wish to improve a system’s reliability and help users make better-informed decisions based upon a predictive ML algorithm output. It also emphasizes the importance of explainability in AI. The paper applies some of these tools to an explainability use case in which real condition monitoring data is used to predict the degradation of a roto-dynamic pump. Additionally, potential avenues are explored to enhance the credibility of explanations generated by XAI tools in condition monitoring applications, aiming to offer more reliable explanations to domain experts. Copyright © 2023 International Conference on Condition Monitoring and Asset Management","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85178623774"
"Boudanga Z.; benhadou S.; Medromi H.","Boudanga, Zineb (57222224957); benhadou, Siham (35323735800); Medromi, Hicham (24725091300)","57222224957; 35323735800; 24725091300","An innovative medical waste management system in a smart city using XAI and vehicle routing optimization","2023","F1000Research","12","","1060","","","","0","10.12688/f1000research.138867.2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176304021&doi=10.12688%2ff1000research.138867.2&partnerID=40&md5=9750b3f905d1a03b1dccf7777ad0b29d","Background: The management of medical waste is a complex task that necessitates effective strategies to mitigate health risks, comply with regulations, and minimize environmental impact. In this study, a novel approach based on collaboration and technological advancements is proposed. Methods: By utilizing colored bags with identification tags, smart containers with sensors, object recognition sensors, air and soil control sensors, vehicles with Global Positioning System (GPS) and temperature humidity sensors, and outsourced waste treatment, the system optimizes waste sorting, storage, and treatment operations. Additionally, the incorporation of explainable artificial intelligence (XAI) technology, leveraging scikit-learn, xgboost, catboost, lightgbm, and skorch, provides real-time insights and data analytics, facilitating informed decision-making and process optimization. Results: The integration of these cutting-edge technologies forms the foundation of an efficient and intelligent medical waste management system. Furthermore, the article highlights the use of genetic algorithms (GA) to solve vehicle routing models, optimizing waste collection routes and minimizing transportation time to treatment centers. Conclusions: Overall, the combination of advanced technologies, optimization algorithms, and XAI contributes to improved waste management practices, ultimately benefiting both public health and the environment. Copyright: © 2023 Boudanga Z et al.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85176304021"
"Alam G.; McChesney I.; Nicholl P.; Rafferty J.","Alam, Gulzar (57225930260); McChesney, Ian (6602209993); Nicholl, Peter (6602918816); Rafferty, Joseph (55780577900)","57225930260; 6602209993; 6602918816; 55780577900","Improving Prospective Healthcare Outcomes by Leveraging Open Data and Explainable AI","2023","2023 3rd International Conference on Computing and Information Technology, ICCIT 2023","","","","486","492","6","0","10.1109/ICCIT58132.2023.10273878","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175494768&doi=10.1109%2fICCIT58132.2023.10273878&partnerID=40&md5=6b8660a40249db6fb4d434f4817b3848","The use of Artificial Intelligence (AI) in healthcare can improve diagnosis and clinical workflows, advancing standards and improving outcomes. AI for healthcare raises significant ethical and legal challenges. AI model development and deployment require transparency and interpretability. Transparency is the capacity to comprehend how an AI model makes recommendations, whereas interpretability is the ability to understand why. One such issue is the lack of transparency and interpretability of AI models, which might lead to a lack of trust in its decisions. Explainable AI (XAI) is an emerging research field that tries to solve this problem by creating AI models that can explain their recommendations. AI models need enough data to be accurate and reliable, thus they must be trained on various and extensive data sets. Open data and data sets (ODDS) can aid AI model development and training. Medical images, lab reports, and patient data are used to train algorithms to identify trends and make predictions in healthcare. Open data can help XAI in healthcare to enable AI models to be transparent, accountable, and interpretable. Open data makes possible AI model validation and replication. This is especially crucial in healthcare, where AI model flaws or biases can have serious implications. This paper discusses the use of ODDS and XAI to improve healthcare. ODDS's capability to promote trust and openness and best practices for using XAI in healthcare are also explored. The benefits and challenges of employing XAI and ODDS in their current forms are also discussed. © 2023 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85175494768"
"Raghavan K.; B S.; v K.","Raghavan, Kaushik (58722319800); B, Sivaselvan (58762707100); v, Kamakoti (58763522400)","58722319800; 58762707100; 58763522400","Attention guided grad-CAM : an improved explainable artificial intelligence model for infrared breast cancer detection","2023","Multimedia Tools and Applications","","","","","","","2","10.1007/s11042-023-17776-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179689283&doi=10.1007%2fs11042-023-17776-7&partnerID=40&md5=7870283558c66030571f434937282d83","Explainable artificial intelligence (XAI) can help build trust between AI models and healthcare professionals in the context of medical image classification. XAI can help explain the reasoning behind predictions, which can help healthcare professionals understand and trust the AI model. This paper presents a novel, ’attention-guided Grad-CAM,’ a class of explainability algorithms that will visually reveal the reasons for prediction in image classification. To implement the proposed methods, we used infrared breast images from the” Database of Mastology Research” First; we built a classifier for detecting breast cancer using an ensemble of three pre-trained networks. Then we implemented an attention-guided Grad-CAM using channel and spatial attention to visualize the critical regions of infrared breast image that will explain the reasons for the predictions. The proposed ensemble of the pre-trained network was able to classify the breast thermograms (Healthy / Tumour) with an accuracy of 98.04% (Precision: 97.22%, Specificity: 97.77%, Sensitivity: 98.21%, F1-Score: 97.49, AUC: 0.97). The proposed Attention guided Grad-CAM method was able distinctively show the hottest regions of the thermograms (tumor regions). The ablation study also showed an average drop in the model’s 42.5% when the explanation maps were used instead of the original image. The activation score also increased by 25.35%. The proposed ensemble of pre-trained networks was able to classify the breast thermograms accurately, and the attention-guided Grad-CAM was able to visually explain the AI model’s prediction using a heat map. The proposed model will aid in the adoption of AI techniques by healthcare professionals with trust. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Article","Article in press","","Scopus","2-s2.0-85179689283"
"Koyyada S.P.; Singh T.P.","Koyyada, Shiva Prasad (58317706900); Singh, Thipendra P. (58310574300)","58317706900; 58310574300","Ensemble of explainable artificial intelligence predictions through discriminate regions: A model to identify COVID-19 from chest X-ray images","2023","Journal of Intelligent Systems","32","1","","","","","0","10.1515/jisys-2023-0163","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183011440&doi=10.1515%2fjisys-2023-0163&partnerID=40&md5=1aafd3b9b040b683fd5210e1b203d4b8","In 2019, lung disease severely affected human health and was later renamed coronavirus disease 2019 (COVID-2019). Since then, several research methods have been proposed, such as reverse transcription polymerase chain reaction (RT-PCR), and disease identification through chest X-rays and computed tomography (CT) scans, to help the healthcare sector. RT-PCR was time-consuming when more patients were present, and a CT scan was costly. Several deep-learning (DL) methods were used to identify diseases using computer-Aided tools. Among those convolutional neural networks (CNNs), the state of the art was adopted in the machinery to predict cancer. However, there is a lack of explainability (XAI) in how CNN predicts the disease. In this article, we construct XAI ensembles with Local Interpretation Model Agnostic Explanation(LIME), Grad CAM, and a Saliency map. It provides a visual explanation for a DL prognostic model that predicts COVID-19 respiratory infection in patients. Our quantitative experimental results have shown that ensemble XAI with an accuracy of 98.85%, although individual LIME has scored an accuracy of 99.62% on test data, is more reliable since it is the combination of models. © 2023 the author(s), published by De Gruyter.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85183011440"
"Bertl M.; Shahin M.; Ross P.; Draheim D.","Bertl, Markus (57263110900); Shahin, Mahtab (57220585503); Ross, Peeter (15760838900); Draheim, Dirk (57200287738)","57263110900; 57220585503; 15760838900; 57200287738","Finding Indicator Diseases of Psychiatric Disorders in BigData using Clustered Association Rule Mining","2023","Proceedings of the ACM Symposium on Applied Computing","","","","826","833","7","3","10.1145/3555776.3577594","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162905543&doi=10.1145%2f3555776.3577594&partnerID=40&md5=3e5554f047f2a96d82c5208d63450187","Psychiatric disorders represent critical non-communicable diseases of the 21st century and are ranked as the leading cause of years lived with disabilities. Nevertheless, data that could be used to improve our understanding of psychiatric diseases remain underutilized. In this research, we apply clustered association rule mining to find comorbidities and indicator diseases for patients with psychiatric illnesses. The model was trained with health insurance billing data from 60,115 patients with a total of 904,821 ICD-10 coded diseases. Nine association rules were found without clustering, 40 with clustering of F diagnoses. The approach proves suitable for further use in the implementation of indicator-based digital decision support systems in psychiatry.  © 2023 ACM.","Conference paper","Final","","Scopus","2-s2.0-85162905543"
"Gurmessa D.K.; Jimma W.","Gurmessa, Daraje Kaba (58000827800); Jimma, Worku (57194761825)","58000827800; 57194761825","A comprehensive evaluation of explainable Artificial Intelligence techniques in stroke diagnosis: A systematic review","2023","Cogent Engineering","10","2","2273088","","","","1","10.1080/23311916.2023.2273088","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174917512&doi=10.1080%2f23311916.2023.2273088&partnerID=40&md5=59351aa60f27a8df683ddc97f7f3e0f3","Stroke presents a formidable global health threat, carrying significant risks and challenges. Timely intervention and improved outcomes hinge on the integration of Explainable Artificial Intelligence (XAI) into medical decision-making. XAI, an evolving field, enhances the transparency of conventional Artificial Intelligence (AI) models. This systematic review addresses key research questions: How is XAI applied in the context of stroke diagnosis? To what extent can XAI elucidate the outputs of machine learning models? Which systematic evaluation methodologies are employed, and what categories of explainable approaches (Model Explanation, Outcome Explanation, Model Inspection) are prevalent We conducted this review following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. Our search encompassed five databases: Google Scholar, PubMed, IEEE Xplore, ScienceDirect, and Scopus, spanning studies published between January 1988 and June 2023. Various combinations of search terms, including “stroke,” “explainable,” “interpretable,” “machine learning,” “artificial intelligence,” and “XAI,” were employed. This study identified 17 primary studies employing explainable machine learning techniques for stroke diagnosis. Among these studies, 94.1% incorporated XAI for model visualization, and 47.06% employed model inspection. It is noteworthy that none of the studies employed evaluation metrics such as D, R, F, or S to assess the performance of their XAI systems. Furthermore, none evaluated human confidence in utilizing XAI for stroke diagnosis. Explainable Artificial Intelligence serves as a vital tool in enhancing trust among both patients and healthcare providers in the diagnostic process. The effective implementation of systematic evaluation metrics is crucial for harnessing the potential of XAI in improving stroke diagnosis. © 2023 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85174917512"
"Shakhgeldyan K.I.; Geltser B.I.; Potapenko B.V.","Shakhgeldyan, Karina I. (41961853900); Geltser, Boris I. (6602487479); Potapenko, Bogdan V. (58527803700)","41961853900; 6602487479; 58527803700","Architecture of a Hybrid Clinical Decision Support System","2023","Lecture Notes in Networks and Systems","777 LNNS","","","146","156","10","0","10.1007/978-3-031-43792-2_15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174439853&doi=10.1007%2f978-3-031-43792-2_15&partnerID=40&md5=ba441224fe260fe4901a585696050761","Precise risk assessment and individualized preventive strategies are essential areas in healthcare, as cardiovascular diseases (CVD) are the leading cause of death in the world. Our prototype Clinical Decision Support System (CDSS) for predicting and preventing cardiovascular risks is based on a hybrid architecture that integrates machine learning models and knowledge bases, utilizing a microservice architecture with a Cloud-Edge approach and implementing the principles of explainable artificial intelligence (XAI) due to their importance for the success of CDSS and its acceptance by healthcare providers. The system incorporates risk assessment tools (SCORE, SCORE 2, GRACE, EuroSCORE II, Diamond-Forrester, etc.) and proprietary machine learning models for predicting in-hospital mortality, development of postoperative atrial fibrillation, and the probability of obstructive coronary artery disease. These models contribute to informed clinical decision-making for the diagnosis, prevention, and treatment of CVD. The prototype system was implemented at the Medical Center of the Far Eastern Federal University and integrated with the healthcare information system “1C”. The implementation experience demonstrated the high potential of hybrid CDSS based on microservice architecture for clinical practice use. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Conference paper","Final","","Scopus","2-s2.0-85174439853"
"","","","23rd International Conference on Computational Science and Its Applications , ICCSA 2023","2023","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13956 LNCS","","","","","1296","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164925881&partnerID=40&md5=b10e0ea5e0b1e9c73a129f2c8fdd1e2e","The proceedings contain 86 papers. The special focus in this conference is on Computational Science and Its Applications. The topics include: A Preliminary Result of Implementing a Deep Learning-Based Earthquake Early Warning System in Italy; structural Node Representation Learning for Detecting Botnet Nodes; a Machine Learning Methodology for Optimal Big Data Processing in Advanced Smart City Environments; from Selecting Best Algorithm to Explaining Why It is: A General Review, Formal Problem Statement and Guidelines Towards to an Empirical Generalization; a Framework to Assist Instructors Help Novice Programmers to Better Comprehend Source Code ─ A Decoding Perspective; interactive Information Visualization Models: A Systematic Literature Review; comparative Analysis of Community Detection and Transformer-Based Approaches for Topic Clustering of Scientific Papers; integrating Counterfactual Evaluations into Traditional Interactive Recommendation Frameworks; eXplainable Artificial Intelligence - A Study of Sentiments About Vaccination in Brazil; design and Implementation of Wind-Powered Charging System to Improve Electric Motorcycle Autonomy; knowledge Management Model: A Process View; A Novel Natural Language Processing Strategy to Improve Digital Accounting Classification Approach for Supplier Invoices ERP Transaction Process; implementation of eXplainable Artificial Intelligence: Case Study on the Assessment of Movements to Support Neuromotor Rehabilitation; automatic Features Extraction from the Optic Cup and Disc Segmentation for Glaucoma Classification; artificial Bee Colony Algorithm for Feature Selection in Fraud Detection Process; convolutional Neural Networks and Ensembles for Visually Impaired Aid; a Software Architecture Based on the Blockchain-Database Hybrid for Electronic Health Records; bibliometric Analysis of Robotic Process Automation Domain: Key Topics, Challenges and Solutions; Enhancing Amazigh Speech Recognition System with MFDWC-SVM; FastCELF++: A Novel and Fast Heuristic for Influence Maximization in Complex Networks; predicting Multiple Domain Queue Waiting Time via Machine Learning.","Conference review","Final","","Scopus","2-s2.0-85164925881"
"Naz Z.; Khan M.U.G.; Saba T.; Rehman A.; Nobanee H.; Bahaj S.A.","Naz, Zubaira (57223103252); Khan, Muhammad Usman Ghani (57482888000); Saba, Tanzila (36110026100); Rehman, Amjad (35093155800); Nobanee, Haitham (16068872700); Bahaj, Saeed Ali (58835236700)","57223103252; 57482888000; 36110026100; 35093155800; 16068872700; 58835236700","An Explainable AI-Enabled Framework for Interpreting Pulmonary Diseases from Chest Radiographs","2023","Cancers","15","1","314","","","","10","10.3390/cancers15010314","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145907107&doi=10.3390%2fcancers15010314&partnerID=40&md5=465f01b584c4db01889145abb098f284","Explainable Artificial Intelligence is a key component of artificially intelligent systems that aim to explain the classification results. The classification results explanation is essential for automatic disease diagnosis in healthcare. The human respiration system is badly affected by different chest pulmonary diseases. Automatic classification and explanation can be used to detect these lung diseases. In this paper, we introduced a CNN-based transfer learning-based approach for automatically explaining pulmonary diseases, i.e., edema, tuberculosis, nodules, and pneumonia from chest radiographs. Among these pulmonary diseases, pneumonia, which COVID-19 causes, is deadly; therefore, radiographs of COVID-19 are used for the explanation task. We used the ResNet50 neural network and trained the network on extensive training with the COVID-CT dataset and the COVIDNet dataset. The interpretable model LIME is used for the explanation of classification results. Lime highlights the input image’s important features for generating the classification result. We evaluated the explanation using radiologists’ highlighted images and identified that our model highlights and explains the same regions. We achieved improved classification results with our fine-tuned model with an accuracy of 93% and 97%, respectively. The analysis of our results indicates that this research not only improves the classification results but also provides an explanation of pulmonary diseases with advanced deep-learning methods. This research would assist radiologists with automatic disease detection and explanations, which are used to make clinical decisions and assist in diagnosing and treating pulmonary diseases in the early stage. © 2023 by the authors.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85145907107"
"Nahid A.-A.; Raihan M.J.; Bulbul A.A.-M.","Nahid, Abdullah-Al (57221567094); Raihan, Md. Johir (57330003500); Bulbul, Abdullah Al-Mamun (57203917014)","57221567094; 57330003500; 57203917014","Breast cancer classification along with feature prioritization using machine learning algorithms","2022","Health and Technology","12","6","","1061","1069","8","0","10.1007/s12553-022-00710-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141404630&doi=10.1007%2fs12553-022-00710-6&partnerID=40&md5=16078864c52adab08fdcf6b9c9e99c89","Purpose: Breast Cancer (BC) is considered one of the lethal diseases that causes a large number of female deaths around the world. Prevention and diagnosis are the best options to reduce cancer death, which can be performed through regular examination of a few health-related issues such as the level of Glucose, Insulin, HOMA, Leptin, etc. Based on a few such kinds of statistics, this work classifies Breast Cancer patients and non-Breast Cancer patients utilizing state-of-the-art Machine Learning (ML) techniques. In this study, we have classified the BC using state-of-the-art ML techniques and analyzed the features that influence the model to predict a certain class. Methods: We have used several Machine Learning (ML) models such as Gradient Boosting (GB), XGBoost (XGB), CatBoost (CB), and Light Gradient Boosting Machine (LGBM) to classify the BC and find the feature importance. To interpret the ML model and find the feature contribution to the prediction of the BC, we have used the Shapley Additive exPlanation (SHAP). Besides, a few filters and wrapper-based feature selection and prioritization algorithms have been used to sort out the priority of the features. To obtain conclusive remarks based on a democratic manner, we have utilized the traditional Borda method. Results: It shows that Gradient Boosting (GB) methods provide the best performances among the selected gradient-based algorithms with 82.85% accuracy, 80.00% precision, 88.89% recall, and 84.21% F1-Score, respectively. It shows that different algorithms provide different precedence of the features. We have utilized the traditional Borda method, which has concluded that Glucose is the most influential parameter for Breast Cancer and non-Breast Cancer patients' selection. Conclusion: In this study, we have classified the BC and found that the GB classifier achieved the highest accuracy among CB. XGB, and LGBM classifier. Using the feature selection technique, SHAP, and Borda method we have found that Glucose is the most influential parameter for the detection of BC. We have also presented and analyzed the samples that were misclassified by the GB classifier. © 2022, The Author(s) under exclusive licence to International Union for Physical and Engineering Sciences in Medicine (IUPESM).","Article","Final","","Scopus","2-s2.0-85141404630"
"Loh H.W.; Ooi C.P.; Seoni S.; Barua P.D.; Molinari F.; Acharya U.R.","Loh, Hui Wen (57220933535); Ooi, Chui Ping (55663773200); Seoni, Silvia (57213608081); Barua, Prabal Datta (36993665100); Molinari, Filippo (7004289592); Acharya, U Rajendra (7004510847)","57220933535; 55663773200; 57213608081; 36993665100; 7004289592; 7004510847","Application of explainable artificial intelligence for healthcare: A systematic review of the last decade (2011–2022)","2022","Computer Methods and Programs in Biomedicine","226","","107161","","","","200","10.1016/j.cmpb.2022.107161","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139596030&doi=10.1016%2fj.cmpb.2022.107161&partnerID=40&md5=b58be41e5d7b4209828ff3915e5921e6","Background and objectives: Artificial intelligence (AI) has branched out to various applications in healthcare, such as health services management, predictive medicine, clinical decision-making, and patient data and diagnostics. Although AI models have achieved human-like performance, their use is still limited because they are seen as a black box. This lack of trust remains the main reason for their low use in practice, especially in healthcare. Hence, explainable artificial intelligence (XAI) has been introduced as a technique that can provide confidence in the model's prediction by explaining how the prediction is derived, thereby encouraging the use of AI systems in healthcare. The primary goal of this review is to provide areas of healthcare that require more attention from the XAI research community. Methods: Multiple journal databases were thoroughly searched using PRISMA guidelines 2020. Studies that do not appear in Q1 journals, which are highly credible, were excluded. Results: In this review, we surveyed 99 Q1 articles covering the following XAI techniques: SHAP, LIME, GradCAM, LRP, Fuzzy classifier, EBM, CBR, rule-based systems, and others. Conclusion: We discovered that detecting abnormalities in 1D biosignals and identifying key text in clinical notes are areas that require more attention from the XAI research community. We hope this is review will encourage the development of a holistic cloud system for a smart city. © 2022 Elsevier B.V.","Review","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85139596030"
"Castro-Gutiérrez J.; Gutiérrez-Estrada J.C.; Aroba J.; Pulido-Calvo I.; Peregrín A.; Báez J.C.; Bellido J.J.; Souviron-Priego L.","Castro-Gutiérrez, J. (57355734300); Gutiérrez-Estrada, J.C. (6506769856); Aroba, J. (10243146100); Pulido-Calvo, I. (6506600630); Peregrín, A. (6508373874); Báez, J.C. (8373427200); Bellido, J.J. (57211382641); Souviron-Priego, L. (57201690136)","57355734300; 6506769856; 10243146100; 6506600630; 6508373874; 8373427200; 57211382641; 57201690136","Estimation of jellyfish abundance in the south-eastern Spanish coastline by using an explainable artificial intelligence model based on fuzzy logic","2022","Estuarine, Coastal and Shelf Science","277","","108062","","","","3","10.1016/j.ecss.2022.108062","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137772177&doi=10.1016%2fj.ecss.2022.108062&partnerID=40&md5=bd64d9e825a6473efc8c9ed8cc00a727","Jellyfish swarms have a direct negative impact on human enterprise, specially on places dependent on the sun and beach economy. The local economy and the health of bathers may be at risk from the emergence of these gelatinous organisms. Economic losses can be mitigated by monitoring the occurrence of jellyfish on the coast. Due to the lack of jellyfish data, environmental citizen science is presented as an alternative for data collection. In this study, fuzzy logic-based models have been used to modelize the knowledge from citizen comments collected by the Infomedusa app. The effect of local climatological factors such as wind speed and direction on the incidence of jellyfish on the coast was studied. The fuzzy logic-based models showed that winds perpendicular to the coast lead to a higher occurrence of jellyfish swarms in central and eastern Malaga, while winds parallel to the coast have a greater influence in the westernmost coasts. Wind speed has a different effect on jellyfish incidence depending on the study area and wind direction. Data extracted from the Infomedusa app can help to address the historical scarcity of scientific data on jellyfish. This app presents an opportunity for future studies to expand the knowledge about the occurrence of these organisms on the coasts and may contribute to the prediction of onshore arrival. © 2022 Elsevier Ltd","Article","Final","","Scopus","2-s2.0-85137772177"
"Abdelsamea M.M.; Zidan U.; Senousy Z.; Gaber M.M.; Rakha E.; Ilyas M.","Abdelsamea, Mohammed M. (55837497100); Zidan, Usama (57823404300); Senousy, Zakaria (57201990856); Gaber, Mohamed Medhat (8927664800); Rakha, Emad (6507328020); Ilyas, Mohammad (35324447000)","55837497100; 57823404300; 57201990856; 8927664800; 6507328020; 35324447000","A survey on artificial intelligence in histopathology image analysis","2022","Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery","12","6","e1474","","","","17","10.1002/widm.1474","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135113816&doi=10.1002%2fwidm.1474&partnerID=40&md5=9c38997515418acbe7e286892939cfc0","The increasing adoption of the whole slide image (WSI) technology in histopathology has dramatically transformed pathologists' workflow and allowed the use of computer systems in histopathology analysis. Extensive research in Artificial Intelligence (AI) with a huge progress has been conducted resulting in efficient, effective, and robust algorithms for several applications including cancer diagnosis, prognosis, and treatment. These algorithms offer highly accurate predictions but lack transparency, understandability, and actionability. Thus, explainable artificial intelligence (XAI) techniques are needed not only to understand the mechanism behind the decisions made by AI methods and increase user trust but also to broaden the use of AI algorithms in the clinical setting. From the survey of over 150 papers, we explore different AI algorithms that have been applied and contributed to the histopathology image analysis workflow. We first address the workflow of the histopathological process. We present an overview of various learning-based, XAI, and actionable techniques relevant to deep learning methods in histopathological imaging. We also address the evaluation of XAI methods and the need to ensure their reliability on the field. This article is categorized under: Application Areas > Health Care. © 2022 The Authors. WIREs Data Mining and Knowledge Discovery published by Wiley Periodicals LLC.","Review","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85135113816"
"de Camargo L.F.; Dias D.R.C.; Brega J.R.F.","de Camargo, Luiz Felipe (57219490684); Dias, Diego Roberto Colombo (36782058600); Brega, José Remo Ferreira (6507059897)","57219490684; 36782058600; 6507059897","Implementation of eXplainable Artificial Intelligence: Case Study on the Assessment of Movements to Support Neuromotor Rehabilitation","2023","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13956 LNCS","","","564","580","16","1","10.1007/978-3-031-36805-9_37","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164920232&doi=10.1007%2f978-3-031-36805-9_37&partnerID=40&md5=dcfc907b19d2acb118b02f44fc44c6d3","Solutions based on Artificial Intelligence are being used to solve problems in various domains. However, many people feel uncomfortable with this type of solution because they must understand how it works. In the face of this, the so-called eXplainable Artificial Intelligence arises, seeking not only to provide the answers produced by Artificial Intelligence but also to offer aspects of explainability, detailing the decision process and generating confidence. In this context, a literature review on eXplainable Artificial Intelligence has presented a brief comparative study between the most popular libraries for this implementation and a deepening of the theme of explainability evaluation and the comprehension process. A proposal for the implementation and evaluation of eXplainable Artificial Intelligence in the context of movement classification to support neuromotor rehabilitation was built from the results obtained. The first experiments performed showed to be promising. The proposal is expected to be relevant for addressing a growing theme in a context, the health area, that demands explicability and transparency in decisions. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Conference paper","Final","","Scopus","2-s2.0-85164920232"
"","","","19th International Symposium on Distributed Computing and Artificial Intelligence,  DCAI 2022","2023","Lecture Notes in Networks and Systems","585 LNNS","","","","","204","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149648475&partnerID=40&md5=125d0caa91cebde403c827214794f075","The proceedings contain 22 papers. The special focus in this conference is on Distributed Computing and Artificial Intelligence. The topics include: Computer Vision: A Review on 3D Object Recognition; An IoUT-Based Platform for Managing Underwater Cultural Heritage; overview: Security in 5G Wireless Systems; a Study on the Application of Protein Language Models in the Analysis of Membrane Proteins; visualization for Infection Analysis and Decision Support in Hospitals; An Intelligent and Green E-healthcare Model for an Early Diagnosis of Medical Images as an IoMT Application; towards Highly Performant Context Awareness in the Internet of Things; adaptive System to Manage User Comfort Preferences and Conflicts at Everyday Environments; ML-Based Automation of Constraint Satisfaction Model Transformation and Solver Configuration; race Condition Error Detection in a Program Executed on a Device with Limited Memory Resources; the Impact of Covid-19 on Student Mental Health and Online Learning Experience; Threat Detection in URLs by Applying Machine Learning Algorithms* ; an Approach to Simulate Malware Propagation in the Internet of Drones; the Use of Corporate Architecture in Planning and Automation of Production Processes; Towards Ontology-Based End-to-End Domain-Oriented KBQA System; TFEEC: Turkish Financial Event Extraction Corpus; denial of Service Attack Detection Based on Feature Extraction and Supervised Techniques; automating the Implementation of Unsupervised Machine Learning Processes in Smart Cities Scenarios; Intelligent Model Hotel Energy Demand Forecasting by Means of LSTM and GRU Neural Networks; explainable Artificial Intelligence on Smart Human Mobility: A Comparative Study Approach.","Conference review","Final","","Scopus","2-s2.0-85149648475"
"Jaramillo B.; Loza-Aguirre E.; Terán L.","Jaramillo, Byron (57195621974); Loza-Aguirre, Edison (57216272465); Terán, Luis (36610928700)","57195621974; 57216272465; 36610928700","Designing a Framework for Explainable Health Recommender System Based on the Ecuadorian Data Protection Regulations","2023","2023 9th International Conference on eDemocracy and eGovernment, ICEDEG 2023","","","","","","","1","10.1109/ICEDEG58167.2023.10122066","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160680426&doi=10.1109%2fICEDEG58167.2023.10122066&partnerID=40&md5=da7218374925e3a0edee2cb85d828f96","Technological advances and development in explainable artificial intelligence (XAI) in the health sector are growing worldwide. However, according to different studies in Latin America, there is a lack of knowledge, know-how, and technical skills to master these new technologies. Most medical software professionals can be considered 'black boxes.' This paper focuses on a case study on the lack of technological uses of XAI methods in the health Ecuadorian medical system to support health professionals in treating and managing patients' diseases based on Ecuadorian data protection regulations. A survey was conducted with 71 Ecuadorian medical professionals to know their technological problems in medical appointments. Related works were reviewed to understand techniques or other existing solutions in the XAI field that can complement the designing of so-called health recommender systems. This paper shows the main results of the survey. These results provide guidelines for further designing a framework for managing sensitive data and developing the XAI health recommender system to optimize medical professionals' decision-making, avoiding third-party use of sensitive patient data for other uses.  © 2023 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85160680426"
"Kovalchuk O.; Radiuk P.; Barmak O.; Petrovskyi S.; Krak I.","Kovalchuk, Oleksii (57816242200); Radiuk, Pavlo (57216894492); Barmak, Oleksander (57217176350); Petrovskyi, Sergіi (58205565100); Krak, Iurii (6602577533)","57816242200; 57216894492; 57217176350; 58205565100; 6602577533","A Novel Feature Vector for ECG Classification using Deep Learning","2023","CEUR Workshop Proceedings","3373","","","227","238","11","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85154023871&partnerID=40&md5=819c3be23e1d9b8d69ca3357d9693702","In the past decade, deep learning techniques have been widely used in the healthcare industry to detect heartbeats and diagnose heart conditions. However, these tools have been criticized for being a “black box” and lacking transparency. Therefore, in this paper, we propose a new approach to making the classification results obtained by deep learning more comprehensible. We suggest forming a vector of features based on ECG signals that correspond to specific heart conditions. This vector includes measurable characteristics of the cardiac cycle, such as wave durations and amplitudes, which are typical and understandable to healthcare professionals. This feature vector serves as input data for a deep neural network that acts as a feature encoder and classifier. Our computational experiments with the handcrafted feature vector achieved an average accuracy of 98.69%, comparable to other deep learning tools based on the complete cardiac cycle. The results of this study suggest that future research should focus on developing interpretable deep learning tools that are transparent and comprehensible to healthcare professionals. © 2023 Copyright for this paper by its authors.","Conference paper","Final","","Scopus","2-s2.0-85154023871"
"Sharma M.; Goel A.K.; Singhal P.","Sharma, Mukta (57198780785); Goel, Amit Kumar (22334427100); Singhal, Priyank (36601564000)","57198780785; 22334427100; 36601564000","Explainable AI Driven Applications for Patient Care and Treatment","2023","Intelligent Systems Reference Library","232","","","135","156","21","2","10.1007/978-3-031-12807-3_7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140775638&doi=10.1007%2f978-3-031-12807-3_7&partnerID=40&md5=801a0a90c994ed62b9f596fa350c79c6","The continuous development of technology has saved countless lives and improved the quality of living. Artificial Intelligence is reshaping the healthcare industry from hospital care to clinical research, drug development, to insurance, and has been able to reduce costs and improve patient outcomes. Most AI system works as a black box with little or no explanation which results in a lack of trust and accountability among patients and doctors. This chapter is written with the intent to share with the audience how exquisitely the health care sector has integrated with the technology. The chapter initiates with a brief description of the use of Artificial intelligence and technology in the health domain, and how computers are helping not only doctors, but patients, health care departments, and Insurance companies. This chapter later focuses on various AI-driven Applications which are used for patient care and treatment. This chapter shed light on the purpose and benefits of XAI along with a few real examples. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Book chapter","Final","","Scopus","2-s2.0-85140775638"
"Qiu W.; Chen H.; Dincer A.B.; Lundberg S.; Kaeberlein M.; Lee S.-I.","Qiu, Wei (57222119175); Chen, Hugh (57219509586); Dincer, Ayse Berceste (57219816951); Lundberg, Scott (57189072796); Kaeberlein, Matt (6602710772); Lee, Su-In (7601391623)","57222119175; 57219509586; 57219816951; 57189072796; 6602710772; 7601391623","Interpretable machine learning prediction of all-cause mortality","2022","Communications Medicine","2","1","125","","","","17","10.1038/s43856-022-00180-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190679961&doi=10.1038%2fs43856-022-00180-x&partnerID=40&md5=a86bcd0cf2ffb41bafa6d4e5bb11feaf","Background: Unlike linear models which are traditionally used to study all-cause mortality, complex machine learning models can capture non-linear interrelations and provide opportunities to identify unexplored risk factors. Explainable artificial intelligence can improve prediction accuracy over linear models and reveal great insights into outcomes like mortality. This paper comprehensively analyzes all-cause mortality by explaining complex machine learning models. Methods: We propose the IMPACT framework that uses XAI technique to explain a state-of-the-art tree ensemble mortality prediction model. We apply IMPACT to understand all-cause mortality for 1-, 3-, 5-, and 10-year follow-up times within the NHANES dataset, which contains 47,261 samples and 151 features. Results: We show that IMPACT models achieve higher accuracy than linear models and neural networks. Using IMPACT, we identify several overlooked risk factors and interaction effects. Furthermore, we identify relationships between laboratory features and mortality that may suggest adjusting established reference intervals. Finally, we develop highly accurate, efficient and interpretable mortality risk scores that can be used by medical professionals and individuals without medical expertise. We ensure generalizability by performing temporal validation of the mortality risk scores and external validation of important findings with the UK Biobank dataset. Conclusions: IMPACT’s unique strength is the explainable prediction, which provides insights into the complex, non-linear relationships between mortality and features, while maintaining high accuracy. Our explainable risk scores could help individuals improve self-awareness of their health status and help clinicians identify patients with high risk. IMPACT takes a consequential step towards bringing contemporary developments in XAI to epidemiology. © 2022, The Author(s).","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85190679961"
"Arias J.T.; Astudillo C.A.","Arias, Javiera T. (58522203100); Astudillo, Cesar A. (36695966600)","58522203100; 36695966600","Enhancing Schizophrenia Prediction Using Class Balancing and SHAP Explainability Techniques on EEG Data","2023","2023 IEEE 13th International Conference on Pattern Recognition Systems, ICPRS 2023","","","","","","","0","10.1109/ICPRS58416.2023.10179002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166639660&doi=10.1109%2fICPRS58416.2023.10179002&partnerID=40&md5=eba59bb4a696764232a26ccb0c10216c","Machine learning (ML) makes predictions or supports decision making based on data, achieving high accuracy, saving time and resources, and even running real-Time analysis. However, one drawback of these models is the lack of transparency in complex models, reducing confidence in sensitive fields such as health. This paper analyzes electroencephalogram (EEG) data to predict schizophrenia in patients. Three classifiers are compared, considering Support Vector Machines(SVM), Adaptive Boosting (AdaBoost), and Extreme Gradient Boosting (XGBoost). Three metrics are used to measure the classification process, including accuracy (ACC), area under the curve (AUC) and F-l score (F1). XAI is incorporated into the pipeline to identify relevant features. XGBoost is the model with the best performance in predicting schizophrenia cases, reaching an ACC = 0.93, AUC = 0.93 and F1 score = 0.92, outperforming the SVM and AdaBoost algorithms. The SHAP explainability technique was applied on the XGBoost model, identifying the sex, IQ, delta T6, and delta Pz waves as the most relevant characteristics in the prediction processes. Based on the data analysis, we found that schizophrenia causes an alteration of the delta wave in an EEG, which is different to other mental illnesses. On the other hand, the study generates a specific impact by showing that XGBoost presents better results in 3 validation metrics (ACC, AUC, and F1 Score) compared to SVM and AdaBoost. It is shown that balancing classes is essential to obtain better ML predictions.  © 2023 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85166639660"
"Bottrighi A.; Pennisi M.; Roveta A.; Massarino C.; Cassinari A.; Betti M.; Bolgeo T.; Bertolotti M.; Rava E.; Maconi A.","Bottrighi, Alessio (8980997600); Pennisi, Marzio (23091502800); Roveta, Annalisa (57203784111); Massarino, Costanza (57222589672); Cassinari, Antonella (57216866037); Betti, Marta (14036920300); Bolgeo, Tatiana (46461158300); Bertolotti, Marinella (23566606800); Rava, Emanuele (57212317095); Maconi, Antonio (57202941947)","8980997600; 23091502800; 57203784111; 57222589672; 57216866037; 14036920300; 46461158300; 23566606800; 57212317095; 57202941947","A machine learning approach for predicting high risk hospitalized patients with COVID-19 SARS-Cov-2","2022","BMC Medical Informatics and Decision Making","22","1","340","","","","4","10.1186/s12911-022-02076-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145047130&doi=10.1186%2fs12911-022-02076-1&partnerID=40&md5=8102c3657a32fb80b60c72a71e36856a","Background: This study aimed to explore whether explainable Artificial Intelligence methods can be fruitfully used to improve the medical management of patients suffering from complex diseases, and in particular to predict the death risk in hospitalized patients with SARS-Cov-2 based on admission data. Methods: This work is based on an observational ambispective study that comprised patients older than 18 years with a positive SARS-Cov-2 diagnosis that were admitted to the hospital Azienda Ospedaliera “SS Antonio e Biagio e Cesare Arrigo”, Alessandria, Italy from February, 24 2020 to May, 31 2021, and that completed the disease treatment inside this structure. The patients’medical history, demographic, epidemiologic and clinical data were collected from the electronic medical records system and paper based medical records, entered and managed by the Clinical Study Coordinators using the REDCap electronic data capture tool patient chart. The dataset was used to train and to evaluate predictive ML models. Results: We overall trained, analysed and evaluated 19 predictive models (both supervised and unsupervised) on data from 824 patients described by 43 features. We focused our attention on models that provide an explanation that is understandable and directly usable by domain experts, and compared the results against other classical machine learning approaches. Among the former, JRIP showed the best performance in 10-fold cross validation, and the best average performance in a further validation test using a different patient dataset from the beginning of the third COVID-19 wave. Moreover, JRIP showed comparable performances with other approaches that do not provide a clear and/or understandable explanation. Conclusions: The ML supervised models showed to correctly discern between low-risk and high-risk patients, even when the medical disease context is complex and the list of features is limited to information available at admission time. Furthermore, the models demonstrated to reasonably perform on a dataset from the third COVID-19 wave that was not used in the training phase. Overall, these results are remarkable: (i) from a medical point of view, these models evaluate good predictions despite the possible differences entitled with different care protocols and the possible influence of other viral variants (i.e. delta variant); (ii) from the organizational point of view, they could be used to optimize the management of health-care path at the admission time. © 2022, The Author(s).","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85145047130"
"Thrun M.C.; Hoffmann J.; Röhnert M.; von Bonin M.; Oelschlägel U.; Brendel C.; Ultsch A.","Thrun, Michael C. (56450887700); Hoffmann, Jörg (55991106000); Röhnert, Maximilian (57189793759); von Bonin, Malte (6506350853); Oelschlägel, Uta (6603586753); Brendel, Cornelia (6603965680); Ultsch, Alfred (55915441200)","56450887700; 55991106000; 57189793759; 6506350853; 6603586753; 6603965680; 55915441200","Flow cytometry datasets consisting of peripheral blood and bone marrow samples for the evaluation of explainable artificial intelligence methods","2022","Data in Brief","43","","108382","","","","2","10.1016/j.dib.2022.108382","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132265895&doi=10.1016%2fj.dib.2022.108382&partnerID=40&md5=f6b94b4478c8e074a57b14150b562dcd","Three different Flow Cytometry datasets consisting of diagnostic samples of either peripheral blood (pB) or bone marrow (BM) from patients without any sign of bone marrow disease at two different health care centers are provided. In Flow Cytometry, each cell rapidly passes through a laser beam one by one, and two light scatter, and eight surface parameters of more than 100.000 cells are measured per sample of each patient. The technology swiftly characterizes cells of the immune system at the single-cell level based on antigens presented on the cell surface that are targeted by a set of fluorochrome-conjugated antibodies. The first dataset consists of N=14 sample files measured in Marburg and the second dataset of N=44 data files measured in Dresden, of which half are BM samples and half are pB samples. The third dataset contains N=25 healthy bone marrow samples and N=25 leukemia bone marrow samples measured in Marburg. The data has been scaled to log between zero and six and used to identify cell populations that are simultaneously meaningful to the clinician and relevant to the distinction of pB vs BM, and BM vs leukemia. Explainable artificial intelligence methods should distinguish these samples and provide meaningful explanations for the classification without taking more than several hours to compute their results. The data described in this article are available in Mendeley Data [1]. © 2022 The Authors","Data paper","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85132265895"
"Jakka A.; Vakula Rani J.","Jakka, Aishwarya (57211293460); Vakula Rani, J. (57211290050)","57211293460; 57211290050","An Explainable AI Approach for Diabetes Prediction","2023","Lecture Notes in Networks and Systems","565 LNNS","","","15","25","10","1","10.1007/978-981-19-7455-7_2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161365190&doi=10.1007%2f978-981-19-7455-7_2&partnerID=40&md5=a8b4bb60fbf1f892a3b33a74a2065b77","Diabetes mellitus is one of the chronic diseases worldwide. According to World Health Organization global report 2019, diabetes was the ninth major cause of mortality. Machine learning techniques have been extensively used in healthcare applications for medical diagnosis and surgical applications. These algorithms often exhibit superior performance and could not explain the predictions because of complex behavior and black-box nature. Explainable AI (XAI) systems have become more popular for post hoc explanations and model transparency. This approach helps to understand the complex structure of ML models and make them transparent and trustworthy in the data-driven and fact-based decision-making process. The practical implementation of the XAI system, the local interpretable model-agnostic explanations (LIME) for diabetes prediction are presented in this paper. The experimental results demonstrate the explanations and factors contributing to diabetes and help the medical practitioners in decision-making to address clinical diagnosis and treatment measures. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Conference paper","Final","","Scopus","2-s2.0-85161365190"
"Inayat Z.","Inayat, Zubaria (55248256900)","55248256900","Evaluation of Quality Requirements for Explanations in AI-based Healthcare Systems","2023","CEUR Workshop Proceedings","3378","","","","","","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158981303&partnerID=40&md5=838f6d54a77a13f99740a1c27e51c914","In the field of explainable artificial intelligence (XAI), methods are being developed to explain AI results. These methods form the range of implementation choices available to XAI designers when dealing with the explainability requirements to a system. While in the discipline of Requirements Engineering, explainability has been conceptualized and operationalized as a nonfunctional requirement, there was so far little focus specifically on the quality aspects of the explanations themselves. Yet, quality requirements issues pertaining to the explanations of AI systems lead to issues such as lack of transparency, trust, and user confidence. The present PhD research makes a step towards closing this gap. The research aims to formulate a solution for determining the quality of explanations in AI systems, particularly in the healthcare domain. We believe that this research will benefit healthcare professionals in maintaining confidence and trust in AI-based healthcare systems. Copyright © 2023 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).","Conference paper","Final","","Scopus","2-s2.0-85158981303"
"Kobusingye B.M.; Dorothy A.; Nakatumba-Nabende J.; Marvin G.","Kobusingye, Belinda Marion (58304838200); Dorothy, Ankunda (58305247700); Nakatumba-Nabende, Joyce (57197759951); Marvin, Ggaliwango (57302525500)","58304838200; 58305247700; 57197759951; 57302525500","Explainable Machine Translation for Intelligent E-Learning of Social Studies","2023","7th International Conference on Trends in Electronics and Informatics, ICOEI 2023 - Proceedings","","","","1066","1072","6","2","10.1109/ICOEI56765.2023.10125599","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161204872&doi=10.1109%2fICOEI56765.2023.10125599&partnerID=40&md5=f8ce125a58457ad1b95e44336bc02be7","Social Studies (SST) fosters civic knowledge of topics such as crime, economic development, and health, allowing students to make informed decisions in a culturally diverse society. Although SST is the subject with the highest performance in the Primary Leaving Examination (PLE), the Ministry of Education reported that pupils who fail do so because they are unable to interpret the questions or respond to them in a timely manner. This is due in part to a linguistic barrier, as children in areas such as rural Central Uganda speak Luganda and only utilize English at school, which affects learning. This prevents them from properly learning to understand the dialogue and exercises provided in English. Machine Translation (MT) is a powerful tool for overcoming language barriers, but the black-box structure of most MT models makes understanding the reasoning behind the translations challenging. Therefore, this paper illustrates the application of Explainable Artificial Intelli-gence(XAI) to support the translation of SST notes from Luganda to English and vice versa. Two transformer models(opus-mt-en-lg and opus-mt-lg-en models) were trained and evaluated using the BiLingual Evaluation Understudy (BLEU) score. The English-Luganda model obtained a 21.43% BLEU score while the Luganda-English model obtained a 16.87% BLEU score, which was higher than the model reference score. This work can be used as an innovative approach to Intelligent E-Learning to overcome barriers to traditional classroom-based language instruction, such as geographical distance and lack of qualified teachers, and can be used to reach a wider audience of children.  © 2023 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85161204872"
"Chalabianloo N.; Can Y.S.; Umair M.; Sas C.; Ersoy C.","Chalabianloo, Niaz (57196246075); Can, Yekta Said (56038955200); Umair, Muhammad (57210678721); Sas, Corina (9636939200); Ersoy, Cem (6701485879)","57196246075; 56038955200; 57210678721; 9636939200; 6701485879","Application level performance evaluation of wearable devices for stress classification with explainable AI","2022","Pervasive and Mobile Computing","87","","101703","","","","10","10.1016/j.pmcj.2022.101703","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140922273&doi=10.1016%2fj.pmcj.2022.101703&partnerID=40&md5=37d5422451212a0cd847f6d5145d6fbf","Stress has become one of the most prominent problems of modern societies and a key contributor to major health issues. Dealing with stress effectively requires detecting it in real-time, informing the user, and giving instructions on how to manage it. Over the past few years, wearable devices equipped with biosensors that can be utilized for stress detection have become increasingly popular. Since they come with various designs and technologies and acquire biosignals from different body locations, choosing a suitable device for a particular application has become a challenge for researchers and end-users. This study compares seven common wearable biosensors for stress detection applications. This was accomplished by collecting physiological sensor data during Baseline, Stress, Recovery, and Cycling sessions from 32 participants. Machine learning algorithms were used to classify four stress classes, and the results obtained from all wearables were compared. Following this, a state-of-the-art explainable artificial intelligence method was employed to clarify our models’ predictions and investigate the influence different features have on the models’ outputs. Despite the results showing that ECG wearables perform slightly better than the rest of the devices, adding a second biosignal (EDA) improved the results significantly, tipping the balance toward multisensor wearables. Finally, we concluded that although the output results of each model can be affected by various factors, in most cases, there is no significant difference in the accuracy of stress detection by different wearables. However, the decision to select a particular wearable for stress detection applications must be made carefully considering the trade-off between the users’ expectations and preferences and the pros and cons of each device. © 2022","Article","Final","","Scopus","2-s2.0-85140922273"
"Suh B.; Yu H.; Kim H.; Lee S.; Kong S.; Kim J.-W.; Choi J.","Suh, Bogyeong (58067525600); Yu, Heejin (57214397027); Kim, Hyeyeon (57193213804); Lee, Sanghwa (58067494900); Kong, Sunghye (57192955603); Kim, Jin-Woo (55333003000); Choi, Jongeun (8850554800)","58067525600; 57214397027; 57193213804; 58067494900; 57192955603; 55333003000; 8850554800","Interpretable Deep-Learning Approaches for Osteoporosis Risk Screening and Individualized Feature Analysis Using Large Population-Based Data: Model Development and Performance Evaluation","2023","Journal of Medical Internet Research","25","","e40179","","","","6","10.2196/40179","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146364834&doi=10.2196%2f40179&partnerID=40&md5=88f8763d0479700d3cfc731f43206bdd","Background: Osteoporosis is one of the diseases that requires early screening and detection for its management. Common clinical tools and machine-learning (ML) models for screening osteoporosis have been developed, but they show limitations such as low accuracy. Moreover, these methods are confined to limited risk factors and lack individualized explanation. Objective: The aim of this study was to develop an interpretable deep-learning (DL) model for osteoporosis risk screening with clinical features. Clinical interpretation with individual explanations of feature contributions is provided using an explainable artificial intelligence (XAI) technique. Methods: We used two separate data sets: the National Health and Nutrition Examination Survey data sets from the United States (NHANES) and South Korea (KNHANES) with 8274 and 8680 respondents, respectively. The study population was classified according to the T-score of bone mineral density at the femoral neck or total femur. A DL model for osteoporosis diagnosis was trained on the data sets and significant risk factors were investigated with local interpretable model-agnostic explanations (LIME). The performance of the DL model was compared with that of ML models and conventional clinical tools. Additionally, contribution ranking of risk factors and individualized explanation of feature contribution were examined. Results: Our DL model showed area under the curve (AUC) values of 0.851 (95% CI 0.844-0.858) and 0.922 (95% CI 0.916-0.928) for the femoral neck and total femur bone mineral density, respectively, using the NHANES data set. The corresponding AUC values for the KNHANES data set were 0.827 (95% CI 0.821-0.833) and 0.912 (95% CI 0.898-0.927), respectively. Through the LIME method, significant features were induced, and each feature’s integrated contribution and interpretation for individual risk were determined. Conclusions: The developed DL model significantly outperforms conventional ML models and clinical tools. Our XAI model produces high-ranked features along with the integrated contributions of each feature, which facilitates the interpretation of individual risk. In summary, our interpretable model for osteoporosis risk screening outperformed state-of-the-art methods. ©Bogyeong Suh, Heejin Yu, Hyeyeon Kim, Sanghwa Lee, Sunghye Kong, Jin-Woo Kim, Jongeun Choi.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85146364834"
"Da Silva Oliveira F.R.; De Lima Neto F.B.","Da Silva Oliveira, Flavio Rosendo (24484211900); De Lima Neto, Fernando Buarque (21742253600)","24484211900; 21742253600","Method to Produce More Reasonable Candidate Solutions With Explanations in Intelligent Decision Support Systems","2023","IEEE Access","11","","","20861","20876","15","0","10.1109/ACCESS.2023.3250262","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149374194&doi=10.1109%2fACCESS.2023.3250262&partnerID=40&md5=20030ee2f300d2646c8f7a9a55f3f95a","The integration of Artificial Intelligence techniques into Decision Support Systems yields effective solutions to decision problems, especially when complex scenarios are at hand. However, the use of intelligent black-box models can hinder the decision support system's potential to be fully adopted because opaque processes raise suspicions and doubts among careful decision makers. Moreover, appropriate and comprehensible explanations may foster trustworthiness and allow for reasonable adjustments or even corrections. This work proposed an approach that incorporates three reasonability aspects into Decision Systems: feasibility, rationality, and plausibility. Thus, by providing decision makers with reasonable candidate solutions for a complex problem, they are expected to perform their tasks more effectively (i.e. decide with more efficiency as well as efficacy). The new approach is accompanied by two proofs of concept in the health and public security areas. Comparative results using random and rational approaches, including the simulation of distinct user profiles, are presented. The proposed approach achieved superior metrics with regard to feasibility and plausibility, suggesting that this proposition can be applied to real-world applications. © 2013 IEEE.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85149374194"
"Horst F.; Slijepcevic D.; Simak M.; Horsak B.; Schöllhorn W.I.; Zeppelzauer M.","Horst, Fabian (57190395360); Slijepcevic, Djordje (57200123809); Simak, Marvin (57250785200); Horsak, Brian (36642262700); Schöllhorn, Wolfgang Immanuel (6505862957); Zeppelzauer, Matthias (16200453900)","57190395360; 57200123809; 57250785200; 36642262700; 6505862957; 16200453900","Modeling biological individuality using machine learning: A study on human gait","2023","Computational and Structural Biotechnology Journal","21","","","3414","3423","9","5","10.1016/j.csbj.2023.06.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162987300&doi=10.1016%2fj.csbj.2023.06.009&partnerID=40&md5=94ac7022d86d66ffa2857d1a6db1f294","Human gait is a complex and unique biological process that can offer valuable insights into an individual's health and well-being. In this work, we leverage a machine learning-based approach to model individual gait signatures and identify factors contributing to inter-individual variability in gait patterns. We provide a comprehensive analysis of gait individuality by (1) demonstrating the uniqueness of gait signatures in a large-scale dataset and (2) highlighting the gait characteristics that are most distinctive to each individual. We utilized the data from three publicly available datasets comprising 5368 bilateral ground reaction force recordings during level overground walking from 671 distinct healthy individuals. Our results show that individuals can be identified with a prediction accuracy of 99.3% by using the bilateral signals of all three ground reaction force components, with only 10 out of 1342 recordings in our test data being misclassified. This indicates that the combination of bilateral ground reaction force signals with all three components provides a more comprehensive and accurate representation of an individual's gait signature. The highest accuracy was achieved by (linear) Support Vector Machines (99.3%), followed by Random Forests (98.7%), Convolutional Neural Networks (95.8%), and Decision Trees (82.8%). The proposed approach provides a powerful tool to better understand biological individuality and has potential applications in personalized healthcare, clinical diagnosis, and therapeutic interventions. © 2023 The Authors","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85162987300"
"Mishra H.M.; Ahmed B.; Shuja M.; Qtaishat A.; Kumar M.","Mishra, Hari Mohan (58250907300); Ahmed, Bilal (58255302600); Shuja, Mirza (57225994336); Qtaishat, Ahmed (57879504700); Kumar, Mukesh (57216577687)","58250907300; 58255302600; 57225994336; 57879504700; 57216577687","Future Directions of Artificial Intelligence and Machine Learning in Healthcare: A Systematic Analysis and Mapping Study","2023","2023 6th International Conference on Information Systems and Computer Networks, ISCON 2023","","","","","","","0","10.1109/ISCON57294.2023.10111959","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159422905&doi=10.1109%2fISCON57294.2023.10111959&partnerID=40&md5=509218d752258babe623c66bea434475","In order to organise previously conducted research on using machine learning in medical contexts, the authors of this paper employ a method known as systematic mapping. In order to do this, we looked through the abstracts of a variety of publications, including medical journals, healthcare periodicals, and conference proceedings, looking for the phrase 'use of artificial intelligence and machine learning in healthcare.' After doing a search on Google Scholar, which resulted in the retrieval of 500 papers, we classified these studies in accordance with their objectives, approaches, primary concerns, and illnesses. With the use of this technique, we were able to organise our findings into the five categories that are as follows: privacy and security; a framework for privacy and security; interpretable machine learning; medical image assessment; electronic health record processing; and transfer learning. In addition, we found that the evaluation of medical images is the topic that receives the most research, that interpretable machine learning and explainable artificial intelligence are becoming increasingly popular, and that most authors are primarily interested in cancer research. To restate our mission, we want to provide future generations of scholars with an accurate picture of where the field is now and where it is headed. . © 2023 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85159422905"
"Ali S.; Imran A.S.; Kastrati Z.; Daudpota S.M.","Ali, Subhan (58190535600); Imran, Ali Shariq (56109077100); Kastrati, Zenun (56960506200); Daudpota, Sher Muhammad (57190295678)","58190535600; 56109077100; 56960506200; 57190295678","Visualizing Research on Explainable Artificial Intelligence for Medical and Healthcare","2023","2023 4th International Conference on Computing, Mathematics and Engineering Technologies: Sustainable Technologies for Socio-Economic Development, iCoMET 2023","","","","","","","0","10.1109/iCoMET57998.2023.10099343","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159031140&doi=10.1109%2fiCoMET57998.2023.10099343&partnerID=40&md5=0dd2930a7b8e8c72e67a9819f7622c18","Understanding complex machine learning and artificial intelligence models have always been challenging because these models are black-box, and often we don't know what information models rely upon to infer. Explainable Artificial Intelligence (XAI) has emerged as a new exciting field to explain and understand these machine learning models as humans can understand and improve them. In the past few years, there have been numerous research articles on explainable artificial intelligence for medical and healthcare. 1687 documents are being studied and analysed using bibliometric methods in this work. There are certain systematic reviews on the same topic, but this study is the first of its kind to use a quantitative method to analyze a large number of publications. The results of this study show that the research in this field took pace in 2011, and there have been quite many publications in the following years. We have also identified top-cited journals and articles. Through thematic analysis, we have found some important thematic areas of research in the field of XAI for medical and healthcare. The findings showed that the USA is the global leader in XAI research, followed by China and Canada at second and third place, respectively.  © 2023 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85159031140"
"Confortola G.; Takata M.; Yokoi N.; Egi M.","Confortola, Giada (58083618600); Takata, Mika (34881102400); Yokoi, Naoaki (58169730900); Egi, Masashi (35221365400)","58083618600; 34881102400; 58169730900; 35221365400","Actionable Suggestions in Support of Rehospitalization Risk Predicted by Artificial Intelligence","2023","Proceedings - 2023 IEEE International Conference on Big Data and Smart Computing, BigComp 2023","","","","155","162","7","0","10.1109/BigComp57234.2023.00034","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151509791&doi=10.1109%2fBigComp57234.2023.00034&partnerID=40&md5=74932ed74da280e49cbb34d00c576c79","Discharging a patient from hospital is a crucial decision, as wrong timing may lead to rehospitalization. In this scenario, Artificial Intelligence (AI) can support physicians in the decision-making process. However, AI provides a probability that an event will happen, without information regarding how to reduce such probability. EXplainable Artificial Intelligence (XAI) methods, allow to better understand how the score is computed but still lack in providing specific suggestions. We propose a method that creates a set of actionable suggestions on how to change the feature values to reduce a risk predicted by AI, and support the physicians in deciding when to discharge a patient. We utilize historical data distribution of features to understand how values for the current case should be modified. After retrieving a set of relevant historical data by considering the similarity with the current point, we use statistical analysis to highlight differences between the feature values of current and historical cases. A suggestion is made whenever a significative difference on a controllable feature is found. For the evaluation, we used an electronic health record (MIMIC-III) to produce suggestions to reduce 30 days rehospitalization probability. A ground truth set of suggestions was created for 54 patients, by considering how changes in feature values affected the risk, as well as if feature values fell within literature ranges. The proposed approach reached a precision of 80%. The adoption of this method will allow to decrease costs related to rehospitalizations and positively impact patients' health. © 2023 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85151509791"
"Ivanovic M.; Autexier S.; Kokkonidis M.; Rust J.","Ivanovic, Mirjana (7005907326); Autexier, Serge (57193752568); Kokkonidis, Miltiadis (23027828700); Rust, Johannes (57384023600)","7005907326; 57193752568; 23027828700; 57384023600","Quality medical data management within an open AI architecture–cancer patients case","2023","Connection Science","35","1","2194581","","","","5","10.1080/09540091.2023.2194581","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153114060&doi=10.1080%2f09540091.2023.2194581&partnerID=40&md5=5b7c57353363f2df0c929e8adeddbec6","In contemporary society people constantly are facing situations that influence appearance of serious diseases. For the development of intelligent decision support systems and services in medical and health domains, it is necessary to collect huge amount of patients’ complex data. Patient’s multimodal data must be properly prepared for intelligent processing and obtained results should be presented in a friendly way to the physicians/caregivers to recommend tailored actions that will improve patients’ quality of life. Advanced artificial intelligence approaches like machine/deep learning, federated learning, explainable artificial intelligence open new paths for more quality use of medical and health data in future. In this paper, we will focus on presentation of a part of a novel Open AI Architecture for cancer patients that is devoted to intelligent medical data management. Essential activities are data collection, proper design and preparation of data to be used for training machine learning predictive models. Another key aspect is oriented towards intelligent interpretation and visualisation of results about patient’s quality of life obtained from machine learning models. The Architecture has been developed as a part of complex project in which 15 institutions from 8 European countries have been participated. © 2023 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85153114060"
"Saravanan S.; Ramkumar K.; Narasimhan K.; Vairavasundaram S.; Kotecha K.; Abraham A.","Saravanan, S. (58688350600); Ramkumar, Kannan (57119513100); Narasimhan, K. (35189341400); Vairavasundaram, Subramaniyaswamy (54888993500); Kotecha, Ketan (6506676097); Abraham, Ajith (7202760099)","58688350600; 57119513100; 35189341400; 54888993500; 6506676097; 7202760099","Explainable Artificial Intelligence (EXAI) Models for Early Prediction of Parkinson's Disease Based on Spiral and Wave Drawings","2023","IEEE Access","11","","","68366","68378","12","6","10.1109/ACCESS.2023.3291406","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164451991&doi=10.1109%2fACCESS.2023.3291406&partnerID=40&md5=7ad2e405c8940fb2db9b48bdd29fcc94","Parkinson's disease (PD) is a rapidly growing neurodegenerative disorder that primarily affects the elderly population. Until now, there has been no antidote for PD. However, diagnosing Parkinson's disease in its early stages is difficult. Early treatment will help people with Parkinson's disease improve their quality of life. The primary goal of this work is to increase the early diagnostic accuracy of Parkinson's disease using deep learning models and to make the models more transparent and trustworthy. It proved challenging to comprehend the methods by which the classifiers made predictions about Parkinson's disease. It would be valuable if the outcomes generated by these classifiers could be clarified in a reliable and trustworthy manner. Explainable Artificial Intelligence (EXAI) focuses on enhancing clinical health practises and bringing transparency to predictive analysis, both of which are critical in the healthcare arena. We proposed a new hybrid deep transfer learning model to distinguish PD patients from healthy individuals. The proposed architecture combines the advantages of both VGG19 Net and Google Net. This study also shows the experimental outcomes of various pre-trained models such as Alex Net, DenseNet-201, VGG-19 Net, Squeeze Net1.1, and ResNet-50. The VGG19-INC model predicts PD with an accuracy of 98.45%, which is greater than other state-of-the-art approaches, demonstrating the proposed work's superiority and robustness. To demystify the VGG19-INC model, explainable AI approaches such as LIME are used to identify the specific parts of the spiral and wave drawings that contribute most to the model's prediction. These methods provide local interpretation, making it easier to understand how the model arrives at its conclusions. © 2013 IEEE.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85164451991"
"Roy S.; Meena T.; Lim S.-J.","Roy, Sudipta (56406670700); Meena, Tanushree (57221382965); Lim, Se-Jung (57205217776)","56406670700; 57221382965; 57205217776","Demystifying Supervised Learning in Healthcare 4.0: A New Reality of Transforming Diagnostic Medicine","2022","Diagnostics","12","10","2549","","","","69","10.3390/diagnostics12102549","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140635489&doi=10.3390%2fdiagnostics12102549&partnerID=40&md5=5465f01681023c2b25d41b7144b19fb4","The global healthcare sector continues to grow rapidly and is reflected as one of the fastest-growing sectors in the fourth industrial revolution (4.0). The majority of the healthcare industry still uses labor-intensive, time-consuming, and error-prone traditional, manual, and manpower-based methods. This review addresses the current paradigm, the potential for new scientific discoveries, the technological state of preparation, the potential for supervised machine learning (SML) prospects in various healthcare sectors, and ethical issues. The effectiveness and potential for innovation of disease diagnosis, personalized medicine, clinical trials, non-invasive image analysis, drug discovery, patient care services, remote patient monitoring, hospital data, and nanotechnology in various learning-based automation in healthcare along with the requirement for explainable artificial intelligence (AI) in healthcare are evaluated. In order to understand the potential architecture of non-invasive treatment, a thorough study of medical imaging analysis from a technical point of view is presented. This study also represents new thinking and developments that will push the boundaries and increase the opportunity for healthcare through AI and SML in the near future. Nowadays, SML-based applications require a lot of data quality awareness as healthcare is data-heavy, and knowledge management is paramount. Nowadays, SML in biomedical and healthcare developments needs skills, quality data consciousness for data-intensive study, and a knowledge-centric health management system. As a result, the merits, demerits, and precautions need to take ethics and the other effects of AI and SML into consideration. The overall insight in this paper will help researchers in academia and industry to understand and address the future research that needs to be discussed on SML in the healthcare and biomedical sectors. © 2022 by the authors.","Review","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85140635489"
"Sekaran K.; Polachirakkal Varghese R.; Gnanasambandan R.; Karthik G.; Ramya I.; George Priya Doss C.","Sekaran, Karthik (57208071216); Polachirakkal Varghese, Rinku (58019979200); Gnanasambandan, R. (55977409800); Karthik, G. (46062121100); Ramya, I. (36017552100); George Priya Doss, C. (57218609913)","57208071216; 58019979200; 55977409800; 46062121100; 36017552100; 57218609913","Molecular modeling of C1-inhibitor as SARS-CoV-2 target identified from the immune signatures of multiple tissues: An integrated bioinformatics study","2023","Cell Biochemistry and Function","41","1","","112","127","15","0","10.1002/cbf.3769","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144098297&doi=10.1002%2fcbf.3769&partnerID=40&md5=9ec68aa7fbecced29a41d1c9490ccc7c","The expeditious transmission of the severe acute respiratory coronavirus 2 (SARS-CoV-2), a strain of COVID-19, crumbled the global economic strength and caused a veritable collapse in health infrastructure. The molecular modeling of the novel coronavirus research sounds promising and equips more evidence about the pragmatic therapeutic options. This article proposes a machine-learning framework for identifying potential COVID-19 transcriptomic signatures. The transcriptomics data contains immune-related genes collected from multiple tissues (blood, nasal, and buccal) with accession number: GSE183071. Extensive bioinformatics work was carried out to identify the potential candidate markers, including differential expression analysis, protein interactions, gene ontology, and KEGG (Kyoto Encyclopedia of Genes and Genomes) pathway enrichment studies. The overlapping investigation found SERPING1, the gene that encodes a glycosylated plasma protein C1-INH, in all three datasets. Furthermore, the immuno-informatics study was conducted on the C1-INH protein. 5DU3, the protein identifier of C1-INH, was fetched to identify the antigenicity, major histocompatibility (MHC) Class I and II binding epitopes, allergenicity, toxicity, and immunogenicity. The screening of peptides satisfying the vaccine-design criteria based on the metrics mentioned above is performed. The drug–gene interaction study reported that Rhucin is strongly associated with SERPING1. HSIC-Lasso (Hilbert–Schmidt independence criterion-least absolute shrinkage and selection operator), a model-free biomarker selection technique, was employed to identify the genes having a nonlinear relationship with the target class. The gene subset is trained with supervised machine learning models by a leave-one-out cross-validation method. Explainable artificial intelligence techniques perform the model interpretation analysis. © 2022 John Wiley & Sons Ltd.","Article","Final","","Scopus","2-s2.0-85144098297"
"Nguyen D.Q.; Vo N.Q.; Nguyen T.T.; Nguyen-An K.; Nguyen Q.H.; Tran D.N.; Quan T.T.","Nguyen, Duc Q. (57435775000); Vo, Nghia Q. (57687483800); Nguyen, Thinh T. (57222350851); Nguyen-An, Khuong (57191976807); Nguyen, Quang H. (57220935005); Tran, Dang N. (56176864100); Quan, Tho T. (23398146600)","57435775000; 57687483800; 57222350851; 57191976807; 57220935005; 56176864100; 23398146600","BeCaked: An Explainable Artificial Intelligence Model for COVID-19 Forecasting","2022","Scientific Reports","12","1","7969","","","","7","10.1038/s41598-022-11693-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130028197&doi=10.1038%2fs41598-022-11693-9&partnerID=40&md5=61dff2f4dad84749b8fadf99f5c51f01","From the end of 2019, one of the most serious and largest spread pandemics occurred in Wuhan (China) named Coronavirus (COVID-19). As reported by the World Health Organization, there are currently more than 100 million infectious cases with an average mortality rate of about five percent all over the world. To avoid serious consequences on people’s lives and the economy, policies and actions need to be suitably made in time. To do that, the authorities need to know the future trend in the development process of this pandemic. This is the reason why forecasting models play an important role in controlling the pandemic situation. However, the behavior of this pandemic is extremely complicated and difficult to be analyzed, so that an effective model is not only considered on accurate forecasting results but also the explainable capability for human experts to take action pro-actively. With the recent advancement of Artificial Intelligence (AI) techniques, the emerging Deep Learning (DL) models have been proving highly effective when forecasting this pandemic future from the huge historical data. However, the main weakness of DL models is lacking the explanation capabilities. To overcome this limitation, we introduce a novel combination of the Susceptible-Infectious-Recovered-Deceased (SIRD) compartmental model and Variational Autoencoder (VAE) neural network known as BeCaked. With pandemic data provided by the Johns Hopkins University Center for Systems Science and Engineering, our model achieves 0.98 R2 and 0.012 MAPE at world level with 31-step forecast and up to 0.99 R2 and 0.0026 MAPE at country level with 15-step forecast on predicting daily infectious cases. Not only enjoying high accuracy, but BeCaked also offers useful justifications for its results based on the parameters of the SIRD model. Therefore, BeCaked can be used as a reference for authorities or medical experts to make on time right decisions. © 2022, The Author(s).","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85130028197"
"Rajabi E.; Kafaie S.","Rajabi, Enayat (42862135700); Kafaie, Somayeh (42961589600)","42862135700; 42961589600","Knowledge Graphs and Explainable AI in Healthcare","2022","Information (Switzerland)","13","10","459","","","","11","10.3390/info13100459","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140638079&doi=10.3390%2finfo13100459&partnerID=40&md5=22c15a56cc7280cfaaf83d448ab77d5a","Building trust and transparency in healthcare can be achieved using eXplainable Artificial Intelligence (XAI), as it facilitates the decision-making process for healthcare professionals. Knowledge graphs can be used in XAI for explainability by structuring information, extracting features and relations, and performing reasoning. This paper highlights the role of knowledge graphs in XAI models in healthcare, considering a state-of-the-art review. Based on our review, knowledge graphs have been used for explainability to detect healthcare misinformation, adverse drug reactions, drug-drug interactions and to reduce the knowledge gap between healthcare experts and AI-based models. We also discuss how to leverage knowledge graphs in pre-model, in-model, and post-model XAI models in healthcare to make them more explainable. © 2022 by the authors.","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85140638079"
"Javed A.R.; Khan H.U.; Alomari M.K.B.; Sarwar M.U.; Asim M.; Almadhor A.S.; Khan M.Z.","Javed, Abdul Rehman (57215024733); Khan, Habib Ullah (55507524000); Alomari, Mohammad Kamel Bader (55389149200); Sarwar, Muhammad Usman (57216155362); Asim, Muhammad (57200862047); Almadhor, Ahmad S. (57202875014); Khan, Muhammad Zahid (57203529114)","57215024733; 55507524000; 55389149200; 57216155362; 57200862047; 57202875014; 57203529114","Toward explainable AI-empowered cognitive health assessment","2023","Frontiers in Public Health","11","","1024195","","","","7","10.3389/fpubh.2023.1024195","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150733319&doi=10.3389%2ffpubh.2023.1024195&partnerID=40&md5=8fcc2e6b2cfe0e067cd5f576e729d1b2","Explainable artificial intelligence (XAI) is of paramount importance to various domains, including healthcare, fitness, skill assessment, and personal assistants, to understand and explain the decision-making process of the artificial intelligence (AI) model. Smart homes embedded with smart devices and sensors enabled many context-aware applications to recognize physical activities. This study presents XAI-HAR, a novel XAI-empowered human activity recognition (HAR) approach based on key features identified from the data collected from sensors located at different places in a smart home. XAI-HAR identifies a set of new features (i.e., the total number of sensors used in a specific activity), as physical key features selection (PKFS) based on weighting criteria. Next, it presents statistical key features selection (SKFS) (i.e., mean, standard deviation) to handle the outliers and higher class variance. The proposed XAI-HAR is evaluated using machine learning models, namely, random forest (RF), K-nearest neighbor (KNN), support vector machine (SVM), decision tree (DT), naive Bayes (NB) and deep learning models such as deep neural network (DNN), convolution neural network (CNN), and CNN-based long short-term memory (CNN-LSTM). Experiments demonstrate the superior performance of XAI-HAR using RF classifier over all other machine learning and deep learning models. For explainability, XAI-HAR uses Local Interpretable Model Agnostic (LIME) with an RF classifier. XAI-HAR achieves 0.96% of F-score for health and dementia classification and 0.95 and 0.97% for activity recognition of dementia and healthy individuals, respectively. Copyright © 2023 Javed, Khan, Alomari, Sarwar, Asim, Almadhor and Khan.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85150733319"
"Priya Dharshini S.; Ram Kumar K.; Venkatesh S.; Narasimhan K.; Adalarasu K.","Priya Dharshini, S. (58264749500); Ram Kumar, K. (57212750437); Venkatesh, S. (58039592900); Narasimhan, K. (35189341400); Adalarasu, K. (55799536200)","58264749500; 57212750437; 58039592900; 35189341400; 55799536200","An Overview Of Interpretability Techniques For Explainable Artificial Intelligence (XAI) In Deep Learning-Based Medical Image Analysis","2023","2023 9th International Conference on Advanced Computing and Communication Systems, ICACCS 2023","","","","175","182","7","1","10.1109/ICACCS57279.2023.10113001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159769897&doi=10.1109%2fICACCS57279.2023.10113001&partnerID=40&md5=d8c3218d625352c3b389a13d5b1e5929","In the past two years, technology has undergone significant changes that have had a major impact on healthcare systems. Artificial intelligence (AI) is a key component of this change, and it can assist doctors with various healthcare systems and intelligent health systems. AI is crucial in diagnosing common diseases, developing new medications, and analyzing patient information from electronic health records. However, one of the main issues with adopting AI in healthcare is the lack of transparency, as doctors must interpret the output of the AI. Explainable AI (XAI) is extremely important for the healthcare sector and comes into play in this regard. With XAI, doctors, patients, and other stakeholders can more easily examine a decision's reliability by knowing its reasoning due to XAI's interpretable explanations. Deep learning is used in this study to discuss explainable artificial intelligence (XAI) in medical image analysis. The primary goal of this paper is to provide a generic six-category XAI architecture for classifying DL-based medical image analysis and interpretability methods.The interpretability method/XAI approach for medical image analysis is often categorized based on the explanation and technical method. In XAI approaches, the explanation method is further sub-categorized into three types: text-based, visual-based, and examples-based. In interpretability technical method, it was divided into nine categories. Finally, the paper discusses the advantages, disadvantages, and limitations of each neural network-based interpretability method for medical imaging analysis. © 2023 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85159769897"
"Baweja A.K.; Aditya S.; Kanchana M.","Baweja, Asis Kaur (57439990700); Aditya, S. (58373212800); Kanchana, M. (57216477082)","57439990700; 58373212800; 57216477082","Leprosy Diagnosis using Explainable Artificial Intelligence Techniques","2023","2nd International Conference on Sustainable Computing and Data Communication Systems, ICSCDS 2023 - Proceedings","","","","551","556","5","2","10.1109/ICSCDS56580.2023.10104958","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159096925&doi=10.1109%2fICSCDS56580.2023.10104958&partnerID=40&md5=b4c651ea70228b5967489dee532756d4","LRprosy, also known as Hansen's Disease is a chronic curable infection that seldom causes skin lesion and nerve damage. The World Health organization reported 127558 new leprosy cases detected globally in 2020. National Leprosy Eradication Program (NLEP) initiated by India, is the largest leprosy eradication program of the world. Despite this, 53.64% of leprosy cases (120,000 to 130,000 per year) are in India. Although this disease is curable at the later stages, an early diagnosis eradicates the complications incurred by nerve involvement. Existing research predicts leprosy on basis of electronic health records which require complex contact based techniques for procuration. Image based predictors lack credibility as it is hard to determine what features are being taken into consideration for prediction. This paper proposes an optimal AXI-CNN architecture for leprosy prediction called LeprosyNet. Deep Learning models are seldom black box in nature, in order to understand the area of interest in the images, explainable AI techniques like Activation Layer Visualization, Occlusion Sensitivity and Grad-CAM are used. The proposed model is compared with famous state-of-art architectures AlexNet and ResNet. Evaluation of the model has been undertaken using a ROC curve and confusion matrix. Accuracy obtained on LeprosyNet is 98%.  © 2023 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85159096925"
"Upadhyay R.; Knoth P.; Pasi G.; Viviani M.","Upadhyay, Rishabh (57231318200); Knoth, Petr (36028520200); Pasi, Gabriella (7003307397); Viviani, Marco (13410043100)","57231318200; 36028520200; 7003307397; 13410043100","Explainable online health information truthfulness in Consumer Health Search","2023","Frontiers in Artificial Intelligence","6","","1184851","","","","1","10.3389/frai.2023.1184851","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161702064&doi=10.3389%2ffrai.2023.1184851&partnerID=40&md5=c0f0962e3b443ff0eac8a95284cf664c","Introduction: People are today increasingly relying on health information they find online to make decisions that may impact both their physical and mental wellbeing. Therefore, there is a growing need for systems that can assess the truthfulness of such health information. Most of the current literature solutions use machine learning or knowledge-based approaches treating the problem as a binary classification task, discriminating between correct information and misinformation. Such solutions present several problems with regard to user decision making, among which: (i) the binary classification task provides users with just two predetermined possibilities with respect to the truthfulness of the information, which users should take for granted; indeed, (ii) the processes by which the results were obtained are often opaque and the results themselves have little or no interpretation. Methods: To address these issues, we approach the problem as an ad hoc retrieval task rather than a classification task, with reference, in particular, to the Consumer Health Search task. To do this, a previously proposed Information Retrieval model, which considers information truthfulness as a dimension of relevance, is used to obtain a ranked list of both topically-relevant and truthful documents. The novelty of this work concerns the extension of such a model with a solution for the explainability of the results obtained, by relying on a knowledge base consisting of scientific evidence in the form of medical journal articles. Results and discussion: We evaluate the proposed solution both quantitatively, as a standard classification task, and qualitatively, through a user study to examine the “explained” ranked list of documents. The results obtained illustrate the solution's effectiveness and usefulness in making the retrieved results more interpretable by Consumer Health Searchers, both with respect to topical relevance and truthfulness. Copyright © 2023 Upadhyay, Knoth, Pasi and Viviani.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85161702064"
"Shaikh T.A.; Rasool T.; Rasool A.","Shaikh, Tawseef Ayoub (57193888269); Rasool, Tabasum (57216812426); Rasool, Abrar (58190028200)","57193888269; 57216812426; 58190028200","Explainable artificial intelligence in smart health care systems","2022","AI-Enabled IoT for Smart Health Care Systems","","","","175","214","39","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152977910&partnerID=40&md5=d67b284a160e7550e447963ec0d63438","Introduction: With recent breakthroughs in organized and amorphous data and rapid growth in analysis methodologies, Artificial Intelligence (AI) is causing a revolution in the health care industry. The value of Artificial Intelligence in health care is becoming recognized at the same time that people are becoming concerned about the models' possible lack of explainability and bias. This discusses the concept of explainable artificial intelligence (XAI), which enhances a system's trustworthiness, resulting in more widespread AI adoption in health care. In this chapter, we discuss several perspectives on explainable artificial intelligence principles, as well as the understandability and interpretability of explainable AI systems, with a particular focus on the health care sector. This chapter uses AI explainability as a way to help build trustworthiness in the medical domain and takes a look at the recent developments in the area of explainable AI, which encourages creativity and at times are necessary in practice to raise awareness. The goal is to teach health care providers about the interpretability and understandability of explainable AI systems. © 2023 by Nova Science Publishers, Inc. All rights reserved.","Book chapter","Final","","Scopus","2-s2.0-85152977910"
"Jeyashree G.; Padmavathi S.","Jeyashree, G. (57564258700); Padmavathi, S. (19933819300)","57564258700; 19933819300","IHAR—A fog-driven interpretable human activity recognition system","2022","Transactions on Emerging Telecommunications Technologies","33","9","e4506","","","","2","10.1002/ett.4506","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127716187&doi=10.1002%2fett.4506&partnerID=40&md5=c6537b7d3f03e1ec8fb52fdf015006e5","Human activity recognition (HAR) is an active research area that is currently being applied to various healthcare applications such as fall detection, assisted living, etc. These applications make use of the Internet of Things which is widespread across today's world. One of the major challenges in these applications is the need for quick intelligent decisions. The deployment of hierarchical edge-fog-cloud computing architecture is the potential solution to address the latency issue. In this article, we proposed an interpretable human activity recognition (IHAR) framework that supports HAR, leveraging the advantages of the fog layer. The proposed framework used a deep learning (DL) model at the cloud infrastructure to classify the activities of humans. The trained DL model is made to run on local fog nodes. However, the DL model being black-box does not provide any explanations for the output to make them acceptable by the users and physicians. Hence, this article also incorporated an explainable artificial intelligence model at the fog layer, to gain insights into the classified output of the DL model. The effective model-level insights emphasized the need for explainable HAR. The article also included an analysis of the results of the existing explainable AI models such as LIME and SHAP to understand which model leads to better performance in the domain of HAR. Results show that the SHAP model has a 33% higher success rate in generating explanations when compared to the LIME model. © 2022 John Wiley & Sons, Ltd.","Article","Final","","Scopus","2-s2.0-85127716187"
"Sokhansanj B.A.; Zhao Z.; Rosen G.L.","Sokhansanj, Bahrad A. (55885818400); Zhao, Zhengqiao (57201860478); Rosen, Gail L. (9335288900)","55885818400; 57201860478; 9335288900","Interpretable and Predictive Deep Neural Network Modeling of the SARS-CoV-2 Spike Protein Sequence to Predict COVID-19 Disease Severity","2022","Biology","11","12","1786","","","","3","10.3390/biology11121786","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144727503&doi=10.3390%2fbiology11121786&partnerID=40&md5=acabefda3b5587144e92217ab49b22d4","Through the COVID-19 pandemic, SARS-CoV-2 has gained and lost multiple mutations in novel or unexpected combinations. Predicting how complex mutations affect COVID-19 disease severity is critical in planning public health responses as the virus continues to evolve. This paper presents a novel computational framework to complement conventional lineage classification and applies it to predict the severe disease potential of viral genetic variation. The transformer-based neural network model architecture has additional layers that provide sample embeddings and sequence-wide attention for interpretation and visualization. First, training a model to predict SARS-CoV-2 taxonomy validates the architecture’s interpretability. Second, an interpretable predictive model of disease severity is trained on spike protein sequence and patient metadata from GISAID. Confounding effects of changing patient demographics, increasing vaccination rates, and improving treatment over time are addressed by including demographics and case date as independent input to the neural network model. The resulting model can be interpreted to identify potentially significant virus mutations and proves to be a robust predctive tool. Although trained on sequence data obtained entirely before the availability of empirical data for Omicron, the model can predict the Omicron’s reduced risk of severe disease, in accord with epidemiological and experimental data. © 2022 by the authors.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85144727503"
"Byeon H.","Byeon, Haewon (36452769600)","36452769600","Advances in Machine Learning and Explainable Artificial Intelligence for Depression Prediction","2023","International Journal of Advanced Computer Science and Applications","14","6","","520","526","6","3","10.14569/IJACSA.2023.0140656","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165013433&doi=10.14569%2fIJACSA.2023.0140656&partnerID=40&md5=dfd4dbcca832bbd47521828404971591","There is a growing interest in applying AI technology in the field of mental health, particularly as an alternative to complement the limitations of human analysis, judgment, and accessibility in mental health assessments and treatments. The current mental health treatment service faces a gap in which individuals who need help are not receiving it due to negative perceptions of mental health treatment, lack of professional manpower, and physical accessibility limitations. To overcome these difficulties, there is a growing need for a new approach, and AI technology is being explored as a potential solution. Explainable artificial intelligence (X-AI) with both accuracy and interpretability technology can help improve the accuracy of expert decision-making, increase the accessibility of mental health services, and solve the psychological problems of high-risk groups of depression. In this review, we examine the current use of X-AI technology in mental health assessments for depression. As a result of reviewing 6 studies that used X-AI to discriminate high-risk groups of depression, various algorithms such as SHAP (SHapley Additive exPlanations) and Local Interpretable Model-Agnostic Explanation (LIME) were used for predicting depression. In the field of psychiatry, such as predicting depression, it is crucial to ensure AI prediction justifications are clear and transparent. Therefore, ensuring interpretability of AI models will be important in future research. © 2023, International Journal of Advanced Computer Science and Applications. All Rights Reserved.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85165013433"
"Allen B.; Lane M.; Steeves E.A.; Raynor H.","Allen, Ben (57853800500); Lane, Morgan (57853691700); Steeves, Elizabeth Anderson (55535115700); Raynor, Hollie (6602547633)","57853800500; 57853691700; 55535115700; 6602547633","Using Explainable Artificial Intelligence to Discover Interactions in an Ecological Model for Obesity","2022","International Journal of Environmental Research and Public Health","19","15","9447","","","","4","10.3390/ijerph19159447","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136342022&doi=10.3390%2fijerph19159447&partnerID=40&md5=34aac9d48d7766c189287c76eb95d2fe","Ecological theories suggest that environmental, social, and individual factors interact to cause obesity. Yet, many analytic techniques, such as multilevel modeling, require manual specification of interacting factors, making them inept in their ability to search for interactions. This paper shows evidence that an explainable artificial intelligence approach, commonly employed in genomics research, can address this problem. The method entails using random intersection trees to decode interactions learned by random forest models. Here, this approach is used to extract interactions between features of a multi-level environment from random forest models of waist-to-height ratios using 11,112 participants from the Adolescent Brain Cognitive Development study. This study shows that methods used to discover interactions between genes can also discover interacting features of the environment that impact obesity. This new approach to modeling ecosystems may help shine a spotlight on combinations of environmental features that are important to obesity, as well as other health outcomes. © 2022 by the authors.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85136342022"
"Maqsood S.; Damaševičius R.; Maskeliūnas R.","Maqsood, Sarmad (57212142651); Damaševičius, Robertas (6603451290); Maskeliūnas, Rytis (27467587600)","57212142651; 6603451290; 27467587600","Multi-Modal Brain Tumor Detection Using Deep Neural Network and Multiclass SVM","2022","Medicina (Lithuania)","58","8","1090","","","","96","10.3390/medicina58081090","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136629340&doi=10.3390%2fmedicina58081090&partnerID=40&md5=535fd38c4a147afdfd3b6e410863e4ef","Background and Objectives: Clinical diagnosis has become very significant in today’s health system. The most serious disease and the leading cause of mortality globally is brain cancer which is a key research topic in the field of medical imaging. The examination and prognosis of brain tumors can be improved by an early and precise diagnosis based on magnetic resonance imaging. For computer-aided diagnosis methods to assist radiologists in the proper detection of brain tumors, medical imagery must be detected, segmented, and classified. Manual brain tumor detection is a monotonous and error-prone procedure for radiologists; hence, it is very important to implement an automated method. As a result, the precise brain tumor detection and classification method is presented. Materials and Methods: The proposed method has five steps. In the first step, a linear contrast stretching is used to determine the edges in the source image. In the second step, a custom 17-layered deep neural network architecture is developed for the segmentation of brain tumors. In the third step, a modified MobileNetV2 architecture is used for feature extraction and is trained using transfer learning. In the fourth step, an entropy-based controlled method was used along with a multiclass support vector machine (M-SVM) for the best features selection. In the final step, M-SVM is used for brain tumor classification, which identifies the meningioma, glioma and pituitary images. Results: The proposed method was demonstrated on BraTS 2018 and Figshare datasets. Experimental study shows that the proposed brain tumor detection and classification method outperforms other methods both visually and quantitatively, obtaining an accuracy of 97.47% and 98.92%, respectively. Finally, we adopt the eXplainable Artificial Intelligence (XAI) method to explain the result. Conclusions: Our proposed approach for brain tumor detection and classification has outperformed prior methods. These findings demonstrate that the proposed approach obtained higher performance in terms of both visually and enhanced quantitative evaluation with improved accuracy. © 2022 by the authors.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85136629340"
"Lessage X.; Nedoszytko M.; Mahmoudi S.; Marey L.; Debauche O.; Mahmoudi S.A.","Lessage, Xavier (57249163400); Nedoszytko, Michal (57246533700); Mahmoudi, Saïd (16177658000); Marey, Lilian (57966119400); Debauche, Olivier (57195530506); Mahmoudi, Sidi Ahmed (55410455500)","57249163400; 57246533700; 16177658000; 57966119400; 57195530506; 55410455500","A New Coronary Artery Stenosis Detection Method with a Hybrid LSTM-CNN Model","2023","Lecture Notes in Networks and Systems","635 LNNS","","","70","77","7","1","10.1007/978-3-031-26254-8_10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151163909&doi=10.1007%2f978-3-031-26254-8_10&partnerID=40&md5=b8f389ef78bb27b4cd7af60b1e2f8584","Screening for coronary artery disease is a major health issue, knowing that the most common cause of death in industrialized countries is cardiovascular pathology (coronary artery disease, stroke, other cardiovascular diseases). Computer Aided Diagnosis systems (CADx) can assist cardiologists to and play a key role in detecting abnormalities and treating coronary arteries. In this paper we propose a deep learning classification method based on a new Hybrid CNN-LSTM Architecture. The aim of our method is to detect the presence of stenosis in the coronary arteries and to classify the type of arteries. Our experiments have been conducted using an anonymized database from a Belgian hospital (CHR Mons-Hainaut) thanks to a retrospective study. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Conference paper","Final","","Scopus","2-s2.0-85151163909"
"Aljameel S.S.","Aljameel, Sumayh S. (57193404866)","57193404866","A Proactive Explainable Artificial Neural Network Model for the Early Diagnosis of Thyroid Cancer","2022","Computation","10","10","183","","","","2","10.3390/computation10100183","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140652714&doi=10.3390%2fcomputation10100183&partnerID=40&md5=3188e8bcde1a1a3d33657dd6d40dfbf0","Early diagnosis of thyroid cancer can reduce mortality, and can decrease the risk of recurrence, side effects, or the need for lengthy surgery. In this study, an explainable artificial neural network (EANN) model was developed to distinguish between malignant and benign nodules and to understand the factors that are predictive of malignancy. The study was conducted using the records of 724 patients who were admitted to Shengjing Hospital of China Medical University. The dataset contained the patients’ demographic information, nodule characteristics, blood test findings, and thyroid characteristics. The performance of the model was evaluated using the metrics of accuracy, sensitivity, specificity, F1 score, and area under the curve (AUC). The SMOTEENN combined sampling method was used to correct for a significant imbalance between malignant and benign nodules in the dataset. The proposed model outperformed a baseline study, with an accuracy of 0.99 and an AUC of 0.99. The proposed EANN model can assist health care professionals by enabling them to make effective early cancer diagnoses. © 2022 by the author.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85140652714"
"Volkov E.N.; Averkin A.N.","Volkov, Egor N. (58493887900); Averkin, Aleksej N. (57192557075)","58493887900; 57192557075","Explainable Artificial Intelligence in Medical Image Analysis: State of the Art and Prospects","2023","Proceedings of 2023 26th International Conference on Soft Computing and Measurements, SCM 2023","","","","134","137","3","2","10.1109/SCM58628.2023.10159033","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165160205&doi=10.1109%2fSCM58628.2023.10159033&partnerID=40&md5=a0923c5bdc44a13387b786b28fdb9681","to increase transparency in the work of artificial intelligence systems in the analysis of medical images is called to use methods of explainable artificial intelligence. Our study provides an overview of the current state of application of explainable artificial intelligence methods for medical image analysis, and also considers potentially promising approaches to improving the technology. © 2023 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85165160205"
"Giuste F.; Shi W.; Zhu Y.; Naren T.; Isgut M.; Sha Y.; Tong L.; Gupte M.; Wang M.D.","Giuste, Felipe (55427597100); Shi, Wenqi (57220119218); Zhu, Yuanda (57211216027); Naren, Tarun (57220119335); Isgut, Monica (57194205970); Sha, Ying (56763552300); Tong, Li (57188816402); Gupte, Mitali (57392544400); Wang, May D. (10739831700)","55427597100; 57220119218; 57211216027; 57220119335; 57194205970; 56763552300; 57188816402; 57392544400; 10739831700","Explainable Artificial Intelligence Methods in Combating Pandemics: A Systematic Review","2023","IEEE Reviews in Biomedical Engineering","16","","","5","21","16","26","10.1109/RBME.2022.3185953","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133767411&doi=10.1109%2fRBME.2022.3185953&partnerID=40&md5=b7fd325e1596b2fa6a51abe24ba3ec78","Despite the myriad peer-reviewed papers demonstrating novel Artificial Intelligence (AI)-based solutions to COVID-19 challenges during the pandemic, few have made a significant clinical impact, especially in diagnosis and disease precision staging. One major cause for such low impact is the lack of model transparency, significantly limiting the AI adoption in real clinical practice. To solve this problem, AI models need to be explained to users. Thus, we have conducted a comprehensive study of Explainable Artificial Intelligence (XAI) using PRISMA technology. Our findings suggest that XAI can improve model performance, instill trust in the users, and assist users in decision-making. In this systematic review, we introduce common XAI techniques and their utility with specific examples of their application. We discuss the evaluation of XAI results because it is an important step for maximizing the value of AI-based clinical decision support systems. Additionally, we present the traditional, modern, and advanced XAI models to demonstrate the evolution of novel techniques. Finally, we provide a best practice guideline that developers can refer to during the model experimentation. We also offer potential solutions with specific examples for common challenges in AI model experimentation. This comprehensive review, hopefully, can promote AI adoption in biomedicine and healthcare.  © 2008-2011 IEEE.","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85133767411"
"Szymanski M.; Verbert K.; Abeele V.V.","Szymanski, Maxwell (57223024617); Verbert, Katrien (13605498800); Abeele, Vero Vanden (23984073500)","57223024617; 13605498800; 23984073500","Designing and evaluating explainable AI for non-AI experts: challenges and opportunities","2022","RecSys 2022 - Proceedings of the 16th ACM Conference on Recommender Systems","","","","735","736","1","3","10.1145/3523227.3547427","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139553621&doi=10.1145%2f3523227.3547427&partnerID=40&md5=47b4540869e6796367dc95743c83526c","Artificial intelligence (AI) has seen a steady increase in use in the health and medical field, where it is used by lay users and health experts alike. However, these AI systems often lack transparency regarding the inputs and decision making process (often called black boxes), which in turn can be detrimental to the user's satisfaction and trust towards these systems. Explainable AI (XAI) aims to overcome this problem by opening up certain aspects of the black box, and has proven to be a successful means of increasing trust, transparency and even system effectiveness. However, for certain groups (i.e. lay users in health), explanation methods and evaluation metrics still remain underexplored. In this paper, we will outline our research regarding designing and evaluating explanations for health recommendations for lay users and domain experts, as well as list a few takeaways we were already able to find in our initial studies.  © 2022 Owner/Author.","Conference paper","Final","","Scopus","2-s2.0-85139553621"
"Basahel A.M.; Yamin M.","Basahel, Abdullah M. (26435660300); Yamin, Mohammad (6701534154)","26435660300; 6701534154","Quantum Inspired Differential Evolution with Explainable Artificial Intelligence-Based COVID-19 Detection","2023","Computer Systems Science and Engineering","46","1","","209","224","15","0","10.32604/csse.2023.034449","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147446461&doi=10.32604%2fcsse.2023.034449&partnerID=40&md5=1b82533acceecf0d026cdc09e8faa4bf","Recent advancements in the Internet of Things (Io), 5G networks, and cloud computing (CC) have led to the development of Human-centric IoT (HIoT) applications that transform human physical monitoring based on machine monitoring. The HIoT systems find use in several applications such as smart cities, healthcare, transportation, etc. Besides, the HIoT system and explainable artificial intelligence (XAI) tools can be deployed in the healthcare sector for effective decision-making. The COVID-19 pandemic has become a global health issue that necessitates automated and effective diagnostic tools to detect the disease at the initial stage. This article presents a new quantum-inspired differential evolution with explainable artificial intelligence based COVID-19 Detection and Classification (QIDEXAI-CDC) model for HIoT systems. The QIDEXAI-CDC model aims to identify the occurrence of COVID-19 using the XAI tools on HIoT systems. The QIDEXAI-CDC model primarily uses bilateral filtering (BF) as a preprocessing tool to eradicate the noise. In addition, RetinaNet is applied for the generation of useful feature vectors from radiological images. For COVID-19 detection and classification, quantum-inspired differential evolution (QIDE) with kernel extreme learning machine (KELM) model is utilized. The utilization of the QIDE algorithm helps to appropriately choose the weight and bias values of the KELM model. In order to report the enhanced COVID-19 detection outcomes of the QIDEXAI-CDC model, a wide range of simulations was carried out. Extensive comparative studies reported the supremacy of the QIDEXAI-CDC model over the recent approaches. © 2023 Authors. All rights reserved.","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85147446461"
"","","","21st International Conference on Artificial Intelligence in Medicine, AIME 2023","2023","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13897 LNAI","","","","","383","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163962413&partnerID=40&md5=0138af810fcca3f41792b84b08448a3b","The proceedings contain 47 papers. The special focus in this conference is on Artificial Intelligence in Medicine. The topics include: Management of Patient and Physician Preferences and Explanations for Participatory Evaluation of Treatment with an Ethical Seal; causal Discovery with Missing Data in a Multicentric Clinical Study; novel Approach for Phenotyping Based on Diverse Top-K Subgroup Lists; patient Event Sequences for Predicting Hospitalization Length of Stay; Autoencoder-Based Prediction of ICU Clinical Codes; Hospital Length of Stay Prediction Based on Multi-modal Data Towards Trustworthy Human-AI Collaboration in Radiomics; preface; survival Hierarchical Agglomerative Clustering: A Semi-Supervised Clustering Method Incorporating Survival Data; explainable Artificial Intelligence for Cytological Image Analysis; federated Learning to Improve Counterfactual Explanations for Sepsis Treatment Prediction; Explainable AI for Medical Event Prediction for Heart Failure Patients; adversarial Robustness and Feature Impact Analysis for Driver Drowsiness Detection; Computational Evaluation of Model-Agnostic Explainable AI Using Local Feature Importance in Healthcare; batch Integrated Gradients: Explanations for Temporal Electronic Health Records; improving Stroke Trace Classification Explainability Through Counterexamples; spatial Knowledge Transfer with Deep Adaptation Network for Predicting Hospital Readmission; dealing with Data Scarcity in Rare Diseases: Dynamic Bayesian Networks and Transfer Learning to Develop Prognostic Models of Amyotrophic Lateral Sclerosis; a Rule-Free Approach for Cardiological Registry Filling from Italian Clinical Notes with Question Answering Transformers; boosted Random Forests for Predicting Treatment Failure of Chemotherapy Regimens; classification of Fall Types in Parkinson's Disease from Self-report Data Using Natural Language Processing; BERT for Complex Systematic Review Screening to Support the Future of Medical Research; GGTWEAK: Gene Tagging with Weak Supervision for German Clinical Text; soft-Prompt Tuning to Predict Lung Cancer Using Primary Care Free-Text Dutch Medical Notes; machine Learning Models for Automatic Gene Ontology Annotation of Biological Texts; COVID-19 Diagnosis in 3D Chest CT Scans with Attention-Based Models.","Conference review","Final","","Scopus","2-s2.0-85163962413"
"Tešić M.; Hahn U.","Tešić, Marko (57195308697); Hahn, Ulrike (57205216629)","57195308697; 57205216629","Can counterfactual explanations of AI systems’ predictions skew lay users’ causal intuitions about the world? If so, can we correct for that?","2022","Patterns","3","12","100635","","","","3","10.1016/j.patter.2022.100635","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145782032&doi=10.1016%2fj.patter.2022.100635&partnerID=40&md5=f356227930d1425e3fb694633c775f6c","Counterfactual (CF) explanations have been employed as one of the modes of explainability in explainable artificial intelligence (AI)—both to increase the transparency of AI systems and to provide recourse. Cognitive science and psychology have pointed out that people regularly use CFs to express causal relationships. Most AI systems, however, are only able to capture associations or correlations in data, so interpreting them as casual would not be justified. In this perspective, we present two experiments (total n = 364) exploring the effects of CF explanations of AI systems’ predictions on lay people's causal beliefs about the real world. In Experiment 1, we found that providing CF explanations of an AI system's predictions does indeed (unjustifiably) affect people's causal beliefs regarding factors/features the AI uses and that people are more likely to view them as causal factors in the real world. Inspired by the literature on misinformation and health warning messaging, Experiment 2 tested whether we can correct for the unjustified change in causal beliefs. We found that pointing out that AI systems capture correlations and not necessarily causal relationships can attenuate the effects of CF explanations on people's causal beliefs. © 2022 The Author(s)","Review","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85145782032"
"Tharmakulasingam M.; Wang W.; Kerby M.; Ragione R.L.; Fernando A.","Tharmakulasingam, Mukunthan (57216134064); Wang, Wenwu (56100717600); Kerby, Michael (58496337100); Ragione, Roberto La (57214258681); Fernando, Anil (57957434000)","57216134064; 56100717600; 58496337100; 57214258681; 57957434000","TransAMR: An Interpretable Transformer Model for Accurate Prediction of Antimicrobial Resistance Using Antibiotic Administration Data","2023","IEEE Access","11","","","75337","75350","13","0","10.1109/ACCESS.2023.3296221","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165260176&doi=10.1109%2fACCESS.2023.3296221&partnerID=40&md5=a0267f04ec00d777bf4186678cd8b83e","Antimicrobial Resistance (AMR) is a growing public and veterinary health concern, and the ability to accurately predict AMR from antibiotics administration data is crucial for effectively treating and managing infections. While genomics-based approaches can provide better results, sequencing, assembling, and applying Machine Learning (ML) methods can take several hours. Therefore, alternative approaches are required. This study focused on using ML for antimicrobial stewardship by utilising data extracted from hospital electronic health records, which can be done in real-time, and developing an interpretable 1D-Transformer model for predicting AMR. A multi-baseline Integrated Gradient pipeline was also incorporated to interpret the model, and quantitative validation metrics were introduced to validate the model. The performance of the proposed 1D-Transformer model was evaluated using a dataset of urinary tract infection (UTI) patients with four antibiotics. The proposed 1D-Transformer model achieved 10% higher area under curve (AUC) in predicting AMR and outperformed traditional ML models. The Explainable Artificial Intelligence (XAI) pipeline also provided interpretable results, identifying the signatures contributing to the predictions. This could be used as a decision support tool for personalised treatment, introducing AMR-aware food and management of AMR, and it could also be used to identify signatures for targeted interventions.  © 2013 IEEE.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85165260176"
"Sharma M.; Lodhi H.; Yadav R.; Sampathila N.; Swathi K.S.; Acharya U.R.","Sharma, Manish (7403269252); Lodhi, Harsh (58171388700); Yadav, Rishita (58172175100); Sampathila, Niranjana (56584740000); Swathi, K.S. (58880972700); Acharya, U. Rajendra (7004510847)","7403269252; 58171388700; 58172175100; 56584740000; 58880972700; 7004510847","Automated Explainable Detection of Cyclic Alternating Pattern (CAP) Phases and Sub-Phases Using Wavelet-Based Single-Channel EEG Signals","2023","IEEE Access","11","","","50946","50961","15","5","10.1109/ACCESS.2023.3278800","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161024532&doi=10.1109%2fACCESS.2023.3278800&partnerID=40&md5=2a7a66f2822fb241b56a7e693d2d92c4","Sleep is a crucial component of health and well-being. It maintains the metabolism of the body and covers one-third of total life. The assessment of sleep quality is typically done by evaluating the macrostructure-based sleep stages, however, it does not take into account transient phenomena like K-complexes and transient fluctuations, which are crucial for the diagnosis of various sleep disorders. Cyclic alternating pattern (CAP) is a recurrent physiological electroencephalogram (EEG) activity that takes place in the brain during sleep and it is considered as a microstructure of sleep that can provide more accurate and relevant evaluation of sleep. The traditional way of CAP phase division is done manually by sleep specialists, which is sensitive, time-consuming, and prone to inaccuracies. Hence, there is a need for automated detection techniques that can solve the problems. This study proposes an automated, computerized approach for developing a machine learning model with explainable artificial intelligence (XAI) capabilities, using wavelet-based Hjorth parameters for classifying CAP A & B phases and phases A sub-phases (A1, A2, A3). The study utilizes SHAP (SHapley Additive exPlanations)-based feature ranking to provide insights into the model. This study uses the publicly accessible Physionet CAP sleep database. The model is developed using single-channel standardized EEG recordings from healthy subjects and patients with five types of sleep disorders, namely, insomnia, nocturnal frontal lobe epilepsy (NFLE), periodic leg movement disorder (PLM), rapid eye movement behavior disorder (RBD) and narcolepsy. The best performance is obtained using k-nearest neighbors (KNN) and ensemble bagged trees (EbagT) classifiers. The proposed model achieved a average classification accuracy of 91.6% for healthy subjects and 94.33%, 86.3%, 88.68%, 84.43%, and 88.5% for narcolepsy, RBD, PLM, NFLE, and insomnia subjects respectively, for classifying phases A and B. Our model achieved a average classification accuracy of 92.85% for healthy subjects and 93.9%, 84.9%, 88.0%, 80.92%, and 89.41% for narcolepsy, RBD, PLM, NFLE, and insomnia subjects, respectively while categorizing A subphases (A1, A2, A3). The proposed method may help sleep experts to examine a person's sleep quality automatically using the microstructure of sleep.  © 2013 IEEE.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85161024532"
"Karamanou A.; Kalampokis E.; Tarabanis K.","Karamanou, Areti (50261804900); Kalampokis, Evangelos (24766142200); Tarabanis, Konstantinos (6603842415)","50261804900; 24766142200; 6603842415","Linked Open Government Data to Predict and Explain House Prices: The Case of Scottish Statistics Portal","2022","Big Data Research","30","","100355","","","","15","10.1016/j.bdr.2022.100355","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140298765&doi=10.1016%2fj.bdr.2022.100355&partnerID=40&md5=5cee545da7b0d4b8d1c398ae57a29d5b","Accurately estimating the prices of houses is important for various stakeholders including house owners, real estate agencies, government agencies, and policy-makers. Towards this end, traditional statistics and, only recently, advanced machine learning and artificial intelligence models are used. Open Government Data (OGD) have a huge potential especially when combined with AI technologies. OGD are often published as linked data to facilitate data integration and re-usability. EXplainable Artificial Intelligence (XAI) can be used by stakeholders to understand the decisions of a predictive model. This work creates a model that predicts house prices by applying machine learning on linked OGD. We present a case study that uses XGBoost, a powerful machine learning algorithm, and linked OGD from the official Scottish data portal to predict the probability the mean prices of houses in the various data zones of Scotland to be higher than the average price in Scotland. XAI is also used to globally and locally explain the decisions of the model. The created model has Receiver Operating Characteristic (ROC) AUC score 0.923 and Precision Recall Curve (PRC) AUC score 0.891. According to XAI, the variable that mostly affects the decisions of the model is Comparative Illness Factor, an indicator of health conditions. However, local explainability shows that the decisions made in some data zones may be mostly affected by other variables such as the percent of detached dwellings and employment deprived population. © 2022 Elsevier Inc.","Article","Final","","Scopus","2-s2.0-85140298765"
"Qureshi R.; Irfan M.; Ali H.; Khan A.; Nittala A.S.; Ali S.; Shah A.; Gondal T.M.; Sadak F.; Shah Z.; Hadi M.U.; Khan S.; Al-Tashi Q.; Wu J.; Bermak A.; Alam T.","Qureshi, Rizwan (57193871773); Irfan, Muhammad (41561393200); Ali, Hazrat (56501336300); Khan, Arshad (55511990000); Nittala, Aditya Shekhar (56159493000); Ali, Shawkat (56936169800); Shah, Abbas (58247554000); Gondal, Taimoor Muzaffar (57208206380); Sadak, Ferhat (57204569726); Shah, Zubair (56428700200); Hadi, Muhammad Usman (57194410682); Khan, Sheheryar (57202720144); Al-Tashi, Qasem (57202329233); Wu, Jia (57191738625); Bermak, Amine (6701561781); Alam, Tanvir (56188785000)","57193871773; 41561393200; 56501336300; 55511990000; 56159493000; 56936169800; 58247554000; 57208206380; 57204569726; 56428700200; 57194410682; 57202720144; 57202329233; 57191738625; 6701561781; 56188785000","Artificial Intelligence and Biosensors in Healthcare and Its Clinical Relevance: A Review","2023","IEEE Access","11","","","61600","61620","20","12","10.1109/ACCESS.2023.3285596","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163149746&doi=10.1109%2fACCESS.2023.3285596&partnerID=40&md5=e9b5d3cf6b3131a40e255306aafc8d9f","Data generated from sources such as wearable sensors, medical imaging, personal health records, and public health organizations have resulted in a massive information increase in the medical sciences over the last decade. Advances in computational hardware, such as cloud computing, graphical processing units (GPUs), Field-programmable gate arrays (FPGAs) and tensor processing units (TPUs), provide the means to utilize these data. Consequently, an array of sophisticated Artificial Intelligence (AI) techniques have been devised to extract valuable insights from the extensive datasets in the healthcare industry. Here, we present an overview of recent progress in AI and biosensors in medical and life sciences. We discuss the role of machine learning in medical imaging, precision medicine, and biosensors for the Internet of Things (IoT). We review the latest advancements in wearable biosensing technologies. These innovative solutions employ AI to assist in monitoring of bodily electro-physiological and electro-chemical signals, as well as in disease diagnosis. These advancements exemplify the trend towards personalized medicine, delivering highly effective, cost-efficient, and precise point-of-care treatment.Furthermore, an overview of the advances in computing technologies, such as accelerated AI, edge computing, and federated learning for medical data, are also documented. Finally, we investigate challenges in data-driven AI approaches, the potential issues generated by biosensors and IoT-based healthcare, and the distribution shifts that occur among different data modalities, concluding with an overview of future prospects.  © 2013 IEEE.","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85163149746"
"Chen Z.S.; Kulkarni P.P.; Galatzer-Levy I.R.; Bigio B.; Nasca C.; Zhang Y.","Chen, Zhe Sage (57771110800); Kulkarni, Prathamesh (Param) (57201734211); Galatzer-Levy, Isaac R. (16303648200); Bigio, Benedetta (56024789800); Nasca, Carla (50162123400); Zhang, Yu (55977925700)","57771110800; 57201734211; 16303648200; 56024789800; 50162123400; 55977925700","Modern views of machine learning for precision psychiatry","2022","Patterns","3","11","100602","","","","25","10.1016/j.patter.2022.100602","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141482681&doi=10.1016%2fj.patter.2022.100602&partnerID=40&md5=861be0aa1aa9a34dbff1ba931cae2728","In light of the National Institute of Mental Health (NIMH)’s Research Domain Criteria (RDoC), the advent of functional neuroimaging, novel technologies and methods provide new opportunities to develop precise and personalized prognosis and diagnosis of mental disorders. Machine learning (ML) and artificial intelligence (AI) technologies are playing an increasingly critical role in the new era of precision psychiatry. Combining ML/AI with neuromodulation technologies can potentially provide explainable solutions in clinical practice and effective therapeutic treatment. Advanced wearable and mobile technologies also call for the new role of ML/AI for digital phenotyping in mobile mental health. In this review, we provide a comprehensive review of ML methodologies and applications by combining neuroimaging, neuromodulation, and advanced mobile technologies in psychiatry practice. We further review the role of ML in molecular phenotyping and cross-species biomarker identification in precision psychiatry. We also discuss explainable AI (XAI) and neuromodulation in a closed human-in-the-loop manner and highlight the ML potential in multi-media information extraction and multi-modal data fusion. Finally, we discuss conceptual and practical challenges in precision psychiatry and highlight ML opportunities in future research. © 2022 The Author(s)","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85141482681"
"Chaddad A.; Peng J.; Xu J.; Bouridane A.","Chaddad, Ahmad (53982608400); Peng, Jihao (58075430400); Xu, Jian (58075694500); Bouridane, Ahmed (6701348149)","53982608400; 58075430400; 58075694500; 6701348149","Survey of Explainable AI Techniques in Healthcare","2023","Sensors","23","2","634","","","","88","10.3390/s23020634","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146527613&doi=10.3390%2fs23020634&partnerID=40&md5=bc15de9fd726bf0a092858446adebdd0","Artificial intelligence (AI) with deep learning models has been widely applied in numerous domains, including medical imaging and healthcare tasks. In the medical field, any judgment or decision is fraught with risk. A doctor will carefully judge whether a patient is sick before forming a reasonable explanation based on the patient’s symptoms and/or an examination. Therefore, to be a viable and accepted tool, AI needs to mimic human judgment and interpretation skills. Specifically, explainable AI (XAI) aims to explain the information behind the black-box model of deep learning that reveals how the decisions are made. This paper provides a survey of the most recent XAI techniques used in healthcare and related medical imaging applications. We summarize and categorize the XAI types, and highlight the algorithms used to increase interpretability in medical imaging topics. In addition, we focus on the challenging XAI problems in medical applications and provide guidelines to develop better interpretations of deep learning models using XAI concepts in medical image and text analysis. Furthermore, this survey provides future directions to guide developers and researchers for future prospective investigations on clinical topics, particularly on applications with medical imaging. © 2023 by the authors.","Review","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85146527613"
"Thomas A.W.; Ré C.; Poldrack R.A.","Thomas, Armin W. (57208344290); Ré, Christopher (10739281400); Poldrack, Russell A. (7004739390)","57208344290; 10739281400; 7004739390","Interpreting mental state decoding with deep learning models","2022","Trends in Cognitive Sciences","26","11","","972","986","14","12","10.1016/j.tics.2022.07.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139590885&doi=10.1016%2fj.tics.2022.07.003&partnerID=40&md5=0de6c87cf5d2e6660e515837643acd69","In mental state decoding, researchers aim to identify the set of mental states (e.g., experiencing happiness or fear) that can be reliably identified from the activity patterns of a brain region (or network). Deep learning (DL) models are highly promising for mental state decoding because of their unmatched ability to learn versatile representations of complex data. However, their widespread application in mental state decoding is hindered by their lack of interpretability, difficulties in applying them to small datasets, and in ensuring their reproducibility and robustness. We recommend approaching these challenges by leveraging recent advances in explainable artificial intelligence (XAI) and transfer learning, and also provide recommendations on how to improve the reproducibility and robustness of DL models in mental state decoding. © 2022 Elsevier Ltd","Review","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85139590885"
"Tasnim N.; Mamun S.A.","Tasnim, Nusrat (57216868201); Mamun, Shamim Al (24824208800)","57216868201; 24824208800","Comparative Performance Analysis of Feature Selection for Mortality Prediction in ICU with Explainable Artificial Intelligence","2023","3rd International Conference on Electrical, Computer and Communication Engineering, ECCE 2023","","","","","","","1","10.1109/ECCE57851.2023.10101553","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158925523&doi=10.1109%2fECCE57851.2023.10101553&partnerID=40&md5=3670b3a47973efe408fe56c7d09e2224","The mortality prediction model in the Intensive Care Unit (ICU) can be a great tool for assisting physicians in decision-making for the optimal allocation of ICU according to the patient's health conditions. Traditional scoring-based systems for mortality prediction don't provide good predictive performance in the case of a large dataset. Moreover, machine learning models can also provide poor performance for the lack of proper feature selection. A comparison of the performance of machine learning models with and without feature selection was explored in this study. Principal Component Analysis (PCA) was used to choose features for this investigation. For the classification job, the most widely used and diversified classifiers from the literature were used, including Logistic Regression(LR), Decision Tree (DT), K Nearest Neighbours (KNN), and Support Vector Machine (SVM). The Medical Information Mart for Intensive Care III (MIMIC-III) dataset was used to collect data on heart failure patients. On the MIMIC-III dataset, it was discovered that feature selection significantly improved the performance of the described machine learning models. Without feature selection, the accuracy of LR, DT, KNN, and SVM models was 86.66%, 80.12%, 85.13%, and 86.49%, respectively, however with PCA, the accuracy was improved to 88.0%, 80.46%, 86.83%, and 87.34%, respectively with only 5 principal components. Finally, the model's decision-making process was analyzed with explainable artificial intelligence using Local Interpretable Model-agnostic Explanations (LIME). This analysis can help to understand the feature's contribution to the model's prediction process. It was also observed that the features involved in the prediction process were mostly common with the first 15 features found in feature importance hierarchy. © 2023 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85158925523"
"Mollaei N.; Fujao C.; Silva L.; Rodrigues J.; Cepeda C.; Gamboa H.","Mollaei, Nafiseh (57197755008); Fujao, Carlos (57216463630); Silva, Luis (57196057670); Rodrigues, Joao (57214371139); Cepeda, Catia (57189332012); Gamboa, Hugo (57200265948)","57197755008; 57216463630; 57196057670; 57214371139; 57189332012; 57200265948","Human-Centered Explainable Artificial Intelligence: Automotive Occupational Health Protection Profiles in Prevention Musculoskeletal Symptoms","2022","International Journal of Environmental Research and Public Health","19","15","9552","","","","7","10.3390/ijerph19159552","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136340814&doi=10.3390%2fijerph19159552&partnerID=40&md5=6b7ff81119893acdf76af67538810526","In automotive and industrial settings, occupational physicians are responsible for monitoring workers’ health protection profiles. Workers’ Functional Work Ability (FWA) status is used to create Occupational Health Protection Profiles (OHPP). This is a novel longitudinal study in comparison with previous research that has predominantly relied on the causality and explainability of human-understandable models for industrial technical teams like ergonomists. The application of artificial intelligence can support the decision-making to go from a worker’s Functional Work Ability to explanations by integrating explainability into medical (restriction) and support in contexts of individual, work-related, and organizational risk conditions. A sample of 7857 for the prognosis part of OHPP based on Functional Work Ability in the Portuguese language in the automotive industry was taken from 2019 to 2021. The most suitable regression models to predict the next medical appointment for the workers’ body parts protection were the models based on CatBoost regression, with an RMSLE of 0.84 and 1.23 weeks (mean error), respectively. CatBoost algorithm is also used to predict the next body part severity of OHPP. This information can help our understanding of potential risk factors for OHPP and identify warning signs of the early stages of musculoskeletal symptoms and work-related absenteeism. © 2022 by the authors.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85136340814"
"Iliadou E.; Su Q.; Kikidis D.; Bibas T.; Kloukinas C.","Iliadou, Eleftheria (57219273310); Su, Qiqi (57889791300); Kikidis, Dimitrios (25521861600); Bibas, Thanos (57300524100); Kloukinas, Christos (55913575000)","57219273310; 57889791300; 25521861600; 57300524100; 55913575000","Profiling hearing aid users through big data explainable artificial intelligence techniques","2022","Frontiers in Neurology","13","","933940","","","","5","10.3389/fneur.2022.933940","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137982746&doi=10.3389%2ffneur.2022.933940&partnerID=40&md5=15d5eeef19e6b18e1c63b092f3e0a968","Debilitating hearing loss (HL) affects ~6% of the human population. Only 20% of the people in need of a hearing assistive device will eventually seek and acquire one. The number of people that are satisfied with their Hearing Aids (HAids) and continue using them in the long term is even lower. Understanding the personal, behavioral, environmental, or other factors that correlate with the optimal HAid fitting and with users' experience of HAids is a significant step in improving patient satisfaction and quality of life, while reducing societal and financial burden. In SMART BEAR we are addressing this need by making use of the capacity of modern HAids to provide dynamic logging of their operation and by combining this information with a big amount of information about the medical, environmental, and social context of each HAid user. We are studying hearing rehabilitation through a 12-month continuous monitoring of HL patients, collecting data, such as participants' demographics, audiometric and medical data, their cognitive and mental status, their habits, and preferences, through a set of medical devices and wearables, as well as through face-to-face and remote clinical assessments and fitting/fine-tuning sessions. Descriptive, AI-based analysis and assessment of the relationships between heterogeneous data and HL-related parameters will help clinical researchers to better understand the overall health profiles of HL patients, and to identify patterns or relations that may be proven essential for future clinical trials. In addition, the future state and behavioral (e.g., HAids Satisfiability and HAids usage) of the patients will be predicted with time-dependent machine learning models to assist the clinical researchers to decide on the nature of the interventions. Explainable Artificial Intelligence (XAI) techniques will be leveraged to better understand the factors that play a significant role in the success of a hearing rehabilitation program, constructing patient profiles. This paper is a conceptual one aiming to describe the upcoming data collection process and proposed framework for providing a comprehensive profile for patients with HL in the context of EU-funded SMART BEAR project. Such patient profiles can be invaluable in HL treatment as they can help to identify the characteristics making patients more prone to drop out and stop using their HAids, using their HAids sufficiently long during the day, and being more satisfied by their HAids experience. They can also help decrease the number of needed remote sessions with their Audiologist for counseling, and/or HAids fine tuning, or the number of manual changes of HAids program (as indication of poor sound quality and bad adaptation of HAids configuration to patients' real needs and daily challenges), leading to reduced healthcare cost. Copyright © 2022 Iliadou, Su, Kikidis, Bibas and Kloukinas.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85137982746"
"Martínez-Agüero S.; Soguero-Ruiz C.; Alonso-Moral J.M.; Mora-Jiménez I.; Álvarez-Rodríguez J.; Marques A.G.","Martínez-Agüero, Sergio (57209537593); Soguero-Ruiz, Cristina (55207356700); Alonso-Moral, Jose M. (58258461100); Mora-Jiménez, Inmaculada (56039860600); Álvarez-Rodríguez, Joaquín (55710257300); Marques, Antonio G. (14067702400)","57209537593; 55207356700; 58258461100; 56039860600; 55710257300; 14067702400","Interpretable clinical time-series modeling with intelligent feature selection for early prediction of antimicrobial multidrug resistance","2022","Future Generation Computer Systems","133","","","68","83","15","13","10.1016/j.future.2022.02.021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126585690&doi=10.1016%2fj.future.2022.02.021&partnerID=40&md5=6188e1fe18aa49efc264a6506983a767","Electronic health records provide rich, heterogeneous data about the evolution of the patients’ health status. However, such data need to be processed carefully, with the aim of extracting meaningful information for clinical decision support. In this paper, we leverage interpretable (deep) learning and signal processing tools to deal with multivariate time-series data collected from the Intensive Care Unit (ICU) of the University Hospital of Fuenlabrada (Madrid, Spain). The presence of antimicrobial multidrug-resistant (AMR) bacteria is one of the greatest threats to the health system in general and to the ICUs in particular due to the critical health status of the patients therein. Thus, early identification of bacteria at the ICU and early prediction of their antibiotic resistance are key for the patients’ prognosis. While intelligent data-based processing and learning schemes can contribute to this early prediction, their acceptance and deployment in the ICUs require the automatic schemes to be not only accurate but also understandable by clinicians. Accordingly, we have designed trustworthy intelligent models for the early prediction of AMR based on the combination of meaningful feature selection with interpretable recurrent neural networks. These models were created using irregularly sampled clinical measurements, both considering the health status of the patient and the global ICU environment. We explored several strategies to cope with strongly imbalance data, since only a few ICU patients are infected by AMR bacteria. It is worth noting that our approach exhibits a good balance between performance and interpretability, especially when considering the difficulty of the classification task at hand. A multitude of factors are involved in the emergence of AMR (several of them not fully understood), and the records only contain a subset of them. In addition, the limited number of patients, the imbalance between classes, and the irregularity of the data render the problem harder to solve. Our models are also enriched with SHAP post-hoc interpretability and validated by clinicians who considered model understandability and trustworthiness of paramount concern for pragmatic purposes. Moreover, we use linguistic fuzzy systems to provide clinicians with explanations in natural language. Such explanations are automatically generated from a pool of interpretable rules that describe the interaction among the most relevant features identified by SHAP. Notice that clinicians were especially satisfied with new insights provided by our models. Such insights helped them to trust the automatic schemes and use them to make (better) decisions to mitigate AMR spreading in the ICU. All in all, this work paves the way towards more comprehensible time-series analysis in the context of early AMR prediction in ICUs and reduces the time of detection of infectious diseases, opening the door to better hospital care. © 2022 The Author(s)","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85126585690"
"Junaid M.A.; Anwar S.; Sikander G.; Khan M.T.","Junaid, Muhammad Asad (59054732100); Anwar, Shahzad (56970729700); Sikander, Gulbadan (56038589900); Khan, Muhammad Tahir (55845698500)","59054732100; 56970729700; 56038589900; 55845698500","Generative Adversarial Network based Chest Disease Detection and Binary Mask Generation","2023","2023 International Conference on Robotics and Automation in Industry, ICRAI 2023","","","","","","","0","10.1109/ICRAI57502.2023.10089542","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153570022&doi=10.1109%2fICRAI57502.2023.10089542&partnerID=40&md5=f390341c7518bd2aab8bb5994acf4ef2","The infectious disease tuberculosis (TB) continues to pose a serious risk to global health specially in developing world. There are over 10 million new cases of tuberculosis each year. Machine and deep learning models are trained to recognize specific pixels inside a medical image for the purposes of classification and disease progression tracking, however the decision-making mechanism of these models is hidden from the user. In this context, Explainable artificial intelligence (XAI) refers to strategies that allow humans to understand the results of AI algorithms. Recently, a variety of XAI methods for classification and generative have been proposed; however, these methods only use a subset of most discriminative power characteristics, resulting in false positives. This article proposes CycleGAN-based multi-functional generative adversarial networks to efficiently solve these challenges. Proposed model is trained in weakly supervised context to identify and visualize the disease effects and finally generate binary mask in data augmentation context. The model takes a Chest radiography (CXR) as input, creates a change map showing the disease's effect at a specific spot, and then uses this map to create a binary mask of original image. Results on publicly available TB dataset, TBX11K, confirm that the proposed model produces highly accurate result.  © 2023 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85153570022"
"Aalbers G.; Hendrickson A.T.; Abeele M.M.V.; Keijsers L.","Aalbers, George (57204862588); Hendrickson, Andrew T. (57197756572); Abeele, Mariek MP Vanden (37088078900); Keijsers, Loes (24341193100)","57204862588; 57197756572; 37088078900; 24341193100","Smartphone-Tracked Digital Markers of Momentary Subjective Stress in College Students: Idiographic Machine Learning Analysis","2023","JMIR mHealth and uHealth","11","","e37469","","","","3","10.2196/37469","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151043226&doi=10.2196%2f37469&partnerID=40&md5=0c0918f272cae5a9b276d1c6c0cc59a1","Background: Stress is an important predictor of mental health problems such as burnout and depression. Acute stress is considered adaptive, whereas chronic stress is viewed as detrimental to well-being. To aid in the early detection of chronic stress, machine learning models are increasingly trained to learn the quantitative relation from digital footprints to self-reported stress. Prior studies have investigated general principles in population-wide studies, but the extent to which the findings apply to individuals is understudied. Objective: We aimed to explore to what extent machine learning models can leverage features of smartphone app use log data to recognize momentary subjective stress in individuals, which of these features are most important for predicting stress and represent potential digital markers of stress, the nature of the relations between these digital markers and stress, and the degree to which these relations differ across people. Methods: Student participants (N=224) self-reported momentary subjective stress 5 times per day up to 60 days in total (44,381 observations); in parallel, dedicated smartphone software continuously logged their smartphone app use. We extracted features from the log data (eg, time spent on app categories such as messenger apps and proxies for sleep duration and onset) and trained machine learning models to predict momentary subjective stress from these features using 2 approaches: modeling general relations at the group level (nomothetic approach) and modeling relations for each person separately (idiographic approach). To identify potential digital markers of momentary subjective stress, we applied explainable artificial intelligence methodology (ie, Shapley additive explanations). We evaluated model accuracy on a person-to-person basis in out-of-sample observations. Results: We identified prolonged use of messenger and social network site apps and proxies for sleep duration and onset as the most important features across modeling approaches (nomothetic vs idiographic). The relations of these digital markers with momentary subjective stress differed from person to person, as did model accuracy. Sleep proxies, messenger, and social network use were heterogeneously related to stress (ie, negative in some and positive or zero in others). Model predictions correlated positively and statistically significantly with self-reported stress in most individuals (median person-specific correlation=0.15-0.19 for nomothetic models and median person-specific correlation=0.00-0.09 for idiographic models). Conclusions: Our findings indicate that smartphone log data can be used for identifying digital markers of stress and also show that the relation between specific digital markers and stress differs from person to person. These findings warrant follow-up studies in other populations (eg, professionals and clinical populations) and pave the way for similar research using physiological measures of stress. © 2023 JMIR Publications. All rights reserved.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85151043226"
"Park Y.; Kim Y.; Mun J.; Choi J.; Choi J.; Cho Y.","Park, Yohan (58123752300); Kim, Yongjin (58123692000); Mun, Jonghyeok (57208572962); Choi, Jongsun (55722464800); Choi, Jaeyoung (56812522400); Cho, Yongyun (8868551400)","58123752300; 58123692000; 57208572962; 55722464800; 56812522400; 8868551400","Exaggerated Advertisement Inspection System for Judging the Suitability of Advertisements in Social Media Environment","2023","International Conference on Information Networking","2023-January","","","778","781","3","0","10.1109/ICOIN56518.2023.10048933","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149171414&doi=10.1109%2fICOIN56518.2023.10048933&partnerID=40&md5=74008e028d21fb97799ab22d1ffa7ba7","Recently, as the social media markets are expanding, the amount of health functional food advertisements posted by individual users such as influencers and social media promoters is increasing. Therefore, users need a system that supports them to post false advertisements after inspecting them. In this paper, we propose an exaggerated advertisement inspection system that judges the suitable of advertisements and presents the grounds for disqualification. The proposed system consists of a module that classifies advertisements and explainable artificial intelligence(XAI). The system provides a rationale for judging the results of advertising classification and exaggerated advertisements. Therefore, the user may know why his or her writing is classified as exaggerated advertisement. The language model and embedding model, used in the exaggerated advertisement classification step, check the accuracy of the confusion matrix through the evaluation data. The XAI model checks performance by inputting data designated as exaggerated advertisement by health functional food-related institutions. © 2023 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85149171414"
"Ellis C.A.; Miller R.L.; Calhoun V.D.","Ellis, Charles A. (57213591885); Miller, Robyn L. (57226618089); Calhoun, Vince D. (57898536200)","57213591885; 57226618089; 57898536200","Towards greater neuroimaging classification transparency via the integration of explainability methods and confidence estimation approaches","2023","Informatics in Medicine Unlocked","37","","101176","","","","4","10.1016/j.imu.2023.101176","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146652419&doi=10.1016%2fj.imu.2023.101176&partnerID=40&md5=24949271ba2d0b6fc9650ad408c681de","The field of neuroimaging has increasingly sought to develop artificial intelligence-based models for neurological and neuropsychiatric disorder automated diagnosis and clinical decision support. However, if these models are to be implemented in a clinical setting, transparency will be vital. Two aspects of transparency are (1) confidence estimation and (2) explainability. Confidence estimation approaches indicate confidence in individual predictions. Explainability methods give insight into the importance of features to model predictions. In this study, we integrate confidence estimation and explainability approaches for the first time. We demonstrate their viability for schizophrenia diagnosis using resting state functional magnetic resonance imaging (rs-fMRI) dynamic functional network connectivity (dFNC) data. We compare two confidence estimation approaches: Monte Carlo dropout (MCD) and MC batch normalization (MCBN). We combine them with two gradient-based explainability approaches, saliency and layer-wise relevance propagation (LRP), and examine their effects upon explanations. We find that MCD often adversely affects model gradients, making it ill-suited for integration with gradient-based explainability methods. In contrast, MCBN does not affect model gradients. Additionally, we find many participant-level differences between regular explanations and the distributions of explanations for combined explainability and confidence estimation approaches. This suggests that a similar confidence estimation approach used in a clinical context with explanations only output for the regular model would likely not yield adequate explanations. We hope that our findings will provide a starting point for the integration of the two fields, provide useful guidance for future studies, and accelerate the development of transparent neuroimaging clinical decision support systems. © 2023 The Authors","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85146652419"
"Killian M.O.; Tian S.; Xing A.; Hughes D.; Gupta D.; Wang X.; He Z.","Killian, Michael O. (57191370542); Tian, Shubo (57226693180); Xing, Aiwen (57204558170); Hughes, Dana (57801241500); Gupta, Dipankar (56668263100); Wang, Xiaoyu (58609879000); He, Zhe (55320918000)","57191370542; 57226693180; 57204558170; 57801241500; 56668263100; 58609879000; 55320918000","Prediction of Outcomes After Heart Transplantation in Pediatric Patients Using National Registry Data: Evaluation of Machine Learning Approaches","2023","JMIR Cardio","7","","e45352","","","","0","10.2196/45352","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164453032&doi=10.2196%2f45352&partnerID=40&md5=cc9a0fa2bee58196bedbe749592fa074","Background: The prediction of posttransplant health outcomes for pediatric heart transplantation is critical for risk stratification and high-quality posttransplant care. Objective: The purpose of this study was to examine the use of machine learning (ML) models to predict rejection and mortality for pediatric heart transplant recipients. Methods: Various ML models were used to predict rejection and mortality at 1, 3, and 5 years after transplantation in pediatric heart transplant recipients using United Network for Organ Sharing data from 1987 to 2019. The variables used for predicting posttransplant outcomes included donor and recipient as well as medical and social factors. We evaluated 7 ML models-extreme gradient boosting (XGBoost), logistic regression, support vector machine, random forest (RF), stochastic gradient descent, multilayer perceptron, and adaptive boosting (AdaBoost)-as well as a deep learning model with 2 hidden layers with 100 neurons and a rectified linear unit (ReLU) activation function followed by batch normalization for each and a classification head with a softmax activation function. We used 10-fold cross-validation to evaluate model performance. Shapley additive explanations (SHAP) values were calculated to estimate the importance of each variable for prediction. Results: RF and AdaBoost models were the best-performing algorithms for different prediction windows across outcomes. RF outperformed other ML algorithms in predicting 5 of the 6 outcomes (area under the receiver operating characteristic curve [AUROC] 0.664 and 0.706 for 1-year and 3-year rejection, respectively, and AUROC 0.697, 0.758, and 0.763 for 1-year, 3-year, and 5-year mortality, respectively). AdaBoost achieved the best performance for prediction of 5-year rejection (AUROC 0.705). Conclusions: This study demonstrates the comparative utility of ML approaches for modeling posttransplant health outcomes using registry data. ML approaches can identify unique risk factors and their complex relationship with outcomes, thereby identifying patients considered to be at risk and informing the transplant community about the potential of these innovative approaches to improve pediatric care after heart transplantation. Future studies are required to translate the information derived from prediction models to optimize counseling, clinical care, and decision-making within pediatric organ transplant centers. © Michael O Killian, Shubo Tian, Aiwen Xing, Dana Hughes, Dipankar Gupta, Xiaoyu Wang, Zhe He. Originally published in JMIR Cardio (https://cardio.jmir.org), 20.06.2023.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85164453032"
"Islam M.S.; Awal M.A.; Laboni J.N.; Pinki F.T.; Karmokar S.; Mumenin K.M.; Al-Ahmadi S.; Rahman M.A.; Hossain M.S.; Mirjalili S.","Islam, Md Saiful (55367348900); Awal, Md. Abdul (59024320500); Laboni, Jinnaton Nessa (57724482500); Pinki, Farhana Tazmim (57203059308); Karmokar, Shatu (57724321000); Mumenin, Khondoker Mirazul (57439583300); Al-Ahmadi, Saad (56223622500); Rahman, Md. Ashfikur (57212494944); Hossain, Md. Shahadat (57221664954); Mirjalili, Seyedali (51461922300)","55367348900; 59024320500; 57724482500; 57203059308; 57724321000; 57439583300; 56223622500; 57212494944; 57221664954; 51461922300","HGSORF: Henry Gas Solubility Optimization-based Random Forest for C-Section prediction and XAI-based cause analysis","2022","Computers in Biology and Medicine","147","","105671","","","","20","10.1016/j.compbiomed.2022.105671","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131221843&doi=10.1016%2fj.compbiomed.2022.105671&partnerID=40&md5=af8131862b67b70fb463729d7e83e283","A stable predictive model is essential for forecasting the chances of cesarean or C-section (CS) delivery, as unnecessary CS delivery can adversely affect neonatal, maternal, and pediatric morbidity and mortality, and can incur significant financial burdens. Limited state-of-the-art machine learning models have been applied in this area in recent years, and the current models are insufficient to correctly predict the probability of CS delivery. To alleviate this drawback, we have proposed a Henry gas solubility optimization (HGSO)-based random forest (RF), with an improved objective function, called HGSORF, for the classification of CS and non-CS classes. Real-world CS datasets can be noisy, such as the Pakistan Demographic and Health Survey (PDHS) dataset used in this study. The HGSO can provide fine-tuned hyperparameters of RF by avoiding local minima points. To compare performance, Gaussian Naive Bayes (GNB), linear discriminant analysis (LDA), K-nearest neighbors (KNN), gradient boosting classifier (GBC), and logistic regression (LR) have been considered in this research. The ADAptive SYNthetic (ADASYN) algorithm has been used to balance the model, and the proposed HGSORF has been compared with other classifiers as well as with other studies. The superior performance was achieved by HGSORF with an accuracy of 98.33% for the PDHS dataset. The hyperparameters of RF have also been optimized by using commonly used hyperparameter-optimization algorithms, and the proposed HGSORF provided comparatively better performance. Additionally, to analyze the causes of CS and their significance, the HGSORF is explained locally and globally using eXplainable artificial intelligence (XAI)-based tools such as SHapely Additive exPlanation (SHAP) and Local Interpretable Model-Agnostic Explanations (LIME). A decision support system has been developed as a potential application to support clinical staffs. All pre-trained models and relevant codes are available on: https://github.com/MIrazul29/HGSORF_CSection. © 2022 Elsevier Ltd","Article","Final","","Scopus","2-s2.0-85131221843"
"Kibria H.B.; Nahiduzzaman M.; Goni M.O.F.; Ahsan M.; Haider J.","Kibria, Hafsa Binte (57222373843); Nahiduzzaman, Md (57216695486); Goni, Md. Omaer Faruq (57223025739); Ahsan, Mominul (56238324600); Haider, Julfikar (9938926300)","57222373843; 57216695486; 57223025739; 56238324600; 9938926300","An Ensemble Approach for the Prediction of Diabetes Mellitus Using a Soft Voting Classifier with an Explainable AI","2022","Sensors","22","19","7268","","","","24","10.3390/s22197268","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139984325&doi=10.3390%2fs22197268&partnerID=40&md5=fd6d556cdc8ee0759f7134c30758d21d","Diabetes is a chronic disease that continues to be a primary and worldwide health concern since the health of the entire population has been affected by it. Over the years, many academics have attempted to develop a reliable diabetes prediction model using machine learning (ML) algorithms. However, these research investigations have had a minimal impact on clinical practice as the current studies focus mainly on improving the performance of complicated ML models while ignoring their explainability to clinical situations. Therefore, the physicians find it difficult to understand these models and rarely trust them for clinical use. In this study, a carefully constructed, efficient, and interpretable diabetes detection method using an explainable AI has been proposed. The Pima Indian diabetes dataset was used, containing a total of 768 instances where 268 are diabetic, and 500 cases are non-diabetic with several diabetic attributes. Here, six machine learning algorithms (artificial neural network (ANN), random forest (RF), support vector machine (SVM), logistic regression (LR), AdaBoost, XGBoost) have been used along with an ensemble classifier to diagnose the diabetes disease. For each machine learning model, global and local explanations have been produced using the Shapley additive explanations (SHAP), which are represented in different types of graphs to help physicians in understanding the model predictions. The balanced accuracy of the developed weighted ensemble model was 90% with a F1 score of 89% using a five-fold cross-validation (CV). The median values were used for the imputation of the missing values and the synthetic minority oversampling technique (SMOTETomek) was used to balance the classes of the dataset. The proposed approach can improve the clinical understanding of a diabetes diagnosis and help in taking necessary action at the very early stages of the disease. © 2022 by the authors.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85139984325"
"Erdeniz S.P.; Schrempf M.; Kramer D.; Rainer P.P.; Felfernig A.; Tran T.; Burgstaller T.; Lubos S.","Erdeniz, Seda Polat (57195231828); Schrempf, Michael (57223707523); Kramer, Diether (57194330023); Rainer, Peter P. (35590576100); Felfernig, Alexander (6701498411); Tran, Trang (57195229497); Burgstaller, Tamim (58263130300); Lubos, Sebastian (57913057100)","57195231828; 57223707523; 57194330023; 35590576100; 6701498411; 57195229497; 58263130300; 57913057100","Computational Evaluation of Model-Agnostic Explainable AI Using Local Feature Importance in Healthcare","2023","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13897 LNAI","","","114","119","5","1","10.1007/978-3-031-34344-5_14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164017028&doi=10.1007%2f978-3-031-34344-5_14&partnerID=40&md5=7a883bf2465965a2ac6a644f50d52e39","Explainable artificial intelligence (XAI) is essential for enabling clinical users to get informed decision support from AI and comply with evidence-based medical practice. In the XAI field, effective evaluation methods are still being developed. The straightforward way is to evaluate via user feedback. However, this needs big efforts (applying on high number of users and test cases) and can still include various biases inside. A computational evaluation of explanation methods is also not easy since there is not yet a standard output of XAI models and the unsupervised learning behavior of XAI models. In this paper, we propose a computational evaluation method for XAI models which generate local feature importance as explanations. We use the output of XAI model (local feature importances) as features and the output of the prediction problem (labels) again as labels. We evaluate the method based a real-world tabular electronic health records dataset. At the end, we answer the research question: “How can we computationally evaluate XAI Models for a specific prediction model and dataset?”. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Conference paper","Final","","Scopus","2-s2.0-85164017028"
"Bharati S.; Mondal M.R.H.; Podder P.; Kose U.","Bharati, Subrato (57207728258); Mondal, M. Rubaiyat Hossain (57219750977); Podder, Prajoy (56607203500); Kose, Utku (36544118500)","57207728258; 57219750977; 56607203500; 36544118500","Explainable Artificial Intelligence (XAI) with IoHT for Smart Healthcare: A Review","2023","Internet of Things","Part F739","","","1","24","23","2","10.1007/978-3-031-08637-3_1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163810000&doi=10.1007%2f978-3-031-08637-3_1&partnerID=40&md5=e23e3f55fa12697e89229a872e60a624","Discussing the use of artificial intelligence (AI) in healthcare, explainability is a highly contentious topic. AI-powered systems may be superior at certain analytical tasks, but their lack of explanation continues to breed distrust. Because the majority of existing AI systems are incomprehensible and opaque, it is unlikely that AI technologies will be properly exploited and incorporated into standard clinical practice. We begin by discussing the present state of XAI development, with a focus on its applications in healthcare. Numerous IoHT-related linked health applications have been examined in XAI to establish their privacy, security, and explainability effectiveness. If we employ clinical decision assistance systems (CDAS) based on artificial intelligence, our approach will combine legal, technological, patient, and medical considerations. To gain a better grasp of the significance of explainability in clinical practice, several disciplines focus on distinct fundamental concerns and values. Explainability must be technically appraised in terms of how it could be attained and what it entails for future development. Important legal checkpoints for explainability include informed consent, certification, and licensing for medical equipment. It is important to look at the relationship between medical AI and people from both the patient’s and the doctor’s points of view. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Book chapter","Final","","Scopus","2-s2.0-85163810000"
"Buyuktepe O.; Catal C.; Kar G.; Bouzembrak Y.; Marvin H.; Gavai A.","Buyuktepe, Okan (58347656800); Catal, Cagatay (22633325800); Kar, Gorkem (56417439300); Bouzembrak, Yamine (44860959400); Marvin, Hans (6701404990); Gavai, Anand (35366319900)","58347656800; 22633325800; 56417439300; 44860959400; 6701404990; 35366319900","Food fraud detection using explainable artificial intelligence","2023","Expert Systems","","","","","","","9","10.1111/exsy.13387","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162857263&doi=10.1111%2fexsy.13387&partnerID=40&md5=0f9a117037ecb4e1e75d6b53ee3e0c73","Recently, the global food supply chain has become increasingly complex, and its scalability has grown. From farm to fork, the performance of food-producing systems is influenced by significant changes in the environment, population and economy. These changes may cause an increase in food fraud and safety hazards and hence, harm human health. Adopting artificial intelligence (AI) technology in the food supply chain is one strategy to reduce these hazards. Although the use of AI has been rising in numerous industries, such as precision nutrition, self-driving cars, precision agriculture, precision medicine and food safety, much of what AI systems do is a black box due to its poor explainability. This study covers numerous use cases of food fraud risk prediction using explainable artificial intelligence (XAI) techniques, such as LIME, SHAP and WIT. We aimed to interpret the predictions of a machine learning model with the aid of these technologies. The case study was performed on a food fraud dataset using adulteration/fraud notifications retrieved from the Rapid Alert System for Food and Feed system and economically motivated adulteration database. A deep learning model was built based on this dataset and XAI tools have been investigated on the proposed deep learning model. Both features and shortcomings of the current XAI tools in the food fraud area have been presented. © 2023 The Authors. Expert Systems published by John Wiley & Sons Ltd.","Article","Article in press","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85162857263"
"Wójcik P.; Andruszek K.","Wójcik, Piotr (7003830235); Andruszek, Krystian (57297367900)","7003830235; 57297367900","Predicting intra-urban well-being from space with nonlinear machine learning","2022","Regional Science Policy and Practice","14","4","","891","913","22","5","10.1111/rsp3.12478","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117189179&doi=10.1111%2frsp3.12478&partnerID=40&md5=a9c31affe98e983f76f1bdfa4076c55c","There is a growing need to analyze welfare at an intra-urban level because cities often evince stark divisions. It is therefore important to identify inequalities within them. However, data are hardly available – or very expensive. The purpose of this article is to test whether nonlinear machine learning algorithms provide more accurate predictions of intra-city well-being than linear models. In addition, we aim to check if freely available and easily accessible data from Open Street Map offer an alternative to high-resolution daytime satellite images from Google Maps in accurately predicting well-being on a local level. Inspired by the Local Human Development Index, we construct a well-being index based on three dimensions: health, education, and welfare. Potential predictors of well-being include indicators related to the urbanization rate, access to natural amenities, the transportation system, and access to public transport. Four nonlinear machine learning algorithms (support vector regression with polynomial and radial kernel, random forest, and xgboost) are compared with the linear LASSO approach for the 18 districts of Warsaw, Poland. In addition, we apply innovative tools of explainable artificial intelligence (XAI) to identify the most important predictors of well-being (measuring model-agnostic feature importance) and to disclose the shape of relationships between well-being and its most important predictors. We conclude that the application of nonlinear machine learning algorithms to modeling well-being not only allows us to reach higher predictive accuracy, but also to better identify and explain the impact of its predictors. © 2021 The Authors. Regional Science Policy & Practice © 2021 Regional Science Association International.","Article","Final","","Scopus","2-s2.0-85117189179"
"Chamola V.; Hassija V.; Sulthana A.R.; Ghosh D.; Dhingra D.; Sikdar B.","Chamola, Vinay (55427784900); Hassija, Vikas (57209808751); Sulthana, A Razia (58497994700); Ghosh, Debshishu (57952252700); Dhingra, Divyansh (58498194700); Sikdar, Biplab (7003523486)","55427784900; 57209808751; 58497994700; 57952252700; 58498194700; 7003523486","A Review of Trustworthy and Explainable Artificial Intelligence (XAI)","2023","IEEE Access","11","","","78994","79015","21","10","10.1109/ACCESS.2023.3294569","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165321235&doi=10.1109%2fACCESS.2023.3294569&partnerID=40&md5=cc43b4117a1bdd1ef33be47c2f960dd8","The advancement of Artificial Intelligence (AI) technology has accelerated the development of several systems that are elicited from it. This boom has made the systems vulnerable to security attacks and allows considerable bias in order to handle errors in the system. This puts humans at risk and leaves machines, robots, and data defenseless. Trustworthy AI (TAI) guarantees human value and the environment. In this paper, we present a comprehensive review of the state-of-the-art on how to build a Trustworthy and eXplainable AI, taking into account that AI is a black box with little insight into its underlying structure. The paper also discusses various TAI components, their corresponding bias, and inclinations that make the system unreliable. The study also discusses the necessity for TAI in many verticals, including banking, healthcare, autonomous system, and IoT. We unite the ways of building trust in all fragmented areas of data protection, pricing, expense, reliability, assurance, and decision-making processes utilizing TAI in several diverse industries and to differing degrees. It also emphasizes the importance of transparent and post hoc explanation models in the construction of an eXplainable AI and lists the potential drawbacks and pitfalls of building eXplainable AI. Finally, the policies for developing TAI in the autonomous vehicle construction sectors are thoroughly examined and eclectic ways of building a reliable, interpretable, eXplainable, and Trustworthy AI systems are explained to guarantee safe autonomous vehicle systems.  © 2013 IEEE.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85165321235"
"","","","23rd International Conference on Computational Science and Its Applications , ICCSA 2023","2023","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13957 LNCS","","","","","1296","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165094352&partnerID=40&md5=67f36137be155372094a6801b37fcca9","The proceedings contain 86 papers. The special focus in this conference is on Computational Science and Its Applications. The topics include: A Preliminary Result of Implementing a Deep Learning-Based Earthquake Early Warning System in Italy; structural Node Representation Learning for Detecting Botnet Nodes; a Machine Learning Methodology for Optimal Big Data Processing in Advanced Smart City Environments; from Selecting Best Algorithm to Explaining Why It is: A General Review, Formal Problem Statement and Guidelines Towards to an Empirical Generalization; a Framework to Assist Instructors Help Novice Programmers to Better Comprehend Source Code ─ A Decoding Perspective; interactive Information Visualization Models: A Systematic Literature Review; comparative Analysis of Community Detection and Transformer-Based Approaches for Topic Clustering of Scientific Papers; integrating Counterfactual Evaluations into Traditional Interactive Recommendation Frameworks; eXplainable Artificial Intelligence - A Study of Sentiments About Vaccination in Brazil; design and Implementation of Wind-Powered Charging System to Improve Electric Motorcycle Autonomy; knowledge Management Model: A Process View; A Novel Natural Language Processing Strategy to Improve Digital Accounting Classification Approach for Supplier Invoices ERP Transaction Process; implementation of eXplainable Artificial Intelligence: Case Study on the Assessment of Movements to Support Neuromotor Rehabilitation; automatic Features Extraction from the Optic Cup and Disc Segmentation for Glaucoma Classification; artificial Bee Colony Algorithm for Feature Selection in Fraud Detection Process; convolutional Neural Networks and Ensembles for Visually Impaired Aid; a Software Architecture Based on the Blockchain-Database Hybrid for Electronic Health Records; bibliometric Analysis of Robotic Process Automation Domain: Key Topics, Challenges and Solutions; Enhancing Amazigh Speech Recognition System with MFDWC-SVM; FastCELF++: A Novel and Fast Heuristic for Influence Maximization in Complex Networks; predicting Multiple Domain Queue Waiting Time via Machine Learning.","Conference review","Final","","Scopus","2-s2.0-85165094352"
"Sinha A.; Garcia D.W.; Kumar B.; Banerjee P.","Sinha, Anurag (57561268800); Garcia, Den Whilrex (57213360908); Kumar, Biresh (57684512200); Banerjee, Pallab (57489653200)","57561268800; 57213360908; 57684512200; 57489653200","Application of Big Data Analytics and Internet of Medical Things (IoMT) in Healthcare with View of Explainable Artificial Intelligence: A Survey","2023","Internet of Things","Part F739","","","129","163","34","5","10.1007/978-3-031-08637-3_8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163826895&doi=10.1007%2f978-3-031-08637-3_8&partnerID=40&md5=a91f906a626fa335dd7ae6f384279e65","The integration of Big Data Analytics (BDA) and the Internet of Medical Things (IoMT) has brought a significant transformation in the healthcare industry. The emergence of Explainable Artificial Intelligence (XAI) has further revolutionized the healthcare sector by providing insights into complex machine learning models. This survey aims to explore the application of BDA and IoMT in healthcare with a view on XAI. The survey highlights the benefits of BDA and IoMT in healthcare, such as improved patient outcomes, reduced healthcare costs, and enhanced personalized medicine. It also discusses the challenges associated with the use of BDA and IoMT, including data privacy, security, and regulatory compliance. The survey provides an overview of the latest research and development in the field of XAI, with particular focus on its application in healthcare. Furthermore, the survey presents a detailed analysis of the existing literature on the integration of BDA, IoMT, and XAI in healthcare. It discusses the various applications of BDA, IoMT, and XAI in healthcare, such as medical imaging, drug discovery, diagnosis, and treatment planning. The survey also highlights the potential benefits of XAI in healthcare, including transparency, interpretability, and fairness. Finally, the survey concludes by discussing the future research directions in the field of BDA, IoMT, and XAI in healthcare. It emphasizes the need for ethical guidelines and best practices for the responsible use of BDA, IoMT, and XAI in healthcare to ensure patient safety and privacy. The survey provides valuable insights into the integration of BDA, IoMT, and XAI in healthcare and their potential to revolutionize the healthcare industry. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Book chapter","Final","","Scopus","2-s2.0-85163826895"
"de Camargo L.F.; da Costa Feitosa J.; Bonatti E.; Simioni G.B.; Brega J.R.F.","de Camargo, Luiz Felipe (57219490684); da Costa Feitosa, Juliana (58089461300); Bonatti, Eloísa (58490450800); Simioni, Giovana Ballminut (58489943900); Brega, José Remo Ferreira (6507059897)","57219490684; 58089461300; 58490450800; 58489943900; 6507059897","eXplainable Artificial Intelligence - A Study of Sentiments About Vaccination in Brazil","2023","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13956 LNCS","","","617","634","17","0","10.1007/978-3-031-36805-9_40","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164954947&doi=10.1007%2f978-3-031-36805-9_40&partnerID=40&md5=ba7b63d04c331d25bd94e88c3c19f3bf","Sentiment analysis in social networks is a focus in several studies on Machine Learning, this happens because the scope and speed with which opinions and emotions about events, controversial issues and products and services are treated on the Internet make it attractive to analyze this medium to obtain relevant information and of interest. Based on this context, this paper presents a sentiment analysis on social networks, focusing on Twitter, about the COVID-19 vaccination campaign in Brazil, using Machine Learning techniques, more specifically, logistic regression, and subsequently the eXplainable Artificial Intelligence (XAI) with the methods LIME, SHAP and Eli5 to interpret the model output. Although there are several applications in the field of sentiment analysis, this study focuses on using real Twitter data, extracted according to the desired context, for five months, processing, analyzing and preparing them for training, and on the explainability of the results obtained during the analysis. The results obtained show that the sample population was mostly in favor of vaccination for issues such as health and the collective good of the population, while those who were against wondered about compulsion and the power of freedom of choice, and expressed fear of being part of an experiment, given the design time of vaccine development. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Conference paper","Final","","Scopus","2-s2.0-85164954947"
"Mutegeki H.; Nahabwe A.; Nakatumba-Nabende J.; Marvin G.","Mutegeki, Henry (58304249200); Nahabwe, Alvin (58304249300); Nakatumba-Nabende, Joyce (57197759951); Marvin, Ggaliwango (57302525500)","58304249200; 58304249300; 57197759951; 57302525500","Interpretable Machine Learning-Based Triage For Decision Support in Emergency Care","2023","7th International Conference on Trends in Electronics and Informatics, ICOEI 2023 - Proceedings","","","","983","990","7","1","10.1109/ICOEI56765.2023.10125918","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161207194&doi=10.1109%2fICOEI56765.2023.10125918&partnerID=40&md5=6ff1b390294b763b1e41525b5c371241","In medicine, triage is a process that rations health services towards patients who are most in need of immediate care. Traditionally, a nurse will carry out an evaluation to ascertain a patient's condition and assign an Emergency Severity Index(ESI) to represent the urgency level needed to attend to them, which is time-consuming and subject to human error and bias. Mistriage could potentially lead to delayed medical attendance and ultimately to death if the condition is not managed, while no triage could overwhelm the finite hospital resources and medical personnel in trying to attend to the huge patient volumes. Existing research in the domain largely focuses on the prediction of patient disposition(whether or not one will be admitted). This work proposes using Explainable AI to generate insights that can support human-in-the-loop Machine Learning triage of patients at the emergency department level. This research utilized Machine learning classification algorithms like the Decision Trees classifier, Random forest classifier, Histogram-Based Gradient Boosting classifier, XGBoost classifier, and ELM Classifier to predict the Emergency Severity Index of a patient based on their medical data. Upon hyperparameter tuning, the models are evaluated and the Histogram-Based Gradient Boosting classifier topped them all as the best-performing model with an average Area Under the Curve(AUC) score of 91 % over the 5 classes and an F1 score of 70%. Explainable AI(XAI) techniques like Local Interpretable Model-Agnostic Explanations(LIME) and Shapley Additive Explanations(SHAP) were used to create more transparent and trustworthy algorithms for Intelligent Healthcare.  © 2023 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85161207194"
"Bunde E.; Eisenhardt D.; Sonntag D.; Profitlich H.-J.; Meske C.","Bunde, Enrico (57218312382); Eisenhardt, Daniel (58304928500); Sonntag, Daniel (12241487800); Profitlich, Hans-Jürgen (6506157925); Meske, Christian (55806873000)","57218312382; 58304928500; 12241487800; 6506157925; 55806873000","Giving DIAnA More TIME – Guidance for the Design of XAI-Based Medical Decision Support Systems","2023","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13873 LNCS","","","107","122","15","0","10.1007/978-3-031-32808-4_7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161153089&doi=10.1007%2f978-3-031-32808-4_7&partnerID=40&md5=05876ce90d402f71972f3a7e34216add","Future healthcare ecosystems integrating human-centered artificial intelligence (AI) will be indispensable. AI-based healthcare technologies can support diagnosis processes and make healthcare more accessible globally. In this context, we conducted a design science research project intending to introduce design principles for user interfaces (UIs) of explainable AI-based (XAI) medical decision support systems (XAI-based MDSS). We used an archaeological approach to analyze the UI of an existing web-based system in the context of skin lesion classification called DIAnA (Dermatological Images – Analysis and Archiving). One of DIAnA’s unique characteristics is that it should be usable for the stakeholder groups of physicians and patients. We conducted the in-situ analysis with these stakeholders using the think-aloud method and semi-structured interviews. We anchored our interview guide in concepts of the Theory of Interactive Media Effects (TIME), which formulates UI features as causes and user psychology as effects. Based on the results, we derived 20 design requirements and developed nine design principles grounded in TIME for this class of XAI-based MDSS, either associated with the needs of physicians, patients, or both. Regarding evaluation, we first conducted semi-structured interviews with software developers to assess the reusability of our design principles. Afterward, we conducted a survey with user experience/interface designers. The evaluation uncovered that 77% of the participants would adopt the design principles, and 82% would recommend them to colleagues for a suitable project. The findings prove the reusability of the design principles and highlight a positive perception by potential implementers. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Conference paper","Final","","Scopus","2-s2.0-85161153089"
"Xiao G.; Pfaff E.; Prud'hommeaux E.; Booth D.; Sharma D.K.; Huo N.; Yu Y.; Zong N.; Ruddy K.J.; Chute C.G.; Jiang G.","Xiao, Guohui (57205067739); Pfaff, Emily (55922283700); Prud'hommeaux, Eric (55978154500); Booth, David (57196803611); Sharma, Deepak K. (58827182800); Huo, Nan (58588632500); Yu, Yue (57211907673); Zong, Nansu (55205153900); Ruddy, Kathryn J. (22951663300); Chute, Christopher G. (7006581202); Jiang, Guoqian (55486706700)","57205067739; 55922283700; 55978154500; 57196803611; 58827182800; 58588632500; 57211907673; 55205153900; 22951663300; 7006581202; 55486706700","FHIR-Ontop-OMOP: Building clinical knowledge graphs in FHIR RDF with the OMOP Common data Model","2022","Journal of Biomedical Informatics","134","","104201","","","","5","10.1016/j.jbi.2022.104201","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138452576&doi=10.1016%2fj.jbi.2022.104201&partnerID=40&md5=f472d69b506336ca3d517cd74ebcbc6b","Background: Knowledge graphs (KGs) play a key role to enable explainable artificial intelligence (AI) applications in healthcare. Constructing clinical knowledge graphs (CKGs) against heterogeneous electronic health records (EHRs) has been desired by the research and healthcare AI communities. From the standardization perspective, community-based standards such as the Fast Healthcare Interoperability Resources (FHIR) and the Observational Medical Outcomes Partnership (OMOP) Common Data Model (CDM) are increasingly used to represent and standardize EHR data for clinical data analytics, however, the potential of such a standard on building CKG has not been well investigated. Objective: To develop and evaluate methods and tools that expose the OMOP CDM-based clinical data repositories into virtual clinical KGs that are compliant with FHIR Resource Description Framework (RDF) specification. Methods: We developed a system called FHIR-Ontop-OMOP to generate virtual clinical KGs from the OMOP relational databases. We leveraged an OMOP CDM-based Medical Information Mart for Intensive Care (MIMIC-III) data repository to evaluate the FHIR-Ontop-OMOP system in terms of the faithfulness of data transformation and the conformance of the generated CKGs to the FHIR RDF specification. Results: A beta version of the system has been released. A total of more than 100 data element mappings from 11 OMOP CDM clinical data, health system and vocabulary tables were implemented in the system, covering 11 FHIR resources. The generated virtual CKG from MIMIC-III contains 46,520 instances of FHIR Patient, 716,595 instances of Condition, 1,063,525 instances of Procedure, 24,934,751 instances of MedicationStatement, 365,181,104 instances of Observations, and 4,779,672 instances of CodeableConcept. Patient counts identified by five pairs of SQL (over the MIMIC database) and SPARQL (over the virtual CKG) queries were identical, ensuring the faithfulness of the data transformation. Generated CKG in RDF triples for 100 patients were fully conformant with the FHIR RDF specification. Conclusion: The FHIR-Ontop-OMOP system can expose OMOP database as a FHIR-compliant RDF graph. It provides a meaningful use case demonstrating the potentials that can be enabled by the interoperability between FHIR and OMOP CDM. Generated clinical KGs in FHIR RDF provide a semantic foundation to enable explainable AI applications in healthcare. © 2022 The Authors","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85138452576"
"S Band S.; Yarahmadi A.; Hsu C.-C.; Biyari M.; Sookhak M.; Ameri R.; Dehzangi I.; Chronopoulos A.T.; Liang H.-W.","S Band, Shahab (57221738247); Yarahmadi, Atefeh (57792578500); Hsu, Chung-Chian (12778673800); Biyari, Meghdad (57891501900); Sookhak, Mehdi (55520828400); Ameri, Rasoul (57188865781); Dehzangi, Iman (23396537700); Chronopoulos, Anthony Theodore (23126220800); Liang, Huey-Wen (56499730200)","57221738247; 57792578500; 12778673800; 57891501900; 55520828400; 57188865781; 23396537700; 23126220800; 56499730200","Application of explainable artificial intelligence in medical health: A systematic review of interpretability methods","2023","Informatics in Medicine Unlocked","40","","101286","","","","14","10.1016/j.imu.2023.101286","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161707943&doi=10.1016%2fj.imu.2023.101286&partnerID=40&md5=df977156749eb13b755566c5ecc5c0c1","This paper investigates the applications of explainable AI (XAI) in healthcare, which aims to provide transparency, fairness, accuracy, generality, and comprehensibility to the results obtained from AI and ML algorithms in decision-making systems. The black box nature of AI and ML systems has remained a challenge in healthcare, and interpretable AI and ML techniques can potentially address this issue. Here we critically review previous studies related to the interpretability of ML and AI methods in medical systems. Descriptions of various types of XAI methods such as layer-wise relevance propagation (LRP), Uniform Manifold Approximation and Projection (UMAP), Local Interpretable Model-agnostic Explanations (LIME), SHapley Additive exPlanations (SHAP), ANCHOR, contextual importance and utility (CIU), Training calibration-based explainers (TraCE), Gradient-weighted Class Activation Mapping (Grad-CAM), t-distributed Stochastic Neighbor Embedding (t-SNE), NeuroXAI, Explainable Cumulative Fuzzy Class Membership Criterion (X-CFCMC) along with the diseases which can be explained through these methods are provided throughout the paper. The paper also discusses how AI and ML technologies can transform healthcare services. The usability and reliability of the presented methods are summarized, including studies on the usability and reliability of XGBoost for mediastinal cysts and tumors, a 3D brain tumor segmentation network, and the TraCE method for medical image analysis. Overall, this paper aims to contribute to the growing field of XAI in healthcare and provide insights for researchers, practitioners, and decision-makers in the healthcare industry. Finally, we discuss the performance of XAI methods applied in medical health care systems. It is also needed to mention that a brief implemented method is provided in the methodology section. © 2023","Article","Final","","Scopus","2-s2.0-85161707943"
"Hema Rajini N.; Anton Smith A.","Hema Rajini, N. (55580316400); Anton Smith, A. (35315513000)","55580316400; 35315513000","Osteoarthritis Detection and Classification in Knee X-Ray Images Using Particle Swarm Optimization with Deep Neural Network","2023","Internet of Things","Part F739","","","91","101","10","0","10.1007/978-3-031-08637-3_5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163831644&doi=10.1007%2f978-3-031-08637-3_5&partnerID=40&md5=18e4f4554c38ec38cedd34b1a823d673","Explainable artificial intelligence (XAI) involves a collection of processes and approaches which enables human users to comprehend and trust the results and output produced by machine learning (ML) approaches. XAI is employed for describing the AI model, its expected impact, and potential biases. At the same time, Internet of Healthcare Things (IoHT) has become a hot research topic in the healthcare sector which assist in the disease diagnostic process. Presently, an efficient computer-aided diagnosis (CAD) model is needed for diagnosing osteoarthritis (OA). This study designs a new particle swarm optimization (PSO) model with deep neural network (DNN), named PSO-DNN technique, for the identification and categorization of osteoarthritis from the knee X-ray images in an IoHT environment. The presented method helps to distinguish between well and diseased knee X-ray images. Here, a guided filter (GF) and adaptive histogram equalization models are correspondingly employed to remove noises and enhance the images. Global thresholding-based segmentation model is employed for extracting the synovial cavity regions from the image, and curvature values are determined. For drawing a good validation, the experimentation takes place on the real-time patient-oriented images gathered from the medical organizations. From the simulation outcome, the presented PSO-DNN model confirmed the superior performance of the applied images. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Book chapter","Final","","Scopus","2-s2.0-85163831644"
"Keleko A.T.; Kamsu-Foguem B.; Ngouna R.H.; Tongne A.","Keleko, Aurelien Teguede (57965332100); Kamsu-Foguem, Bernard (8046606700); Ngouna, Raymond Houe (21934208700); Tongne, Amèvi (56541770600)","57965332100; 8046606700; 21934208700; 56541770600","Health condition monitoring of a complex hydraulic system using Deep Neural Network and DeepSHAP explainable XAI","2023","Advances in Engineering Software","175","","103339","","","","24","10.1016/j.advengsoft.2022.103339","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141891683&doi=10.1016%2fj.advengsoft.2022.103339&partnerID=40&md5=262b08a1a823d150c4f4fd83ae02061a","This paper presents a detailed framework for Condition Monitoring (CM) based on hydraulic systems and multi-sensor data. Nowadays, the CM technique is increasingly deployed for the optimization of quality and manufacturing processes. It is used as a decision-making support tool in maintenance operations or activities. In this environment, the diagnosis, prognosis, or monitoring of interconnected machines has become a crucial issue for improving the cost-effectiveness of manufacturing industries. Some models are available to monitor or predict the degradation of elements within a hydraulic system such as coolers, valves, internal pump leakage, or the condition of the hydraulic accumulator. In this case, we have focused on a data-driven approach, concentrating on the Deep Neural Networks (DNN) multi-class classification for imbalanced data that are adapted to predict the real operating states of the system. Despite their performance, questions remain concerning the reliability of the DNN as “black box” models when used in complex applications, notably regarding the decision-making processes and the possible ethical, socioeconomic, and transparency impacts upon stakeholders. Regarding the explanation approach, we have exploited the Deep SHapley Additive exPlanations (DeepSHAP) methodologies to provide trustworthy results, and to explain the importance (weight) or role that each sensor plays and its contribution to the DNN algorithm's decision-making. The obtained framework based on two principal modules illustrates that the DNN classifier model when evaluated by Accuracy, F1-Score, Recall, and Precision metrics are robust and perform efficiently. Finally, using the DeepSHAP technique provides an explanation of the results of the developed model, and helps humans to understand, interpret and trust the model, with an associated increase in the support or the stimulation of Artificial Intelligence (AI) models applications on large-scale problems including industrial sectors. © 2022 Elsevier Ltd","Review","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85141891683"
"Lu H.; Uddin S.","Lu, Haohui (57224405646); Uddin, Shahadat (57194632476)","57224405646; 57194632476","Explainable Stacking-Based Model for Predicting Hospital Readmission for Diabetic Patients","2022","Information (Switzerland)","13","9","436","","","","3","10.3390/info13090436","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138742824&doi=10.3390%2finfo13090436&partnerID=40&md5=281cbca86478ef70a9d6b9c2c0a6ca9c","Artificial intelligence is changing the practice of healthcare. While it is essential to employ such solutions, making them transparent to medical experts is more critical. Most of the previous work presented disease prediction models, but did not explain them. Many healthcare stakeholders do not have a solid foundation in these models. Treating these models as ‘black box’ diminishes confidence in their predictions. The development of explainable artificial intelligence (XAI) methods has enabled us to change the models into a ‘white box’. XAI allows human users to comprehend the results from machine learning algorithms by making them easy to interpret. For instance, the expenditures of healthcare services associated with unplanned readmissions are enormous. This study proposed a stacking-based model to predict 30-day hospital readmission for diabetic patients. We employed Random Under-Sampling to solve the imbalanced class issue, then utilised SelectFromModel for feature selection and constructed a stacking model with base and meta learners. Compared with the different machine learning models, performance analysis showed that our model can better predict readmission than other existing models. This proposed model is also explainable and interpretable. Based on permutation feature importance, the strong predictors were the number of inpatients, the primary diagnosis, discharge to home with home service, and the number of emergencies. The local interpretable model-agnostic explanations method was also employed to demonstrate explainability at the individual level. The findings for the readmission of diabetic patients could be helpful in medical practice and provide valuable recommendations to stakeholders for minimising readmission and reducing public healthcare costs. © 2022 by the authors.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85138742824"
"Volkov E.N.; Averkin A.N.","Volkov, Egor N. (58493887900); Averkin, Aleksej N. (57192557075)","58493887900; 57192557075","Explainable Artificial Intelligence in Clinical Decision Support Systems","2023","Proceedings of 2023 4th International Conference on Neural Networks and Neurotechnologies, NeuroNT 2023","","","","3","6","3","0","10.1109/NeuroNT58640.2023.10175852","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166635744&doi=10.1109%2fNeuroNT58640.2023.10175852&partnerID=40&md5=93a392dc76db52ef1312c89cac528e01","The development of modern medicine and the gradual transition to the Healthcare 5.0 concept implies an increased role of automated clinical decision support systems (CDSS) in the processes of diagnosis and treatment prescription. However, despite the prospects of using artificial neural networks in CDSS, there is a problem of 'non-Transparency'-The inability to understand the reasons for the results, which is unacceptable for systems of this kind. The use of explainable artificial intelligence in CDSS increases the level of confidence in the technology. In the study the analysis of the current state of use of explanatory artificial intelligence in CDSS.  © 2023 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85166635744"
"Praveen S.; Joshi K.","Praveen, Sheeba (57845922300); Joshi, Kapil (57209511667)","57845922300; 57209511667","Explainable Artificial Intelligence in Health Care: How XAI Improves User Trust in High-Risk Decisions","2023","Studies in Computational Intelligence","1072","","","89","99","10","1","10.1007/978-3-031-18292-1_6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142057764&doi=10.1007%2f978-3-031-18292-1_6&partnerID=40&md5=2c82c9cbd221a9acc577b8ef3a251d79","Explainable AI (XAI) is a set of methodologies, design concepts, and procedures that assist developers and organizations in adding a layer of transparency to AI algorithms so that their predictions can be justified. AI models, their predicted impact, and any biases may all be described using XAI. Human specialists can grasp the forecasts generated by this technology and have trust in the results. Medical AI applications must be transparent in order for doctors to trust them. Explainable artificial intelligence (XAI) research has lately gotten a lot of attention. XAI is critical for medical AI solutions to be accepted and adopted into practice. Health care workers utilize AI to speed up and enhance a variety of functions, including decision-making, forecasting, risk management, and even diagnosis, by analyzing medical pictures for abnormalities and patterns that are undetected to the naked eye. Many health care practitioners already use AI, but it is frequently difficult to understand, causing irritation among clinicians and patients, especially when making high-stakes decisions. That’s why the health-care business requires explainable AI (XAI). Significant AI recommendations, such as surgical treatments or hospitalizations, require explanation from providers and patients. XAI delivers interpretable explanations in natural language or other simple-to-understand formats, allowing physicians, patients, and other stakeholders to better comprehend the logic behind a suggestion—and, if required, to dispute its validity. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Book chapter","Final","","Scopus","2-s2.0-85142057764"
"Ali S.; Abdullah; Armand T.P.T.; Athar A.; Hussain A.; Ali M.; Yaseen M.; Joo M.-I.; Kim H.-C.","Ali, Sikandar (57214290702); Abdullah (57195929327); Armand, Tagne Poupi Theodore (57927917200); Athar, Ali (57224726807); Hussain, Ali (57222488729); Ali, Maisam (58074650100); Yaseen, Muhammad (12751996800); Joo, Moon-Il (54787873400); Kim, Hee-Cheol (55739535700)","57214290702; 57195929327; 57927917200; 57224726807; 57222488729; 58074650100; 12751996800; 54787873400; 55739535700","Metaverse in Healthcare Integrated with Explainable AI and Blockchain: Enabling Immersiveness, Ensuring Trust, and Providing Patient Data Security","2023","Sensors","23","2","565","","","","60","10.3390/s23020565","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146718701&doi=10.3390%2fs23020565&partnerID=40&md5=b6388ed54edcb85142133fa1d7fcc794","Digitization and automation have always had an immense impact on healthcare. It embraces every new and advanced technology. Recently the world has witnessed the prominence of the metaverse which is an emerging technology in digital space. The metaverse has huge potential to provide a plethora of health services seamlessly to patients and medical professionals with an immersive experience. This paper proposes the amalgamation of artificial intelligence and blockchain in the metaverse to provide better, faster, and more secure healthcare facilities in digital space with a realistic experience. Our proposed architecture can be summarized as follows. It consists of three environments, namely the doctor’s environment, the patient’s environment, and the metaverse environment. The doctors and patients interact in a metaverse environment assisted by blockchain technology which ensures the safety, security, and privacy of data. The metaverse environment is the main part of our proposed architecture. The doctors, patients, and nurses enter this environment by registering on the blockchain and they are represented by avatars in the metaverse environment. All the consultation activities between the doctor and the patient will be recorded and the data, i.e., images, speech, text, videos, clinical data, etc., will be gathered, transferred, and stored on the blockchain. These data are used for disease prediction and diagnosis by explainable artificial intelligence (XAI) models. The GradCAM and LIME approaches of XAI provide logical reasoning for the prediction of diseases and ensure trust, explainability, interpretability, and transparency regarding the diagnosis and prediction of diseases. Blockchain technology provides data security for patients while enabling transparency, traceability, and immutability regarding their data. These features of blockchain ensure trust among the patients regarding their data. Consequently, this proposed architecture ensures transparency and trust regarding both the diagnosis of diseases and the data security of the patient. We also explored the building block technologies of the metaverse. Furthermore, we also investigated the advantages and challenges of a metaverse in healthcare. © 2023 by the authors.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85146718701"
"Geltser B.I.; Shakhgeldyan K.I.; Rublev V.Y.; Domzhalov I.G.; Tsivanyuk M.M.; Shekunova O.I.","Geltser, B.I. (6602487479); Shakhgeldyan, K.I. (41961853900); Rublev, V. Yu. (57217211806); Domzhalov, I.G. (58344348100); Tsivanyuk, M.M. (57215665817); Shekunova, O.I. (36020704800)","6602487479; 41961853900; 57217211806; 58344348100; 57215665817; 36020704800","Phenotyping of risk factors and prediction of inhospital mortality in patients with coronary artery disease after coronary artery bypass grafting based on explainable artificial intelligence methods","2023","Russian Journal of Cardiology","28","4","5302","85","93","8","1","10.15829/1560-4071-2023-5302","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162759682&doi=10.15829%2f1560-4071-2023-5302&partnerID=40&md5=3091e623c023208896cbe1d412288ecd","Aim. To develop predictive models of inhospital mortality (IHM) in patients with coronary artery disease after coronary artery bypass grafting (CABG), taking into account the results of phenotyping of preoperative risk factors. Material and methods. This retrospective study was conducted based on the data of 999 electronic health records of patients (805 men, 194 women) aged 35 to 81 years with a median (Me) of 63 years who underwent on-pump elective isolated CABG. Two groups of patients were distinguished, the first of which was represented by 63 (6,3%) patients who died in the hospital during the first 30 days after CABG, the second — 936 (93,7%) with a favorable outcome. Preoperative clinical and functional status was assessed using 102 factors. Chi-squares, Fisher, Mann-Whitney methods were used for data processing and analysis. Threshold values of predictors were determined by methods, including maximizing the ratio of true positive IHM cases to false positive ones. Multivariate logistic regression (MLR) was used to develop predictive models. Model accuracy was assessed using 3 following metrics: area under the ROC curve (AUC), sensitivity (Sens), and specificity (Spec). Results. An analysis of preoperative status of patients made it possible to identify 28 risk factors for IHM, combined into 7 phenotypes. The latter formed the feature space of IHM prognostic model, in which each feature demonstrates the patient's compliance with a certain risk factor phenotype. The author's MLR model had high quality metrics (AUC-0,91; Sen-0,9 and Spec-0,85). Conclusion. The developed data processing and analysis algorithm ensured high quality of preoperative risk factors identification and IHM prediction after CABG. Prospects for further research on this issue are related to the improvement of explainable artificial intelligence technologies, which allow developing information systems for managing clinical practice risks. © 2023, Silicea-Poligraf. All rights reserved.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85162759682"
"Kaczmarek-Majer K.; Casalino G.; Castellano G.; Dominiak M.; Hryniewicz O.; Kamińska O.; Vessio G.; Díaz-Rodríguez N.","Kaczmarek-Majer, Katarzyna (57062244400); Casalino, Gabriella (50261398700); Castellano, Giovanna (7005355310); Dominiak, Monika (6603372725); Hryniewicz, Olgierd (6603093398); Kamińska, Olga (57214218618); Vessio, Gennaro (56407135000); Díaz-Rodríguez, Natalia (55904010200)","57062244400; 50261398700; 7005355310; 6603372725; 6603093398; 57214218618; 56407135000; 55904010200","PLENARY: Explaining black-box models in natural language through fuzzy linguistic summaries","2022","Information Sciences","614","","","374","399","25","17","10.1016/j.ins.2022.10.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140956406&doi=10.1016%2fj.ins.2022.10.010&partnerID=40&md5=983e106f6434a18da7d9d13a5361506e","We introduce an approach called PLENARY (exPlaining bLack-box modEls in Natural lAnguage thRough fuzzY linguistic summaries), which is an explainable classifier based on a data-driven predictive model. Neural learning is exploited to derive a predictive model based on two levels of labels associated with the data. Then, model explanations are derived through the popular SHapley Additive exPlanations (SHAP) tool and conveyed in a linguistic form via fuzzy linguistic summaries. The linguistic summarization allows translating the explanations of the model outputs provided by SHAP into statements expressed in natural language. PLENARY accounts for the imprecision related to model outputs by summarizing them into simple linguistic statements and for the imprecision related to the data labeling process by including additional domain knowledge in the form of middle-layer labels. PLENARY is validated on preprocessed speech signals collected from smartphones from patients with bipolar disorder and on publicly available mental health survey data. The experiments confirm that fuzzy linguistic summarization is an effective technique to support meta-analyses of the outputs of AI models. Also, PLENARY improves explainability by aggregating low-level attributes into high-level information granules, and by incorporating vague domain knowledge into a multi-task sequential and compositional multilayer perceptron. SHAP explanations translated into fuzzy linguistic summaries significantly improve understanding of the predictive modelling process and its outputs. © 2022 The Authors","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85140956406"
"Bellantuono L.; Monaco A.; Amoroso N.; Lacalamita A.; Pantaleo E.; Tangaro S.; Bellotti R.","Bellantuono, Loredana (56166549700); Monaco, Alfonso (7201639219); Amoroso, Nicola (55419832300); Lacalamita, Antonio (57226689619); Pantaleo, Ester (16245945500); Tangaro, Sabina (8712490600); Bellotti, Roberto (8419904800)","56166549700; 7201639219; 55419832300; 57226689619; 16245945500; 8712490600; 8419904800","Worldwide impact of lifestyle predictors of dementia prevalence: An eXplainable Artificial Intelligence analysis","2022","Frontiers in Big Data","5","","1027783","","","","6","10.3389/fdata.2022.1027783","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144553908&doi=10.3389%2ffdata.2022.1027783&partnerID=40&md5=9cf28e0b360c9586ce58cdefb9791f8c","Introduction: Dementia is an umbrella term indicating a group of diseases that affect the cognitive sphere. Dementia is not a mere individual health issue, since its interference with the ability to carry out daily activities entails a series of collateral problems, comprising exclusion of patients from civil rights and welfare, unpaid caregiving work, mostly performed by women, and an additional burden on the public healthcare systems. Thus, gender and wealth inequalities (both among individuals and among countries) tend to amplify the social impact of such a disease. Since at present there is no cure for dementia but only drug treatments to slow down its progress and mitigate the symptoms, it is essential to work on prevention and early diagnosis, identifying the risk factors that increase the probability of its onset. The complex and multifactorial etiology of dementia, resulting from an interplay between genetics and environmental factors, can benefit from a multidisciplinary approach that follows the “One Health” guidelines of the World Health Organization. Methods: In this work, we apply methods of Artificial Intelligence and complex systems physics to investigate the possibility to predict dementia prevalence throughout world countries from a set of variables concerning individual health, food consumption, substance use and abuse, healthcare system efficiency. The analysis uses publicly available indicator values at a country level, referred to a time window of 26 years. Results: Employing methods based on eXplainable Artificial Intelligence (XAI) and complex networks, we identify a group of lifestyle factors, mostly concerning nutrition, that contribute the most to dementia incidence prediction. Discussion: The proposed approach provides a methodological basis to develop quantitative tools for action patterns against such a disease, which involves issues deeply related with sustainable, such as good health and resposible food consumption. Copyright © 2022 Bellantuono, Monaco, Amoroso, Lacalamita, Pantaleo, Tangaro and Bellotti.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85144553908"
"Islam M.S.; Hussain I.; Rahman M.M.; Park S.J.; Hossain M.A.","Islam, Mohammed Saidul (58026853300); Hussain, Iqram (57201649041); Rahman, Md Mezbaur (58027109000); Park, Se Jin (57191670651); Hossain, Md Azam (55320796600)","58026853300; 57201649041; 58027109000; 57191670651; 55320796600","Explainable Artificial Intelligence Model for Stroke Prediction Using EEG Signal","2022","Sensors","22","24","9859","","","","49","10.3390/s22249859","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144522614&doi=10.3390%2fs22249859&partnerID=40&md5=58550e4c1f2c0100f6a3690d04a14e75","State-of-the-art healthcare technologies are incorporating advanced Artificial Intelligence (AI) models, allowing for rapid and easy disease diagnosis. However, most AI models are considered “black boxes,” because there is no explanation for the decisions made by these models. Users may find it challenging to comprehend and interpret the results. Explainable AI (XAI) can explain the machine learning (ML) outputs and contribution of features in disease prediction models. Electroencephalography (EEG) is a potential predictive tool for understanding cortical impairment caused by an ischemic stroke and can be utilized for acute stroke prediction, neurologic prognosis, and post-stroke treatment. This study aims to utilize ML models to classify the ischemic stroke group and the healthy control group for acute stroke prediction in active states. Moreover, XAI tools (Eli5 and LIME) were utilized to explain the behavior of the model and determine the significant features that contribute to stroke prediction models. In this work, we studied 48 patients admitted to a hospital with acute ischemic stroke and 75 healthy adults who had no history of identified other neurological illnesses. EEG was obtained within three months following the onset of ischemic stroke symptoms using frontal, central, temporal, and occipital cortical electrodes (Fz, C1, T7, Oz). EEG data were collected in an active state (walking, working, and reading tasks). In the results of the ML approach, the Adaptive Gradient Boosting models showed around 80% accuracy for the classification of the control group and the stroke group. Eli5 and LIME were utilized to explain the behavior of the stroke prediction model and interpret the model locally around the prediction. The Eli5 and LIME interpretable models emphasized the spectral delta and theta features as local contributors to stroke prediction. From the findings of this explainable AI research, it is expected that the stroke-prediction XAI model will help with post-stroke treatment and recovery, as well as help healthcare professionals, make their diagnostic decisions more explainable. © 2022 by the authors.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85144522614"
"Ahmed S.; Nobel S.N.; Ullah O.","Ahmed, Shamim (55456824200); Nobel, Sm Nuruzzaman (58243910800); Ullah, Oli (58244069600)","55456824200; 58243910800; 58244069600","An Effective Deep CNN Model for Multiclass Brain Tumor Detection Using MRI Images and SHAP Explainability","2023","3rd International Conference on Electrical, Computer and Communication Engineering, ECCE 2023","","","","","","","12","10.1109/ECCE57851.2023.10101503","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158979861&doi=10.1109%2fECCE57851.2023.10101503&partnerID=40&md5=c3558ec1cee44f53ffb0120278a57e45","In today's health system, clinical diagnosis has taken on a crucial role. Brain cancer, which is the most serious illness and the main cause of death worldwide, is a significant area of study in the field of medical imaging. Brain cancers must be categorized and found to evaluate the tumors and choose the most appropriate course of treatment for each class. To find brain tumors, numerous imaging modalities are used. However, MRI is frequently utilized since it produces superior images and uses no ionizing radiation. Machine learning's area of deep learning (DL) lately displayed impressive performance, particularly in classification and segmentation issues. We offer an explanation-driven Deep Learning model for the prediction of discrete subtypes of brain tumors (meningioma, glioma, and pituitary) utilizing a brain tumor MRI imaging dataset employing an EfficientNetB0 convolutional neural network (CNN) and Shapley additive explanation (SHAP). The proposed brain tumor detection and classification method surpasses previous methods both visually and numerically, according to an experimental investigation, and achieves an accuracy of 99.84%. The eXplainable Artificial Intelligence (XAI) approach (SHAP) is then used to explain the outcome. © 2023 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85158979861"
"Joseph L.P.; Joseph E.A.; Prasad R.","Joseph, Lionel P. (57208408537); Joseph, Erica A. (57936855300); Prasad, Ramendra (57109763300)","57208408537; 57936855300; 57109763300","Explainable diabetes classification using hybrid Bayesian-optimized TabNet architecture","2022","Computers in Biology and Medicine","151","","106178","","","","34","10.1016/j.compbiomed.2022.106178","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140319643&doi=10.1016%2fj.compbiomed.2022.106178&partnerID=40&md5=65f737c7f865db885349023b61ea414c","Diabetes is a deadly chronic disease that occurs when the pancreas is not able to produce ample insulin or when the body cannot use insulin effectively. If undetected, it may lead to a host of health complications. Hence, accurate and explainable early-stage detection of diabetes is essential for the proper administration of treatment options in leading a healthy and productive life. For this, we developed an interpretable TabNet model tuned via Bayesian optimization (BO). To achieve model-specific interpretability, the attention mechanism of TabNet architecture was used, which offered the local and global model explanations on the influence of the attributes on the outcomes. The model was further explained locally and globally using more robust model-agnostic LIME and SHAP eXplainable Artificial Intelligence (XAI) tools. The proposed model outperformed all benchmarked models by obtaining high accuracy of 92.2% and 99.4% using the Pima Indians diabetes dataset (PIDD) and the early-stage diabetes risk prediction dataset (ESDRPD), respectively. Based on the XAI results, it was clear that the most influential attribute for diabetes classification using PIDD and ESDRPD were Insulin and Polyuria, respectively. The feature importance values registered for insulin was 0.301 (PIDD) and for polyuria 0.206 was registered (ESDRPD). The high accuracy and ancillary interpretability of our objective model is expected to increase end-users trust and confidence in early-stage detection of diabetes. © 2022 Elsevier Ltd","Article","Final","","Scopus","2-s2.0-85140319643"
"Ullah F.; Moon J.; Naeem H.; Jabbar S.","Ullah, Farhan (57189844739); Moon, Jihoon (57193756583); Naeem, Hamad (57120222200); Jabbar, Sohail (35179598300)","57189844739; 57193756583; 57120222200; 35179598300","Explainable artificial intelligence approach in combating real-time surveillance of COVID19 pandemic from CT scan and X-ray images using ensemble model","2022","Journal of Supercomputing","78","17","","19246","19271","25","13","10.1007/s11227-022-04631-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132156594&doi=10.1007%2fs11227-022-04631-z&partnerID=40&md5=95868a2b13bb43b3c509246414a13e53","Population size has made disease monitoring a major concern in the healthcare system, due to which auto-detection has become a top priority. Intelligent disease detection frameworks enable doctors to recognize illnesses, provide stable and accurate results, and lower mortality rates. An acute and severe disease known as Coronavirus (COVID19) has suddenly become a global health crisis. The fastest way to avoid the spreading of Covid19 is to implement an automated detection approach. In this study, an explainable COVID19 detection in CT scan and chest X-ray is established using a combination of deep learning and machine learning classification algorithms. A Convolutional Neural Network (CNN) collects deep features from collected images, and these features are then fed into a machine learning ensemble for COVID19 assessment. To identify COVID19 disease from images, an ensemble model is developed which includes, Gaussian Naive Bayes (GNB), Support Vector Machine (SVM), Decision Tree (DT), Logistic Regression (LR), K-Nearest Neighbor (KNN), and Random Forest (RF). The overall performance of the proposed method is interpreted using Gradient-weighted Class Activation Mapping (Grad-CAM), and t-distributed Stochastic Neighbor Embedding (t-SNE). The proposed method is evaluated using two datasets containing 1,646 and 2,481 CT scan images gathered from COVID19 patients, respectively. Various performance comparisons with state-of-the-art approaches were also shown. The proposed approach beats existing models, with scores of 98.5% accuracy, 99% precision, and 99% recall, respectively. Further, the t-SNE and explainable Artificial Intelligence (AI) experiments are conducted to validate the proposed approach. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Article","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85132156594"
"Erdeniz S.P.; Veeranki S.; Schrempf M.; Jauk S.; Tran T.N.T.; Felfernig A.; Kramer D.; Leodolter W.","Erdeniz, Seda Polat (57195231828); Veeranki, Sai (57194341358); Schrempf, Michael (57223707523); Jauk, Stefanie (57202286649); Tran, Thi Ngoc Trang (57195229497); Felfernig, Alexander (6701498411); Kramer, Diether (57194330023); Leodolter, Werner (57202284871)","57195231828; 57194341358; 57223707523; 57202286649; 57195229497; 6701498411; 57194330023; 57202284871","Explaining Machine Learning Predictions of Decision Support Systems in Healthcare","2022","Current Directions in Biomedical Engineering","8","2","","117","120","3","3","10.1515/cdbme-2022-1031","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137662205&doi=10.1515%2fcdbme-2022-1031&partnerID=40&md5=1418dbf90fbbb36e5939c76db2bcb58f","Artificial Intelligence (AI) methods, which are often based on Machine Learning (ML) algorithms, are also applied in the healthcare domain to provide predictions to physicians and patients based on electronic health records (EHRs), such as history of laboratory values, applied procedures and diagnoses. The question about these predictions “Why Should I Trust You?” encapsulates the issue with ML black boxes. Therefore, explaining the reasons for these ML predictions to physicians and patients is crucial to allow them to decide whether the prediction is applicable or not. In this paper, we explained and evaluated two prediction explanation methods for healthcare professionals (physicians and nurses). We compared two model-agnostic explanation methods based on global feature importance and local feature importance. We evaluated the user trust and reliance (UTR) for the explanation results of each method in a user study based on real patients’ electronic health records (EHR) and the feedback of healthcare professionals. Based on the user study, we observed that both methods have strengths and weaknesses according to the patients’ data, especially based on the data size of the patient. When the amount of data is small, global feature importance is enough to use. However, when the patient’s data size is big, using a local feature importance method makes more sense. As future work, we will develop a hybrid explanation method (by combining these methods automatically with a smart setting) to obtain higher and more stable performance results in terms of user trust and reliance. © 2022 The Author(s), published by De Gruyter.","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85137662205"
"Gao Y.; Mosalam K.M.","Gao, Yuqing (57201685387); Mosalam, Khalid M. (6701424853)","57201685387; 6701424853","Deep learning visual interpretation of structural damage images","2022","Journal of Building Engineering","60","","105144","","","","8","10.1016/j.jobe.2022.105144","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137584913&doi=10.1016%2fj.jobe.2022.105144&partnerID=40&md5=38c0866583e9c436be859ea2e90504af","In the past decade, Deep Convolutional Neural Network (DCNN) achieved the state-of-the-art performance in computer vision tasks. However, DCNN is usually treated as a “black box”, whose internal working principle is hard to understand. This drawback significantly limits its usage in real-world applications, e.g., vision-based Structural Health Monitoring (SHM), where wrong predictions may lead to catastrophic consequences. To resolve this problem, a framework for the interpretation of the Deep Learning (DL) results called Structural Image Guided Map Analysis Box (SIGMA-Box or Σ-Box) is proposed. In the Σ-Box, visual interpretation results (saliency maps) are produced and used for model quality evaluation along with human experts’ domain knowledge. In this study, the use of the Σ-Box is explored in vision-based SHM applications. Firstly, understanding trained DCNN's performance in concrete cover spalling detection is investigated. Besides, learning procedure at different epochs, learned feature from different network depths, influence of training techniques, and level of semantic abstraction are studied. The experiments demonstrate the good interpretable performance of the Σ-Box which facilitates the understanding of the DCNN models’ recognition capabilities, preferences, and limitations. In conclusion, this study sheds light on the high potential of interpreting the trained DCNN in vision-based SHM, providing confidence to the engineers for practical engineering applications involving DL. © 2022","Article","Final","","Scopus","2-s2.0-85137584913"
"Gimeno M.; San José-Enériz E.; Villar S.; Agirre X.; Prosper F.; Rubio A.; Carazo F.","Gimeno, Marian (57209584593); San José-Enériz, Edurne (16202812700); Villar, Sara (57202853416); Agirre, Xabier (57202226833); Prosper, Felipe (7003649174); Rubio, Angel (35607051100); Carazo, Fernando (57204715305)","57209584593; 16202812700; 57202853416; 57202226833; 7003649174; 35607051100; 57204715305","Explainable artificial intelligence for precision medicine in acute myeloid leukemia","2022","Frontiers in Immunology","13","","977358","","","","11","10.3389/fimmu.2022.977358","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139817717&doi=10.3389%2ffimmu.2022.977358&partnerID=40&md5=f1bc6c46ee9f2d1648e1f07bc2e4e83b","Artificial intelligence (AI) can unveil novel personalized treatments based on drug screening and whole-exome sequencing experiments (WES). However, the concept of “black box” in AI limits the potential of this approach to be translated into the clinical practice. In contrast, explainable AI (XAI) focuses on making AI results understandable to humans. Here, we present a novel XAI method -called multi-dimensional module optimization (MOM)- that associates drug screening with genetic events, while guaranteeing that predictions are interpretable and robust. We applied MOM to an acute myeloid leukemia (AML) cohort of 319 ex-vivo tumor samples with 122 screened drugs and WES. MOM returned a therapeutic strategy based on the FLT3, CBFβ-MYH11, and NRAS status, which predicted AML patient response to Quizartinib, Trametinib, Selumetinib, and Crizotinib. We successfully validated the results in three different large-scale screening experiments. We believe that XAI will help healthcare providers and drug regulators better understand AI medical decisions. Copyright © 2022 Gimeno, San José-Enériz, Villar, Agirre, Prosper, Rubio and Carazo.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85139817717"
"Morais F.L.D.; Garcia A.C.B.; Dos Santos P.S.M.; Ribeiro L.A.P.A.","Morais, Fábio Luiz D. (58486450600); Garcia, Ana Cristina B. (25723096500); Dos Santos, Paulo Sérgio M. (24328396100); Ribeiro, Luiz Alberto P. A. (57224526423)","58486450600; 25723096500; 24328396100; 57224526423","Do Explainable AI techniques effectively explain their rationale? A case study from the domain expert's perspective","2023","Proceedings of the 2023 26th International Conference on Computer Supported Cooperative Work in Design, CSCWD 2023","","","","1569","1574","5","0","10.1109/CSCWD57460.2023.10152722","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164676219&doi=10.1109%2fCSCWD57460.2023.10152722&partnerID=40&md5=cd1c95c2b4dad94d1e0aeebf71d54bd6","Artificial Intelligence (AI) systems are technologies impacting our lives. The systems learn from existing datasets that record past human decisions. Their performance is measured in terms of accuracy, precision, and recall for reproducing already-known results. Understanding the system's rationale is crucial to check for bias and accept such technology. Explainable AI (XAI) is the area devoted to opening the AI black box, and designing guidelines to build explainable AI systems. Nevertheless, it is important to understand the user's needs for these explanations. This paper presents an investigation of the usefulness of XAI systems in the field of cancer diagnosis from the domain expert's (oncologist) perspective. The main findings suggest domain experts (1) understood the outcomes of the XAI systems; (2) considered XAI outcomes as informative, rather than explanatory; (3) would like to go beyond the fixed presented perspective; and (4) missed the causal relation that would reveal the system's rationale.  © 2023 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85164676219"
"Khan I.U.","Khan, Irfan Ullah (57191901860)","57191901860","Explainable Artificial Intelligence (XAI) Model for the Diagnosis of Urinary Tract Infections in Emergency Care Patients","2022","Mathematical Modelling of Engineering Problems","9","4","","971","978","7","1","10.18280/mmep.090414","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138634660&doi=10.18280%2fmmep.090414&partnerID=40&md5=56c58b95e35734cc61355327f66e43ba","Significance of machine learning (ML), deep learning (DL) techniques and the availability of Electronic Health Records (EHR) has motivated the need of automated diagnosis system. Furthermore, this development has transformed the health care systems. Recently, several ML and DL models has been proposed for various diseases and has shown the significant outcomes as well. Unfortunately, Urinary tract infections (UTI) is among the minor diseases that is not investigated a lot interms of diagnosing using computation intelligence techniques. However, these models lack the reliability due to the black box nature of the highly complex logic model. Therefore, we attempt to develop an interpretable deep learning (DL) model for the diagnosis of UTI using the dataset of emergency department (ED) patients from UK. Several sets of experiments were conducted using complete dataset, reduced attribute set identified using recursive feature elimination (RFE) and using the attributes identified by the baseline study. The proposed DL model has improved the baseline study accuracy from 0.875 to 0.9275 for 184 feature and 0.859 to 0.943 for the reduced feature. Furthermore, the model has outperformed interms of sensitivity and specificity as well. Due to the data imbalance positive predicted value (PPV), negative predicted value (NPV) and Youden Index was also used for evaluating the performance of the model. The proposed DL model has achieved the highest outcome using 18 attributes selected with RFE technique. The proposed model will produce reliability in the diagnosis made by the model and provide confidence to the doctors to adopt the system in the real life. © 2022, Mathematical Modelling of Engineering Problems. All Rights Reserved.","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85138634660"
"Ji Y.; Zhi X.; Wu Y.; Zhang Y.; Yang Y.; Peng T.; Ji L.","Ji, Yan (57218196283); Zhi, Xiefei (7005003889); Wu, Ying (58399708300); Zhang, Yanqiu (59072285100); Yang, Yitong (58145605200); Peng, Ting (57193530465); Ji, Luying (57203277802)","57218196283; 7005003889; 58399708300; 59072285100; 58145605200; 57193530465; 57203277802","Regression analysis of air pollution and pediatric respiratory diseases based on interpretable machine learning","2023","Frontiers in Earth Science","11","","1105140","","","","4","10.3389/feart.2023.1105140","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150282025&doi=10.3389%2ffeart.2023.1105140&partnerID=40&md5=0be1cdb954aaa0a51903926866f8b0ab","Air pollution is of high relevance to human health. In this study, multiple machine-learning (ML) models—linear regression, random forest (RF), AdaBoost, and neural networks (NNs)—were used to explore the potential impacts of air-pollutant concentrations on the incidence of pediatric respiratory diseases in Taizhou, China. A number of explainable artificial intelligence (XAI) methods were further applied to analyze the model outputs and quantify the feature importance. Our results demonstrate that there are significant seasonal variations both in the numbers of pediatric respiratory outpatients and the concentrations of air pollutants. The concentrations of NO2, CO, and particulate matter (PM10 and PM2.5), as well as the numbers of outpatients, reach their peak values in the winter. This indicates that air pollution is a major factor in pediatric respiratory diseases. The results of the regression models show that ML methods can capture the trends and turning points of clinic visits, and the non-linear models were superior to the linear ones. Among them, the RF model served as the best-performing model. The analysis on the RF model by XAI found that AQI, O3, PM10, and the current month are the most important predictors affecting the numbers of pediatric respiratory outpatients. This shows that the number of outpatients rises with an increasing AQI, especially with the increasing of particulate matter. Our study indicates that ML models with XAI methods are promising for revealing the underlying impacts of air pollution on the pediatric respiratory diseases, which further assists the health-related decision-making. Copyright © 2023 Ji, Zhi, Wu, Zhang, Yang, Peng and Ji.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85150282025"
"Oberste L.; Rüffer F.; Aydingül O.; Rink J.; Heinzl A.","Oberste, Luis (57224832897); Rüffer, Florian (58304924700); Aydingül, Okan (57191158028); Rink, Johann (57222315500); Heinzl, Armin (6603170209)","57224832897; 58304924700; 57191158028; 57222315500; 6603170209","Designing User-Centric Explanations for Medical Imaging with Informed Machine Learning","2023","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13873 LNCS","","","470","484","14","0","10.1007/978-3-031-32808-4_29","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161230356&doi=10.1007%2f978-3-031-32808-4_29&partnerID=40&md5=002adf140563fc1e96e36e0c72dc2d9d","A flawed algorithm released in clinical practice can cause unintended harm to patient health. Risks, regulation, responsibility, and ethics shape the demand of clinical users to understand and rely on the outputs made by artificial intelligence. Explainable artificial intelligence (XAI) offers methods to render a model’s behavior understandable from different perspectives. Extant XAI, however, is mainly data-driven and designed to meet developers’ demands to correct models rather than clinical users’ expectations to reflect clinically relevant information. To this end, informed machine learning (IML) utilizes prior knowledge jointly with data to generate predictions, a promising paradigm to enrich XAI with medical knowledge. To explore how IML can be used to generate explanations that are congruent to clinical users’ demands and useful to medical decision-making, we conduct Action Design Research (ADR) in collaboration with a team of radiologists. We propose an IML-based XAI system for clinically relevant explanations of diagnostic imaging predictions. With the help of ADR, we reduce the gap between implementation and user evaluation and demonstrate the effectiveness of the system in a real-world application with clinicians. While we develop design principles of using IML for user-centric XAI in diagnostic imaging, the study demonstrates that an IML-based design adequately reflects clinicians’ conceptions. In this way, IML inspires greater understandability and trustworthiness of AI-enabled diagnostic imaging. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Conference paper","Final","","Scopus","2-s2.0-85161230356"
"Sayed G.I.; Hassanien A.E.","Sayed, Gehad Ismail (57023745800); Hassanien, Aboul Ella (57192178208)","57023745800; 57192178208","Explainable AI and Slime Mould Algorithm for Classification of Pistachio Species","2023","Studies in Computational Intelligence","1000","","","29","43","14","3","10.1007/978-3-031-13702-0_3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141680820&doi=10.1007%2f978-3-031-13702-0_3&partnerID=40&md5=72d1415e9023e033faea877db389cf17","The safety and quality of the food are considered an essential issue in the entire world. This is due to food being the basis of human health. Nowadays, machine learning algorithms have embodied the recent technology in all stages of food processing such as food grading, food quality determination, and food classification. Pistachio nuts have an important role in the agricultural economy. To increase the efficiency of post-harvest industrial processes, there is a need to introduce technologies for classifying different species of pistachio. This study considers an automated model to separate pistachio species. The proposed pistachio species classification consists of three main phases; features selection based on slime mould algorithm phase, feature interpretation based on explainable artificial intelligence phase, and finally classification of pistachio species using logistic regression phase. The proposed pistachio species classification model obtained overall 90% classification accuracy, 90% precision, and 91% f1-score. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Book chapter","Final","","Scopus","2-s2.0-85141680820"
"Bora A.; Sah R.; Singh A.; Sharma D.; Ranjan R.K.","Bora, Abhyudaya (58027691800); Sah, Ritika (58027856500); Singh, Alabhya (58027773200); Sharma, Deepak (57211529033); Ranjan, Ranjeet Kumar (57201668761)","58027691800; 58027856500; 58027773200; 57211529033; 57201668761","Interpretation of machine learning models using XAI - A study on health insurance dataset","2022","2022 10th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions), ICRITO 2022","","","","","","","4","10.1109/ICRITO56286.2022.9964649","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144592294&doi=10.1109%2fICRITO56286.2022.9964649&partnerID=40&md5=a0f0d1ae048716d3a64a3ebbd0a12808","This study aims to dive into the complex Machine Learning algorithms using XAI and provide explanations for outcomes that we received from these algorithms. In this paper, we have proposed to predict the cost of health insurance. The proposed work is composed of two Machine Learning algorithms, namely Multiple Linear Regression and Random Forest, followed by an explanation of predicted results using XAI. Here, we first provide a simple explanation with the help of model-specific approaches based on Microsoft's InterpretML library. Then the predicted insurance premium cost is further explained by model-agnostic techniques, LIME, and SHAP. The significance of the study is to provide a better user experience and build trust between the user and the machine. These techniques can help to check the correctness of the prediction models, as the domain experts can analyze the features that affect the outcome the most and provide their expertise. © 2022 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85144592294"
"Panigutti C.; Beretta A.; Giannotti F.; Pedreschi D.","Panigutti, Cecilia (57194345147); Beretta, Andrea (57357289400); Giannotti, Fosca (7004495132); Pedreschi, Dino (6603935985)","57194345147; 57357289400; 7004495132; 6603935985","Understanding the impact of explanations on advice-taking: a user study for AI-based clinical Decision Support Systems","2022","Conference on Human Factors in Computing Systems - Proceedings","","","568","","","","40","10.1145/3491102.3502104","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130523627&doi=10.1145%2f3491102.3502104&partnerID=40&md5=8bd562c8e4aa664a91afd60fa3a1f951","The field of eXplainable Artificial Intelligence (XAI) focuses on providing explanations for AI systems' decisions. XAI applications to AI-based Clinical Decision Support Systems (DSS) should increase trust in the DSS by allowing clinicians to investigate the reasons behind its suggestions. In this paper, we present the results of a user study on the impact of advice from a clinical DSS on healthcare providers' judgment in two different cases: the case where the clinical DSS explains its suggestion and the case it does not. We examined the weight of advice, the behavioral intention to use the system, and the perceptions with quantitative and qualitative measures. Our results indicate a more significant impact of advice when an explanation for the DSS decision is provided. Additionally, through the open-ended questions, we provide some insights on how to improve the explanations in the diagnosis forecasts for healthcare assistants, nurses, and doctors. © 2022 Owner/Author.","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85130523627"
"Hoxhallari K.; Purcell W.; Neubauer T.","Hoxhallari, K. (58619359900); Purcell, W. (57909292600); Neubauer, T. (19933872600)","58619359900; 57909292600; 19933872600","The potential of Explainable Artificial Intelligence in Precision Livestock Farming","2022","Precision Livestock Farming 2022 - Papers Presented at the 10th European Conference on Precision Livestock Farming, ECPLF 2022","","","","710","717","7","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166431352&partnerID=40&md5=c6f1f6139b11dcc7dc34a10c882fead7","In the discussion on the increasing demand for food, which is to be met by efficient and sustainable increases in productivity, animal welfare is becoming increasingly important. Animal health issues must be identified to prevent epidemics that significantly impact the economic performance of farms or even cause societal harm. The use of cutting-edge technologies (IoT, sensors, Big Data processing, etc.) is increasingly enabling early intervention in livestock farming to curb productivity losses through real-time monitoring, alerts, and decision support. The ubiquity of these technological solutions has enabled stakeholders to create more robust agricultural supply chains, that deliver sustainable nutrition for a growing population. However, the increasing use of Artificial Intelligence (AI), which is responsible for many of the current breakthroughs in Precision Livestock Farming (PLF) and agriculture in general, has meant that modern decision-support solutions have increasingly transitioned toward black box systems. It has become apparent that a gap exists between efforts to develop more advanced machine learning models, and the growing demands for ethical assessment and transparency in agriculture decision-making. Explainable Artificial Intelligence (XAI) is one such solution that could prove effective in overcoming the current limitations of black-box models, by allowing the decision-making process of such models to be explored. Through a literature review, we evaluate the potential of XAI in various agricultural use cases and demonstrate the potential benefits of its application to precision livestock management. © ECPLF 2022. All rights reserved.","Conference paper","Final","","Scopus","2-s2.0-85166431352"
"","","","22nd International Conference on Innovations for Community Services, I4CS 2022","2022","Communications in Computer and Information Science","1585 CCIS","","","","","334","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132911578&partnerID=40&md5=a1e52f3cd364e24206bbad6e6745f323","The proceedings contain 23 papers. The special focus in this conference is on Innovations for Community Services. The topics include: Challenges of Future Smart and Secure IoT Networking; towards Simulating a Global Robust Model for Early Asthma Detection; brain-Inspired Approaches to Natural Language Processing and Explainable Artificial Intelligence; a Quantum Annealing Approach for Solving Hard Variants of the Stable Marriage Problem; a Quantum Approach for Tactical Capacity Management of Distributed Electricity Generation; back to the Future of Work: Old Questions for New Technologies; solar Energy Harvesting for the Mobile Robotic Platform; demonstrating Feasibility of Blockchain-Driven Carbon Accounting – A Design Study and Demonstrator; a Platform for Offline Voice Assistants: Development of Assistant Applications Without Being Connected to Commercial Central Services; consensus Algorithms in Cryptocurrency and V2X-IoT: Preliminary Study; Realtime Risk Monitoring of SSH Brute Force Attacks; emergency Evacuation Software Simulation Process for Physical Changes; practical Method for Multidimensional Data Ranking: Application for Virtual Machine Migration; learning Based Hardware-Centric Quantum Circuit Generation; preface; foreword; distributed Quantum Machine Learning; understanding Human Mobility for Data-Driven Policy Making; secure Multi-party Computation and Its Applications; context Information Management in a Microservice Based Measurement and Processing Infrastructure; a Web Architecture for E-Health Applications Supporting the Efficient Multipath Transport of Medical Images; research on Detecting Similarity in Trajectory Data and Possible Use Cases.","Conference review","Final","","Scopus","2-s2.0-85132911578"
"Alfeo A.L.; Cimino M.G.C.A.; Vaglini G.","Alfeo, Antonio L. (57189386776); Cimino, Mario G.C.A. (7005259586); Vaglini, Gigliola (6603600549)","57189386776; 7005259586; 6603600549","Degradation stage classification via interpretable feature learning","2022","Journal of Manufacturing Systems","62","","","972","983","11","17","10.1016/j.jmsy.2021.05.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105746772&doi=10.1016%2fj.jmsy.2021.05.003&partnerID=40&md5=90b9adba2c1b063aaccbd14ca77a0e15","Predictive maintenance (PdM) advocates for the usage of machine learning technologies to monitor asset's health conditions and plan maintenance activities accordingly. However, according to the specific degradation process, some health-related measures (e.g. temperature) may be not informative enough to reliably assess the health stage. Moreover, each measure needs to be properly treated to extract the information linked to the health stage. Those issues are usually addressed by performing a manual feature engineering, which results in high management cost and poor generalization capability of those approaches. In this work, we address this issue by coupling a health stage classifier with a feature learning mechanism. With feature learning, minimally processed data are automatically transformed into informative features. Many effective feature learning approaches are based on deep learning. With those, the features are obtained as a non-linear combination of the inputs, thus it is difficult to understand the input's contribution to the classification outcome and so the reasoning behind the model. Still, these insights are increasingly required to interpret the results and assess the reliability of the model. In this regard, we propose a feature learning approach able to (i) effectively extract high-quality features by processing different input signals, and (ii) provide useful insights about the most informative domain transformations (e.g. Fourier transform or probability density function) of the input signals (e.g. vibration or temperature). The effectiveness of the proposed approach is tested with publicly available real-world datasets about bearings' progressive deterioration and compared with the traditional feature engineering approach. © 2021 The Society of Manufacturing Engineers","Article","Final","","Scopus","2-s2.0-85105746772"
"Yang G.; Ye Q.; Xia J.","Yang, Guang (57216243504); Ye, Qinghao (57212083670); Xia, Jun (36610952100)","57216243504; 57212083670; 36610952100","Unbox the black-box for the medical explainable AI via multi-modal and multi-centre data fusion: A mini-review, two showcases and beyond","2022","Information Fusion","77","","","29","52","23","285","10.1016/j.inffus.2021.07.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127068702&doi=10.1016%2fj.inffus.2021.07.016&partnerID=40&md5=30808f642dec8ce43d426fc67aa356b6","Explainable Artificial Intelligence (XAI) is an emerging research topic of machine learning aimed at unboxing how AI systems’ black-box choices are made. This research field inspects the measures and models involved in decision-making and seeks solutions to explain them explicitly. Many of the machine learning algorithms cannot manifest how and why a decision has been cast. This is particularly true of the most popular deep neural network approaches currently in use. Consequently, our confidence in AI systems can be hindered by the lack of explainability in these black-box models. The XAI becomes more and more crucial for deep learning powered applications, especially for medical and healthcare studies, although in general these deep neural networks can return an arresting dividend in performance. The insufficient explainability and transparency in most existing AI systems can be one of the major reasons that successful implementation and integration of AI tools into routine clinical practice are uncommon. In this study, we first surveyed the current progress of XAI and in particular its advances in healthcare applications. We then introduced our solutions for XAI leveraging multi-modal and multi-centre data fusion, and subsequently validated in two showcases following real clinical scenarios. Comprehensive quantitative and qualitative analyses can prove the efficacy of our proposed XAI solutions, from which we can envisage successful applications in a broader range of clinical questions. © 2021 The Authors","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85127068702"
"Ramon Y.; Farrokhnia R.A.; Matz S.C.; Martens D.","Ramon, Yanou (57210274688); Farrokhnia, R.A. (57220647225); Matz, Sandra C. (56825496400); Martens, David (14034052100)","57210274688; 57220647225; 56825496400; 14034052100","Explainable AI for Psychological Profiling from Behavioral Data: An Application to Big Five Personality Predictions from Financial Transaction Records","2021","Information (Switzerland)","12","12","518","","","","8","10.3390/info12120518","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122394397&doi=10.3390%2finfo12120518&partnerID=40&md5=227e235531fe52e94f837172965815ba","Every step we take in the digital world leaves behind a record of our behavior; a digital footprint. Research has suggested that algorithms can translate these digital footprints into accurate estimates of psychological characteristics, including personality traits, mental health or intelligence. The mechanisms by which AI generates these insights, however, often remain opaque. In this paper, we show how Explainable AI (XAI) can help domain experts and data subjects validate, question, and improve models that classify psychological traits from digital footprints. We elaborate on two popular XAI methods (rule extraction and counterfactual explanations) in the context of Big Five personality predictions (traits and facets) from financial transactions data (N = 6408). First, we demonstrate how global rule extraction sheds light on the spending patterns identified by the model as most predictive for personality, and discuss how these rules can be used to explain, validate, and improve the model. Second, we implement local rule extraction to show that individuals are assigned to personality classes because of their unique financial behavior, and there exists a positive link between the model’s prediction confidence and the number of features that contributed to the prediction. Our experiments highlight the importance of both global and local XAI methods. By better understanding how predictive models work in general as well as how they derive an outcome for a particular person, XAI promotes accountability in a world in which AI impacts the lives of billions of people around the world. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85122394397"
"Oliver-Roig A.; Rico-Juan J.R.; Richart-Martínez M.; Cabrero-García J.","Oliver-Roig, Antonio (26638899800); Rico-Juan, Juan Ramón (6506447412); Richart-Martínez, Miguel (6508038686); Cabrero-García, Julio (55662958100)","26638899800; 6506447412; 6508038686; 55662958100","Predicting exclusive breastfeeding in maternity wards using machine learning techniques","2022","Computer Methods and Programs in Biomedicine","221","","106837","","","","7","10.1016/j.cmpb.2022.106837","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129707782&doi=10.1016%2fj.cmpb.2022.106837&partnerID=40&md5=c98e19de0d018023e6998ab34bf04e23","Background and objective: Adequate support in maternity wards is decisive for breastfeeding outcomes during the first year of life. Quality improvement interventions require the identification of the factors influencing hospital benchmark indicators. Machine Learning (ML) models and post-hoc Explainable Artificial Intelligence (XAI) techniques allow accurate predictions and explaining them. This study aimed to predict exclusive breastfeeding during the in-hospital postpartum stay by ML algorithms and explain the ML model's behaviour to support decision making. Methods: The dataset included 2042 mothers giving birth in 18 hospitals in Eastern Spain. We obtained information on demographics, mothers’ breastfeeding experiences, clinical variables, and participating hospitals’ support conditions. The outcome variable was exclusive breastfeeding during the in-hospital postpartum stay. We tested algorithms from different ML families. To evaluate the ML models, we applied 10-fold stratified cross-validation. We used the following metrics: Area under curve receiver operating characteristic (ROC AUC), area under curve precision-recall (PR AUC), accuracy, and Brier score. After selecting the best fitting model, we calculated Shapley's additive values to assign weights to each predictor depending on its additive contribution to the outcome and to explain the predictions. Results: The XGBoost algorithms showed the best metrics (ROC AUC = 0.78, PR AUC = 0.86, accuracy = 0.75, Brier = 0.17). The main predictors of the model included, in order of importance, the pacifier use, the degree of breastfeeding self-efficacy, the previous breastfeeding experience, the birth weight, the admission of the baby to a neonatal care unit after birth, the moment of the first skin-to-skin contact between mother and baby, and the Baby-Friendly Hospital Initiative accreditation of the hospital. Specific examples for linear and nonlinear relations between main predictors and the outcome and heterogeneity of effects are presented. Also, we describe diverse individual cases showing the variation of the prediction depending on individual characteristics. Conclusion: The ML model adequately predicted exclusive breastfeeding during the in-hospital stay. Our results pointed to opportunities for improving care related to support for specific mother's groups, defined by current and previous infant feeding experiences and clinical conditions of the newborns, and the participating hospitals’ support conditions. Also, XAI techniques allowed identifying non-linearity relations and effect's heterogeneity, explaining specific cases’ risk variations. © 2022 The Authors","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85129707782"
"Valente F.; Paredes S.; Henriques J.; Rocha T.; de Carvalho P.; Morais J.","Valente, F. (57223922827); Paredes, S. (24781056400); Henriques, J. (14010018700); Rocha, T. (9637772400); de Carvalho, P. (26535630100); Morais, J. (57210400438)","57223922827; 24781056400; 14010018700; 9637772400; 26535630100; 57210400438","Interpretability, personalization and reliability of a machine learning based clinical decision support system","2022","Data Mining and Knowledge Discovery","36","3","","1140","1173","33","7","10.1007/s10618-022-00821-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128187311&doi=10.1007%2fs10618-022-00821-8&partnerID=40&md5=8b4a5c6c2ec5a81139bb612babd57039","Artificial intelligence (AI) has achieved notable performances in many fields and its research impact in healthcare has been unquestionable. Nevertheless, the deployment of such computational models in clinical practice is still limited. Some of the major issues recognized as barriers to a successful real-world machine learning applications include lack of: transparency; reliability and personalization. Actually, these aspects are decisive not only for patient safety, but also to assure the confidence of professionals. Explainable AI aims at to achieve solutions for artificial intelligence transparency and reliability concerns, with the capacity to better understand and trust a model, providing the ability to justify its outcomes, thus effectively assisting clinicians in rationalizing the model prediction. This work proposes an innovative machine learning based approach, implementing a hybrid scheme, able to combine in a systematic way knowledge-driven and data-driven techniques. In a first step a global set of interpretable rules is generated, founded on clinical evidence. Then, in a second phase, a machine learning model is trained to select, from the global set of rules, the subset that is more appropriate for a given patient, according to his particular characteristics. This approach addresses simultaneously three of the central requirements of explainable AI—interpretability, personalization, and reliability—without impairing the accuracy of the model’s prediction. The scheme was validated with a real dataset provided by two Portuguese Hospitals, the Santa Cruz Hospital, Lisbon, and the Santo André Hospital, Leiria, comprising a total of N = 1111 patients that suffered an acute coronary syndrome event, where the 30 days mortality was assessed. When compared with standard black-box structures (e.g. feedforward neural network) the proposed scheme achieves similar performances, while ensures simultaneously clinical interpretability and personalization of the model, as well as provides a level of reliability to the estimated mortality risk. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media LLC, part of Springer Nature.","Article","Final","","Scopus","2-s2.0-85128187311"
"Antoniou G.; Papadakis E.; Baryannis G.","Antoniou, Grigoris (7005674407); Papadakis, Emmanuel (57193568542); Baryannis, George (35092202700)","7005674407; 57193568542; 35092202700","Mental Health Diagnosis: A Case for Explainable Artificial Intelligence","2022","International Journal on Artificial Intelligence Tools","31","3","2241003","","","","12","10.1142/S0218213022410032","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129957649&doi=10.1142%2fS0218213022410032&partnerID=40&md5=79d7a6943312fffcb1118fae72884e62","Mental illnesses are becoming increasingly prevalent, in turn leading to an increased interest in exploring artificial intelligence (AI) solutions to facilitate and enhance healthcare processes ranging from diagnosis to monitoring and treatment. In contrast to application areas where black box systems may be acceptable, explainability in healthcare applications is essential, especially in the case of diagnosing complex and sensitive mental health issues. In this paper, we first summarize recent developments in AI research for mental health, followed by an overview of approaches to explainable AI and their potential benefits in healthcare settings. We then present a recent case study of applying explainable AI for ADHD diagnosis which is used as a basis to identify challenges in realizing explainable AI solutions for mental health diagnosis and potential future research directions to address these challenges.  © 2022 World Scientific Publishing Company.","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85129957649"
"Yang C.C.","Yang, Christopher C. (7407740308)","7407740308","Explainable Artificial Intelligence for Predictive Modeling in Healthcare","2022","Journal of Healthcare Informatics Research","6","2","","228","239","11","49","10.1007/s41666-022-00114-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124522335&doi=10.1007%2fs41666-022-00114-1&partnerID=40&md5=f9fbc5269b957f488746c88baa8d5ece","The principle behind artificial intelligence is mimicking human intelligence in the way that it can perform tasks, recognize patterns, or predict outcomes through learning from the acquired data of various sources. Artificial intelligence and machine learning algorithms have been widely used in autonomous driving, recommender systems in electronic commerce and social media, fintech, natural language understanding, and question answering systems. Artificial intelligence is also gradually changing the landscape of healthcare research (Yu et al. in Biomed Eng 2:719–731, 25). The rule-based approach that relied on the curation of medical knowledge and the construction of robust decision rules had drawn significant attention in diagnosing diseases and clinical decision support since half a century ago. In recent years, machine learning algorithms such as deep learning that can account for complex interactions between features is shown to be promising in predictive modeling in healthcare (Deo in Circulation 132:1920–1930, 26). Although many of these artificial intelligence and machine learning algorithms can achieve remarkably high performance, it is often difficult to be completely adopted in practical clinical environments due to the lack of explainability in some of these algorithms. Explainable artificial intelligence (XAI) is emerging to assist in the communication of internal decisions, behavior, and actions to health care professionals. Through explaining the prediction outcomes, XAI gains the trust of the clinicians as they may learn how to apply the predictive modeling in practical situations instead of blindly following the predictions. There are still many scenarios to explore how to make XAI effective in clinical settings due to the complexity of medical knowledge. © 2022, The Author(s), under exclusive licence to Springer Nature Switzerland AG.","Article","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85124522335"
"Uddin M.Z.","Uddin, Md. Zia (24482836700)","24482836700","Depression Detection in Text Using Long Short-Term Memory-Based Neural Structured Learning","2022","2022 International Conference on Innovations in Science, Engineering and Technology, ICISET 2022","","","","408","414","6","1","10.1109/ICISET54810.2022.9775893","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131715390&doi=10.1109%2fICISET54810.2022.9775893&partnerID=40&md5=a37622476f8574e4e2e91a89cb7d84f9","These days, depression is a common illness worldwide which varies from usual mood fluctuations to challenges in everyday life. When depressive symptoms stay for long in the form of moderate or high intensity, it may result in becoming a serious health condition such as moving towards suicide. The advancement of machine learning can help technologies to build smart technologies to detect depression from different sources of data such as text. This work proposes a robust approach to detect depression from text messages using Long Short-Term Memory (LSTM)-based Neural Structured Learning (NSL). First of all, a text dataset is collected from the queries submitted in a Norwegian youth forum and then efficient features are obtained based on pre-defined handcrafted robust features developed by focusing on the symptoms of depression that were defined by medical practitioners and psychologists, rather than applying typical word frequency-based features (e.g., conventional term frequency-inverse document frequency (TF-IDF) and onehot) where the frequency of the words in the texts are focused but not the importance of the words. After that, LSTM-based NSL approach is applied as deep learning method to train the features discriminating depression and non-depression. The trained LSTM-based NSL is then utilized later to detect depression in the testing text messages. Besides, to explain the machine learning model's decision, a popular explainable Artificial Intelligence (XAI) algorithm is applied, which is Local Interpretable ModelAgnostic Explanations (LIME). The proposed approach shows the superiority against the traditional approaches on the dataset consists of Norwegian text, achieving the mean accuracy of 99%. Though it has been applied on Norwegian dataset, but the proposed concept can however be applied on other datasets as well, using the translated features.  © 2022 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85131715390"
"Mohanty A.; Mishra S.","Mohanty, Aryan (57647088000); Mishra, Sushruta (57189066689)","57647088000; 57189066689","A Comprehensive Study of Explainable Artificial Intelligence in Healthcare","2022","Studies in Computational Intelligence","1024","","","475","502","27","44","10.1007/978-981-19-1076-0_25","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128928283&doi=10.1007%2f978-981-19-1076-0_25&partnerID=40&md5=d4c65219747fed3cafdfdd653e0b88b6","The recent development of Artificial intelligence and Machine learning, in general, has exhibited impressive results in a variety of fields, especially through the introduction of deep learning (DL). Even though they show an extraordinary performance in a substantial number of jobs and have tremendous potential. This surge in performance is usually pulled off through the increase in model complexity, giving rise to the black-box model and creating confusion about how they work and, ultimately, how they make judgments. This uncertainty has made it difficult for machine-learning programs to be used in more sensitive but essential areas, such as health care, where their benefits can be enormous, Thus giving birth to the need for Explainable AI. Explainable Artificial Intelligence (XAI) is a new machine-learning research subject aiming at decoding how AI systems make black-box decisions. This chapter focuses on the need for Explainable AI in the field of healthcare and some techniques like LIME, SHAP, PDPs, and a few others, through which complex models can be explained. We will see the use of explainable methods by analyzing two case studies. Through the use of this article, clinicians, theorists, and practitioners can get a better insight into how these models work and can help to bring a high level of accountability and transparency. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Book chapter","Final","","Scopus","2-s2.0-85128928283"
"Aslam N.; Khan I.U.; Aljishi R.F.; Alnamer Z.M.; Alzawad Z.M.; Almomen F.A.; Alramadan F.A.","Aslam, Nida (37013076100); Khan, Irfan Ullah (57191901860); Aljishi, Reem Fadel (57449607600); Alnamer, Zahra Maher (57449054200); Alzawad, Zahra Majed (57448943400); Almomen, Fatima Abdulmohsen (57449715400); Alramadan, Fatima Abbas (57449607700)","37013076100; 57191901860; 57449607600; 57449054200; 57448943400; 57449715400; 57449607700","Explainable Computational Intelligence Model for Antepartum Fetal Monitoring to Predict the Risk of IUGR","2022","Electronics (Switzerland)","11","4","593","","","","10","10.3390/electronics11040593","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124490374&doi=10.3390%2felectronics11040593&partnerID=40&md5=625a074c7f40b03d3afcc27183acc6ed","Intrauterine Growth Restriction (IUGR) is a restriction of the fetus that involves the ab-normal growth rate of the fetus, and it has a huge impact on the new-born’s health. Machine learning (ML) algorithms can help in early prediction and discrimination of the abnormality of the fetus’ health to assist in reducing the risk during the antepartum period. Therefore, in this study, Random Forest (RF), Support Vector Machine (SVM), K Nearest Neighbor (KNN) and Gradient Boosting (GB) was utilized to discriminate whether a fetus was healthy or suffering from IUGR based on the fetal heart rate (FHR). The Recursive Feature Elimination (RFE) method was used to select the sig-nificant feature for the classification of fetus. Furthermore, the study Explainable Artificial Intelligence (EAI) was implemented using LIME and SHAP to generate the explanation and to add com-prehensibility in the proposed models. The experimental results indicate that RF achieved the high-est accuracy (0.97) and F1-score (0.98) with the reduced set of features. However, the SVM outperformed it in terms of Positive Predictive Value (PPV) and specificity (SP). The performance of the model was further validated using another dataset and found that it outperformed the baseline studies for both the datasets. The proposed model can aid doctors in monitoring fetal health and enhancing the prediction process. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85124490374"
"Tahir G.A.; Loo C.K.","Tahir, Ghalib Ahmed (55614470800); Loo, Chu Kiong (55663408900)","55614470800; 55663408900","Explainable deep learning ensemble for food image analysis on edge devices","2021","Computers in Biology and Medicine","139","","104972","","","","11","10.1016/j.compbiomed.2021.104972","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118534024&doi=10.1016%2fj.compbiomed.2021.104972&partnerID=40&md5=8672f6151531800daebad7164c8c9799","Food recognition systems recently garnered much research attention in the relevant field due to their ability to obtain objective measurements for dietary intake. This feature contributes to the management of various chronic conditions. Challenges such as inter and intraclass variations alongside the practical applications of smart glasses, wearable cameras, and mobile devices require resource-efficient food recognition models with high classification performance. Furthermore, explainable AI is also crucial in health-related domains as it characterizes model performance, enhancing its transparency and objectivity. Our proposed architecture attempts to address these challenges by drawing on the strengths of the transfer learning technique upon initializing MobiletNetV3 with weights from a pre-trained model of ImageNet. The MobileNetV3 achieves superior performance using the squeeze and excitation strategy, providing unequal weight to different input channels and contrasting equal weights in other variants. Despite being fast and efficient, there is a high possibility for it to be stuck in the local optima like other deep neural networks, reducing the desired classification performance of the model. Thus, we overcome this issue by applying the snapshot ensemble approach as it enables the M model in a single training process without any increase in the required training time. As a result, each snapshot in the ensemble visits different local minima before converging to the final solution which enhances recognition performance. On overcoming the challenge of explainability, we argue that explanations cannot be monolithic, since each stakeholder perceive the results’, explanations based on different objectives and aims. Thus, we proposed a user-centered explainable artificial intelligence (AI) framework to increase the trust of the involved parties by inferencing and rationalizing the results according to needs and user profile. Our framework is comprehensive in terms of a dietary assessment app as it detects Food/Non-Food, food categories, and ingredients. Experimental results on the standard food benchmarks and newly contributed Malaysian food dataset for ingredient detection demonstrated superior performance on an integrated set of measures over other methodologies. © 2021 Elsevier Ltd","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85118534024"
"Lokesh G.; Kavya Tejaswy T.; Sai Meghana Y.; Kameswara Rao M.","Lokesh, Govvala (57218387070); Kavya Tejaswy, T. (57715349100); Sai Meghana, Y. (57716898400); Kameswara Rao, M. (58181844800)","57218387070; 57715349100; 57716898400; 58181844800","Medical Report Analysis Using Explainable Ai","2022","Lecture Notes in Electrical Engineering","828","","","1083","1090","7","1","10.1007/978-981-16-7985-8_113","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130901212&doi=10.1007%2f978-981-16-7985-8_113&partnerID=40&md5=6f3f791ca83d59a50085fd07db8fb59c","Nowadays health issues are the most significant causes of mortality in the world today. Prediction of any disease is a difficult challenge in the area of medical data analysis. Machine learning (ML) has been demonstrated to be successful in helping with settling on decisions and predictions from the huge amount of information delivered by the medical services industry. In addition to that explainable AI helps the user knowing more about the results. XAI gives artificial intelligence to expand cognitive abilities and it needs very high-level communication with the user. Also, the deep learning models are an important part of an explainable artificial intelligence approach. We are approaching a method that aims at finding significant features by applying machine learning techniques resulting in improving the accuracy in the prediction of health issues. The prediction model is presented with various blends of highlights and a few known grouping procedures. We produce an upgraded execution level with an accuracy level of 84.61% through the prediction model disease with random forest algorithm. Clinical choices are regularly made dependent on specialist’s experience and instinct as opposed to on the information-rich covered up in the information. This prompts blunders and numerous costs that influence the nature of clinical administrations. Utilizing scientific instruments and information displayed can help in upgrading clinical choices. In this way, the objective here is to assemble a web application to help patients with health difficulties of heart, liver, kidney, cancer, diabetes. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Conference paper","Final","","Scopus","2-s2.0-85130901212"
"Leung C.K.; Madill E.W.R.; Souza J.; Zhang C.Y.","Leung, Carson K. (7402612526); Madill, Evan W.R. (57604152200); Souza, Joglas (57203537633); Zhang, Christine Y. (57221301770)","7402612526; 57604152200; 57203537633; 57221301770","Towards Trustworthy Artificial Intelligence in Healthcare","2022","Proceedings - 2022 IEEE 10th International Conference on Healthcare Informatics, ICHI 2022","","","","626","632","6","8","10.1109/ICHI54592.2022.00127","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137043193&doi=10.1109%2fICHI54592.2022.00127&partnerID=40&md5=09ac0ed9188232476fc20392728103e7","Healthcare informatics is an interdisciplinary area where computer science, data science, cognitive science, informatics principles, and information technology meet to address problems and support healthcare, medicine, public health, and/or everyday wellness. In many healthcare and medical applications, it is helpful to have models that can learn from historical healthcare data or instances to make predictions on future instances. For human to trust these models or to perceive these models to be trustworthy, it is equally important to build a trustworthy artificial intelligence (AI) solution. Hence, in this paper, towards trustworthy AI in healthcare, we present an explainable AI (XAI) solution that makes accurate predictions and explains the predictions. Evaluation results on real-life datasets demonstrates the effectiveness of our XAI solution towards trustworthy AI in healthcare.  © 2022 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85137043193"
"Rastogi R.; Rastogi A.R.; Sharma D.; Rastogi M.; Garg P.; Srivastava P.","Rastogi, Rohit (57192103103); Rastogi, Akshit Rajan (57222544366); Sharma, Divya (57225937862); Rastogi, Mukund (57215596071); Garg, Priyanshi (57215606844); Srivastava, Prajwal (57219357114)","57192103103; 57222544366; 57225937862; 57215596071; 57215606844; 57219357114","Detection of Air Pollution and Its Adverse Effects on Human Health: Analytical Approach of Different Data Particles in NCR, India","2022","International Journal of Social Ecology and Sustainable Development","13","1","","","","","0","10.4018/IJSESD.289645","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149461457&doi=10.4018%2fIJSESD.289645&partnerID=40&md5=d3abe9d6d9e95b795f8615769fc84115","The magnitude of air contamination in advanced countries is depreciating drastically as observed in ongoing analysis as compared to the previous decade. Notwithstanding, in developing nations and countries pursuing industrialization and automation, the moderate significant levels of air contamination still serves as a threat, in spite of the levels gradually decreasing or remaining static during fast monetary improvement. Recently, hundreds of examinations in accordance with spread and management of disease have risen demonstrating unpleasant healthy consequences concern with present moment and prolonged exposure to the toxins present in the air. Periodic research and studies held in Asian and developed areas moreover indicated comparative healthy outcomes on impermanence of humanity related with association to particulate matter (PM), sulfur dioxide (SO2), ozone (O3), nitrogen dioxide (NO2) to those examined in Europe and North America. The present work is an effort to critically analyze the air pollution by machine learning and its adverse effects on human health. © 2022, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.","Article","Final","","Scopus","2-s2.0-85149461457"
"","","","9th International Work-Conference on Bioinformatics and Biomedical Engineering, IWBBIO 2022","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13346 LNBI","","","","","922","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133019280&partnerID=40&md5=d8d629a8ec3396b9d6ad59db57b69e04","The proceedings contain 75 papers. The special focus in this conference is on Bioinformatics and Biomedical Engineering. The topics include: Automated TTC Image-Based Analysis of Mouse Brain Lesions; PET-Neuroimaging and Neuropsychological Study for Early Cognitive Impairment in Parkinson’s Disease; architecture and Calibration of a Multi-channel Electrical Impedance Myographer; advanced Incremental Attribute Learning Clustering Algorithm for Medical and Healthcare Applications; Assessment of Inflammation in Non-calcified Artery Plaques with Dynamic 18F-FDG-PET/CT: CT Alone, Does-It Detect the Vulnerable Plaque?; Comparative Analysis of the Spatial Structure Chloroplasts and Cyanobacteria Photosynthetic Systems I and II Genes; Unsupervised Classification of Some Bacteria with 16S RNA Genes; modern Approaches to Cancer Treatment; a Service for Flexible Management and Analysis of Heterogeneous Clinical Data; linear Predictive Modeling for Immune Metabolites Related to Other Metabolites; reconfigurable Arduino Shield for Biosignal Acquisition; smart Watch for Smart Health Monitoring: A Literature Review; Data Quality Enhancement for Machine Learning on Wearable ECGs; Measurable Difference Between Malignant and Benign Tumor of the Thyroid Gland Recognizable Using Echogenicity Index in Ultrasound B-MODE Imaging: An Experimental Blind Study; initial Prototype of Low-Cost Stool Monitoring System for Early Detection of Diseases; Cerebral Activation in Subjects with Developmental Coordination Disorder: A Pilot Study with PET Imaging; on the Use of Explainable Artificial Intelligence for the Differential Diagnosis of Pigmented Skin Lesions; estimating Frontal Body Landmarks from Thermal Sensors Using Residual Neural Networks; NMF for Quality Control of Multi-modal Retinal Images for Diagnosis of Diabetes Mellitus and Diabetic Retinopathy; radiomic-Based Lung Nodule Classification in Low-Dose Computed Tomography; modelling of Arbitrary Shaped Channels and Obstacles by Distance Function; Segmentation of Brain MR Images Using Quantum Inspired Firefly Algorithm with Mutation; a Deep Learning Framework for the Prediction of Conversion to Alzheimer Disease.","Conference review","Final","","Scopus","2-s2.0-85133019280"
"Trigueros O.; Blanco A.; Lebeña N.; Casillas A.; Pérez A.","Trigueros, Owen (57264129500); Blanco, Alberto (57210149702); Lebeña, Nuria (57370186400); Casillas, Arantza (22633528200); Pérez, Alicia (57208931888)","57264129500; 57210149702; 57370186400; 22633528200; 57208931888","Explainable ICD multi-label classification of EHRs in Spanish with convolutional attention","2022","International Journal of Medical Informatics","157","","104615","","","","17","10.1016/j.ijmedinf.2021.104615","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120959162&doi=10.1016%2fj.ijmedinf.2021.104615&partnerID=40&md5=fb65bd6f7cd8792225972ab48dc937c5","Background: This work deals with Natural Language Processing applied to Electronic Health Records (EHRs). EHRs are coded following the International Classification of Diseases (ICD) leading to a multi-label classification problem. Previously proposed approaches act as black-boxes without giving further insights. Explainable Artificial Intelligence (XAI) helps to clarify what brought the model to make the predictions. Goal: This work aims to obtain explainable predictions of the diseases and procedures contained in EHRs. As an application, we show visualizations of the attention stored and propose a prototype of a Decision Support System (DSS) that highlights the text that motivated the choice of each of the proposed ICD codes. Methods: Convolutional Neural Networks (CNNs) with attention mechanisms were used. Attention mechanisms allow to detect which part of the input (EHRs) motivate the output (medical codes), producing explainable predictions. Results: We successfully applied methods in a Spanish corpus getting challenging results. Finally, we presented the idea of extracting the chronological order of the ICDs in a given EHR by anchoring the codes to different stages of the clinical admission. Conclusions: We found that explainable deep learning models applied to predict medical codes store helpful information that could be used to assist medical experts while reaching a solid performance. In particular, we show that the information stored in the attention mechanisms enables DSS and a shallow chronology of diagnoses. © 2021 Elsevier B.V.","Article","Final","","Scopus","2-s2.0-85120959162"
"Petrauskas V.; Jasinevicius R.; Damuleviciene G.; Liutkevicius A.; Janaviciute A.; Lesauskaite V.; Knasiene J.; Meskauskas Z.; Dovydaitis J.; Kazanavicius V.; Bitinaite‐paskeviciene R.","Petrauskas, Vytautas (57057529000); Jasinevicius, Raimundas (25632553900); Damuleviciene, Gyte (26326201900); Liutkevicius, Agnius (35117897000); Janaviciute, Audrone (55366262400); Lesauskaite, Vita (55928122500); Knasiene, Jurgita (37004834900); Meskauskas, Zygimantas (57190856180); Dovydaitis, Juozas (56670374400); Kazanavicius, Vygintas (16175319300); Bitinaite‐paskeviciene, Raminta (57369080700)","57057529000; 25632553900; 26326201900; 35117897000; 55366262400; 55928122500; 37004834900; 57190856180; 56670374400; 16175319300; 57369080700","Explainable artificial intelligence‐based decision support system for assessing the nutrition‐related geriatric syndromes","2021","Applied Sciences (Switzerland)","11","24","11763","","","","6","10.3390/app112411763","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120938649&doi=10.3390%2fapp112411763&partnerID=40&md5=7205e2d8d58c486d28b14624f5a894a8","The use of artificial intelligence in geriatrics is very promising and relevant, as the diagnosis of a geriatric patient is a complex, experience‐based, and time‐consuming process that involves a variety of questionnaires and subjective and inaccurate patient responses. This paper pro-poses the explainable artificial intelligence‐based (XAI) clinical decision support system (CDSS) to assess nutrition‐related factors (symptoms) and to determine the likelihood of geriatric patient health risks associated with four syndromes: malnutrition, oropharyngeal dysphagia, dehydration, and eating disorders in dementia. The proposed system’s prototype was tested under real conditions at the geriatric department of Lithuanian University of Health Sciences Kaunas Hospital. The subjects of this study were 83 geriatric patients with various health conditions. The assessments of the nutritional status and syndromes of the patients provided by the CDSS were compared with the diagnoses of the physicians obtained using standard assessment methods. The results show that proposed CDSS can efficiently diagnose nutrition‐related geriatric syndromes with high accuracy: 87.95% for malnutrition, 87.95% for oropharyngeal dysphagia, 90.36% for eating disorders in dementia, and 86.75% for dehydration. The research confirms that the proposed XAI‐based CDSS is an effective tool, able to assess nutrition‐related health risk factors and their dependencies and, in some cases, makes even a more accurate decision than a less experienced physician. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85120938649"
"Blanco A.; Remmer S.; Pérez A.; Dalianis H.; Casillas A.","Blanco, Alberto (57210149702); Remmer, Sonja (57429238000); Pérez, Alicia (57208931888); Dalianis, Hercules (6506899838); Casillas, Arantza (22633528200)","57210149702; 57429238000; 57208931888; 6506899838; 22633528200","Implementation of specialised attention mechanisms: ICD-10 classification of Gastrointestinal discharge summaries in English, Spanish and Swedish","2022","Journal of Biomedical Informatics","130","","104050","","","","7","10.1016/j.jbi.2022.104050","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130733947&doi=10.1016%2fj.jbi.2022.104050&partnerID=40&md5=385306b1259527de2b167caa22018967","Multi-label classification according to the International Classification of Diseases (ICD) is an Extreme Multi-label Classification task aiming to categorise health records according to a set of relevant ICD codes. We implemented PlaBERT, a new multi-label text classification head with per-label attention, on top of a BERT model. The model assessment is conducted on Electronic Health Records, conveying Discharge Summaries in three languages – English, Spanish, and Swedish. The study focuses on 157 diagnostic codes from the ICD. We additionally measure the labelling noise to estimate the consistency of the gold standard. Our specialised attention mechanism computes attention weights for each input token and label pair, obtaining the specific relevance of every word concerning each ICD code. The PlaBERT model outputs the computed attention importance for each token and label, allowing for visualisation. Our best results are 40.65, 38.36, and 41.13 F1-Score points on the English, Spanish and Swedish datasets, respectively, for the 157 gastrointestinal codes. Besides, Precision is the metric that most significantly improves owing to the attention mechanism of PlaBERT, with an increase of 44.63, 40.93, and 12.92 points, respectively, for the Spanish, Swedish and English datasets. © 2022 Elsevier Inc.","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85130733947"
"Sengoz N.; Yigit T.","Sengoz, Nilgun (57219639156); Yigit, Tuncay (57223411924)","57219639156; 57223411924","Towards Third Generation AI: Explainable and Interpretable AI; [Ufuncu Nesil Yapay Zekaya Dogru: Afiklanabilir ve Yorumlanabilir Yapay Zeka]","2022","Proceedings - 7th International Conference on Computer Science and Engineering, UBMK 2022","","","","523","526","3","0","10.1109/UBMK55850.2022.9919510","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141869333&doi=10.1109%2fUBMK55850.2022.9919510&partnerID=40&md5=9ec04f0a49528268cd2c4302358a5626","Today, artificial intelligence-based systems, especially machine learning and deep neural network algorithms, make decisions that directly affect people's lives, from health to autonomous vehicles and even to the defense sector. And yet, algorithms like deep neural networks that are high in performance accuracy but low in explainability and trust bring problems in ethics and interpretability. Explainable Artificial Intelligence (XAJD) is a branch of artificial intelligence that produces highquality interpretability, end-user-oriented understanding and explainable tools, techniques and algorithms. In this study, general information about XAI is given.  © 2022 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85141869333"
"Tsarapatsani K.-H.; Sakellarios A.; Pezoulas V.C.; Tsakanikas V.D.; Matsopoulos G.K.; Marz W.; Kleber M.; Fotiadis D.I.","Tsarapatsani, Konstantina-Helen (57406228700); Sakellarios, Antonis (36476633700); Pezoulas, Vasileios C. (57194013364); Tsakanikas, Vasilis D. (36718299600); Matsopoulos, George K. (7004652499); Marz, Winfried (57220877383); Kleber, Marcus (57207897615); Fotiadis, Dimitrios I. (55938920100)","57406228700; 36476633700; 57194013364; 36718299600; 7004652499; 57220877383; 57207897615; 55938920100","Machine Learning Models to Predict Myocardial Infarction Within 10-Years Follow-up of Cardiovascular Disease Progression","2022","BHI-BSN 2022 - IEEE-EMBS International Conference on Biomedical and Health Informatics and IEEE-EMBS International Conference on Wearable and Implantable Body Sensor Networks, Symposium Proceedings","","","","","","","0","10.1109/BHI56158.2022.9926803","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143060675&doi=10.1109%2fBHI56158.2022.9926803&partnerID=40&md5=fabdb02e3c11d097dfccc4026cf3d917","The early prevention of myocardial infarction (MI), a complication of cardiovascular disease (CVD), is an urgent need for the timely provision of medical intervention and the reduction of cardiovascular mortality. The performance of machine learning (ML) has proven useful in aiding the early diagnosis of this disease. In this work, we utilize clinical cardiovascular disease risk factors and biochemical data, employing machine learning models i.e. Random Forest (RF), Extreme Grading Boosting (XGBoost) and Adaptive Boosting (AdaBoost), to predict the 10-year risk of myocardial infarction in patients with 10-years follow-up for CVD. We used the cohort of the Ludwigshafen Risk and Cardiovascular Health (LURIC) study, while 3267 patients were included in the analysis (1361 suffered from MI). We calculated the performance of machine learning models, more specifically the mean values of Accuracy (ACC), Sensitivity, Specificity and the area under the receiver operating characteristic curve (AUC) of each model. We also plotted the corresponding receiver operating characteristic curve for each model. The findings of the analysis reveal that the Extreme Gradient Boosting model detects MI with the highest accuracy (74.27 %). Moreover, explainable artificial intelligence was applied, especially the Shapley values were calculated to identify the most important features and interpret the results with XGBoost.  © 2022 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85143060675"
"Holzinger A.; Dehmer M.; Emmert-Streib F.; Cucchiara R.; Augenstein I.; Ser J.D.; Samek W.; Jurisica I.; Díaz-Rodríguez N.","Holzinger, Andreas (23396282000); Dehmer, Matthias (13404645900); Emmert-Streib, Frank (15057742200); Cucchiara, Rita (7006870483); Augenstein, Isabelle (55236320300); Ser, Javier Del (9737598300); Samek, Wojciech (40762215900); Jurisica, Igor (57226221099); Díaz-Rodríguez, Natalia (55904010200)","23396282000; 13404645900; 15057742200; 7006870483; 55236320300; 9737598300; 40762215900; 57226221099; 55904010200","Information fusion as an integrative cross-cutting enabler to achieve robust, explainable, and trustworthy medical artificial intelligence","2022","Information Fusion","79","","","263","278","15","112","10.1016/j.inffus.2021.10.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119285419&doi=10.1016%2fj.inffus.2021.10.007&partnerID=40&md5=6d62a961843ec279ab47d0d8d93efc2d","Medical artificial intelligence (AI) systems have been remarkably successful, even outperforming human performance at certain tasks. There is no doubt that AI is important to improve human health in many ways and will disrupt various medical workflows in the future. Using AI to solve problems in medicine beyond the lab, in routine environments, we need to do more than to just improve the performance of existing AI methods. Robust AI solutions must be able to cope with imprecision, missing and incorrect information, and explain both the result and the process of how it was obtained to a medical expert. Using conceptual knowledge as a guiding model of reality can help to develop more robust, explainable, and less biased machine learning models that can ideally learn from less data. Achieving these goals will require an orchestrated effort that combines three complementary Frontier Research Areas: (1) Complex Networks and their Inference, (2) Graph causal models and counterfactuals, and (3) Verification and Explainability methods. The goal of this paper is to describe these three areas from a unified view and to motivate how information fusion in a comprehensive and integrative manner can not only help bring these three areas together, but also have a transformative role by bridging the gap between research and practical applications in the context of future trustworthy medical AI. This makes it imperative to include ethical and legal aspects as a cross-cutting discipline, because all future solutions must not only be ethically responsible, but also legally compliant. © 2021 The Authors","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85119285419"
"Hines B.D.; Talbert D.A.; Anton S.R.","Hines, Brandon D. (57721707000); Talbert, Douglas A. (8067046600); Anton, Steven R. (16318527200)","57721707000; 8067046600; 16318527200","Improving Trust via XAI and Pre-Processing for Machine Learning of Complex Biomedical Datasets","2022","Proceedings of the International Florida Artificial Intelligence Research Society Conference, FLAIRS","35","","","","","6","1","10.32473/flairs.v35i.130742","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131134966&doi=10.32473%2fflairs.v35i.130742&partnerID=40&md5=42b5e91fed767f4a1a76aa1ba6cd384b","Complex datasets hold a special place among engineers as the engineering community seeks to solve some of the world’s most difficult problems, but with complexity, comes difficulty in analysis and interpretation. Machine learning seeks to solve this problem by aiding in the analysis of these complex datasets, but the implementation of machine learning introduces an entirely new problem, that of transparency. Machine learning often results in models that perform the assigned task quite well, but the issue lies in the often black-box nature of machine learning models. Often in the engineering community, it can be difficult to trust black-box machine learning models to make crucial determinations within a complex dataset. This study explores this issue by expanding on previously conducted work in which a complex biomedical dataset was generated to explore the problem of persistent pain experienced after receiving a total knee replacement (TKR). Previous work saw the generation of simulated TKRs with varying levels of damage, and structural health monitoring (SHM) techniques were used to collect measurements on the samples with the hope of identifying a method to detect the presence of damage. Machine learning was then introduced as a technique to classify the varying levels of damage present in the dataset. This study explores the implementation of explainable artificial intelligence (XAI) as well as data processing techniques as a method to combat the black-box nature of machine learning models and generate more trustworthy models with respect to domain knowledge. © 2022, by the authors. All rights reserved.","Conference paper","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85131134966"
"Ivanovic M.; Autexier S.; Kokkonidis M.","Ivanovic, Mirjana (7005907326); Autexier, Serge (57193752568); Kokkonidis, Miltiadis (23027828700)","7005907326; 57193752568; 23027828700","AI Approaches in Processing and Using Data in Personalized Medicine","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13389 LNCS","","","11","24","13","2","10.1007/978-3-031-15740-0_2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137985637&doi=10.1007%2f978-3-031-15740-0_2&partnerID=40&md5=4a8dce93f72305b0cb7ea0c2280652be","Nowadays, more and more people suffer from serious diseases and doctors and patients need sophisticated medical and health support. Accordingly, prominent health stakeholders have recognized the importance of development of such services to make patients’ life easier. Such support requires the collection of patients’ complex data. Holistic patient’s data must be properly aggregated, processed, analyzed, and presented to the doctors/caregivers to recommend adequate treatment and actions to improve patient’s health related parameters. Advanced artificial intelligence techniques offer the opportunity to analyze such big data, consume them, and derive new knowledge to support (personalized) medical decisions. New approaches like those based on advanced machine/deep learning, federated learning, transfer learning, explainable artificial intelligence open new paths for more quality use of health and medical data in future. In this paper, we will present some crucial aspects and examples of application of artificial intelligence approaches in (personalized) medical decisions. © 2022, Springer Nature Switzerland AG.","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85137985637"
"Boutorh A.; Rahim H.; Bendoumia Y.","Boutorh, Aicha (55321725400); Rahim, Hala (58022273000); Bendoumia, Yassmine (58022061000)","55321725400; 58022273000; 58022061000","Explainable AI Models for COVID-19 Diagnosis Using CT-Scan Images and Clinical Data","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13483 LNBI","","","185","199","14","4","10.1007/978-3-031-20837-9_15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144305806&doi=10.1007%2f978-3-031-20837-9_15&partnerID=40&md5=f766ea2c38ff682100b6ae173a56d0ac","The pandemic of COVID-19 has had a significant impact on global health and is becoming a major international concern. Fortunately, early detection helped decrease its number of deaths. Artificial Intelligence (AI) and Machine Learning (ML) techniques are a new era, where the main objective is no longer to assist experts in decision-making but to improve and increase their capabilities and this is where interpretability comes in. This study aims to address one of the biggest hurdles that AI faces today which is public trust and acceptance due to its black-box strategy. In this paper, we use a deep Convolutional Neural Network (CNN) on chest computed tomography (CT) image data and Support Vector Machine (SVM) and Random Forest (RF) on clinical symptoms data (Bio-data) to diagnose patients positive for COVID-19. Our objective is to present an Explainable AI (XAI) models by using the Local Interpretable Model-agnostic Explanations (LIME) technique to identify positive patients to the virus in an interpreted way. The results are promising and outperformed the state of the art. The CNN model reached an Accuracy and F1-Score of 96% on CT-scan images, and SVM outperformed RF with Accuracy of 90% and Specificity of 91% on Bio-data. The interpretable results of XAI-Img-Model and XAI-Bio-Model, show that LIME explanations help to understand how SVM and CNN black box models behave in making their decision after being trained on different types of COVID-19 dataset. This can significantly increase trust and help experts understand and learn new patterns for the current pandemic. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Conference paper","Final","","Scopus","2-s2.0-85144305806"
"Ruiz C.; Quaresma M.","Ruiz, Cinthia (57226402090); Quaresma, Manuela (6603776143)","57226402090; 6603776143","Explainable AI for Entertainment: Issues on Video on Demand Platforms","2022","Lecture Notes in Networks and Systems","223 LNNS","","","699","707","8","3","10.1007/978-3-030-74614-8_87","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111424856&doi=10.1007%2f978-3-030-74614-8_87&partnerID=40&md5=714d49c4c8d6fa1e414921d7ba6eac02","With the proliferation of Artificial Intelligence-based systems, several questions arise involving ethical principles. In addition, the human-centered approach takes the focus on the user experience with these systems and studies user needs. A growing issue is the relationship between the transparency of these systems and the trust of users, since most systems are considered black-boxes. In this scenario, the Explainable Artificial Intelligence (XAI) emerges, with the proposal to explain the rationale of the decision making of the algorithms. XAI then starts to gain space in systems that involve high risk, such as health. Our research aims to discuss the importance of transparency to improve the user experience with recommendation mechanisms for entertainment, such as Video on Demand (VoD) platforms. In addition, we intent to raise the adjacent consequences of including XAI, such as improving the control and trust of VoD platforms. For this, we conducted an exploratory research method named Directed Storytelling. The study was conducted with thirty-one participants, all users of VoD platforms, regardless of time and frequency of use of this kind of systems. We note that people understand that there is an automated mechanism making recommendations for content in a personalized way for them, based on their browsing history, but the rules are not explicit. Thus, many users are suspicious of being manipulated by the system’s recommendations and resort to external recommendations, such as tips from third parties or Internet searches through specialized channels. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Conference paper","Final","","Scopus","2-s2.0-85111424856"
"Cheng F.; Liu D.; Du F.; Lin Y.; Zytek A.; Li H.; Qu H.; Veeramachaneni K.","Cheng, Furui (57211997821); Liu, Dongyu (57188753761); Du, Fan (56789527300); Lin, Yanna (57219793935); Zytek, Alexandra (57205571900); Li, Haomin (35795854300); Qu, Huamin (7101947159); Veeramachaneni, Kalyan (7801508939)","57211997821; 57188753761; 56789527300; 57219793935; 57205571900; 35795854300; 7101947159; 7801508939","VBridge: Connecting the Dots between Features and Data to Explain Healthcare Models","2022","IEEE Transactions on Visualization and Computer Graphics","28","1","","378","388","10","21","10.1109/TVCG.2021.3114836","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118658605&doi=10.1109%2fTVCG.2021.3114836&partnerID=40&md5=435e6be8ae5517499c3112942cb1c567","Machine learning (ML) is increasingly applied to Electronic Health Records (EHRs) to solve clinical prediction tasks. Although many ML models perform promisingly, issues with model transparency and interpretability limit their adoption in clinical practice. Directly using existing explainable ML techniques in clinical settings can be challenging. Through literature surveys and collaborations with six clinicians with an average of 17 years of clinical experience, we identified three key challenges, including clinicians' unfamiliarity with ML features, lack of contextual information, and the need for cohort-level evidence. Following an iterative design process, we further designed and developed VBridge, a visual analytics tool that seamlessly incorporates ML explanations into clinicians' decision-making workflow. The system includes a novel hierarchical display of contribution-based feature explanations and enriched interactions that connect the dots between ML features, explanations, and data. We demonstrated the effectiveness of VBridge through two case studies and expert interviews with four clinicians, showing that visually associating model explanations with patients' situational records can help clinicians better interpret and use model predictions when making clinician decisions. We further derived a list of design implications for developing future explainable ML tools to support clinical decision-making.  © 1995-2012 IEEE.","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85118658605"
"Gandhi N.; Mishra S.","Gandhi, Neel (57223269216); Mishra, Shakti (57225942508)","57223269216; 57225942508","Explainable AI for Healthcare: A Study for Interpreting Diabetes Prediction","2022","Lecture Notes in Networks and Systems","256","","","95","105","10","2","10.1007/978-3-030-82469-3_9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116938080&doi=10.1007%2f978-3-030-82469-3_9&partnerID=40&md5=ea3060505ed880c9c37e6dc5268f824b","With extensive use of machine learning in field of healthcare, explainable AI becomes a vital part of machine learning process by helping healthcare practitioners for understanding critical decision-making process. Machine learning has greatly impacted medical field by identification of factors responsible for vulnerable health risks and factors affecting them. Hence, interpretation of machine learning model is extremely critical in the healthcare sector. Healthcare practitioners must understand factors that affect decision-making process for diseases like diabetes made by machine learning model. Explainable artificial intelligence helps clinical practitioners in interpretation of black-box models and their decision-making process to verify why a particular decision was taken by machine learning model that is extremely important in medical field. Interpretability in machine learning methods can help healthcare practitioners to understand the features that affect the decision-making process for detection of diseases like diabetes. In the past few years, researchers have been successful in detecting diabetes using machine learning models. Explainable artificial intelligence is exceptionally useful in turning black-box into glass box machine learning model and explains their respective decision-making process in healthcare. The paper presents various interpretable machine learning methods for understanding factors affecting decision-making in case of diabetes prediction that could be explained using model agnostic methods. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Conference paper","Final","","Scopus","2-s2.0-85116938080"
"Smucny J.; Shi G.; Davidson I.","Smucny, Jason (25951785700); Shi, Ge (57764405500); Davidson, Ian (12344826300)","25951785700; 57764405500; 12344826300","Deep Learning in Neuroimaging: Overcoming Challenges With Emerging Approaches","2022","Frontiers in Psychiatry","13","","912600","","","","6","10.3389/fpsyt.2022.912600","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132834895&doi=10.3389%2ffpsyt.2022.912600&partnerID=40&md5=18444a229d139b9b65c42d981a2b2c9d","Deep learning (DL) is of great interest in psychiatry due its potential yet largely untapped ability to utilize multidimensional datasets (such as fMRI data) to predict clinical outcomes. Typical DL methods, however, have strong assumptions, such as large datasets and underlying model opaqueness, that are suitable for natural image prediction problems but not medical imaging. Here we describe three relatively novel DL approaches that may help accelerate its incorporation into mainstream psychiatry research and ultimately bring it into the clinic as a prognostic tool. We first introduce two methods that can reduce the amount of training data required to develop accurate models. These may prove invaluable for fMRI-based DL given the time and monetary expense required to acquire neuroimaging data. These methods are (1) transfer learning − the ability of deep learners to incorporate knowledge learned from one data source (e.g., fMRI data from one site) and apply it toward learning from a second data source (e.g., data from another site), and (2) data augmentation (via Mixup) − a self-supervised learning technique in which “virtual” instances are created. We then discuss explainable artificial intelligence (XAI), i.e., tools that reveal what features (and in what combinations) deep learners use to make decisions. XAI can be used to solve the “black box” criticism common in DL and reveal mechanisms that ultimately produce clinical outcomes. We expect these techniques to greatly enhance the applicability of DL in psychiatric research and help reveal novel mechanisms and potential pathways for therapeutic intervention in mental illness. Copyright © 2022 Smucny, Shi and Davidson.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85132834895"
"De Magistris G.; Russo S.; Roma P.; Starczewski J.T.; Napoli C.","De Magistris, Giorgio (57497416000); Russo, Samuele (57211467840); Roma, Paolo (35900331800); Starczewski, Janusz T. (22434189000); Napoli, Christian (36866742400)","57497416000; 57211467840; 35900331800; 22434189000; 36866742400","An Explainable Fake News Detector Based on Named Entity Recognition and Stance Classification Applied to COVID-19","2022","Information (Switzerland)","13","3","137","","","","29","10.3390/info13030137","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126568633&doi=10.3390%2finfo13030137&partnerID=40&md5=71a083c35c22f11b5151f898cf4a3430","Over the last few years, the phenomenon of fake news has become an important issue, especially during the worldwide COVID-19 pandemic, and also a serious risk for the public health. Due to the huge amount of information that is produced by the social media such as Facebook and Twitter it is becoming difficult to check the produced contents manually. This study proposes an automatic fake news detection system that supports or disproves the dubious claims while returning a set of documents from verified sources. The system is composed of multiple modules and it makes use of different techniques from machine learning, deep learning and natural language processing. Such techniques are used for the selection of relevant documents, to find among those, the ones that are similar to the tested claim and their stances. The proposed system will be used to check medical news and, in particular, the trustworthiness of posts related to the COVID-19 pandemic, vaccine and cure. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85126568633"
"Vo T.H.; Nguyen N.T.K.; Kha Q.H.; Le N.Q.K.","Vo, Thanh Hoa (59049160800); Nguyen, Ngan Thi Kim (57211071169); Kha, Quang Hien (57224131231); Le, Nguyen Quoc Khanh (57208281644)","59049160800; 57211071169; 57224131231; 57208281644","On the road to explainable AI in drug-drug interactions prediction: A systematic review","2022","Computational and Structural Biotechnology Journal","20","","","2112","2123","11","59","10.1016/j.csbj.2022.04.021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129638986&doi=10.1016%2fj.csbj.2022.04.021&partnerID=40&md5=7a28a90f3af401db15cd9b81892b0dca","Over the past decade, polypharmacy instances have been common in multi-diseases treatment. However, unwanted drug-drug interactions (DDIs) that might cause unexpected adverse drug events (ADEs) in multiple regimens therapy remain a significant issue. Since artificial intelligence (AI) is ubiquitous today, many AI prediction models have been developed to predict DDIs to support clinicians in pharmacotherapy-related decisions. However, even though DDI prediction models have great potential for assisting physicians in polypharmacy decisions, there are still concerns regarding the reliability of AI models due to their black-box nature. Building AI models with explainable mechanisms can augment their transparency to address the above issue. Explainable AI (XAI) promotes safety and clarity by showing how decisions are made in AI models, especially in critical tasks like DDI predictions. In this review, a comprehensive overview of AI-based DDI prediction, including the publicly available source for AI-DDIs studies, the methods used in data manipulation and feature preprocessing, the XAI mechanisms to promote trust of AI, especially for critical tasks as DDIs prediction, the modeling methods, is provided. Limitations and the future directions of XAI in DDIs are also discussed. © 2022 The Author(s)","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85129638986"
"Deramgozin M.M.; Jovanovic S.; Arevalillo-Herráez M.; Rabah H.","Deramgozin, Mohammad Mahdi (57190847391); Jovanovic, Slavisa (24479758400); Arevalillo-Herráez, Miguel (6507791214); Rabah, Hassan (6508206988)","57190847391; 24479758400; 6507791214; 6508206988","An Explainable and Reliable Facial Expression Recognition System for Remote Health Monitoring","2022","ICECS 2022 - 29th IEEE International Conference on Electronics, Circuits and Systems, Proceedings","","","","","","","2","10.1109/ICECS202256217.2022.9971040","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145358907&doi=10.1109%2fICECS202256217.2022.9971040&partnerID=40&md5=e9e898fdb730a8af056eaaf4640b84a5","Remote Health Monitoring (RHM) appears as a promising solution for continuous and better healthcare, especially in the rural areas where the basic medical facilities are scarce or often out of reach. Generally, the main focus is on the monitoring of physiological signals (i.e. ECG, EEG) or activities. However, the overall emotional state of the remotely monitored patients is often underestimated due to the complexity of this task. Consequently, it is crucial to propose RHM-suitable technologies for explainable and reliable emotional state recognition. In this work, a free-position model to detect emotion and facial action units (FAU) is proposed. The proposed method has been validated on the public CK+ dataset and shows very promising results in terms of the explainability of the obtained results and performances, which are competitive in comparison with the state-of-the-art (an average accuracy of 93.4 % over all AUs). In addition, the lightweight architecture of the proposed method as well as its low number of parameters, in comparing with other FER models, make it suitable for resource-constrained embedded systems and open the ways for its wide adoption in remote health monitoring applications. © 2022 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85145358907"
"Holzinger A.; Goebel R.; Fong R.; Moon T.; Müller K.-R.; Samek W.","Holzinger, Andreas (23396282000); Goebel, Randy (7102516324); Fong, Ruth (57200621318); Moon, Taesup (56954364900); Müller, Klaus-Robert (15042362900); Samek, Wojciech (40762215900)","23396282000; 7102516324; 57200621318; 56954364900; 15042362900; 40762215900","xxAI - Beyond Explainable Artificial Intelligence","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13200 LNAI","","","3","10","7","16","10.1007/978-3-031-04083-2_1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128906717&doi=10.1007%2f978-3-031-04083-2_1&partnerID=40&md5=8cf203855f78513aff15d563922a3324","The success of statistical machine learning from big data, especially of deep learning, has made artificial intelligence (AI) very popular. Unfortunately, especially with the most successful methods, the results are very difficult to comprehend by human experts. The application of AI in areas that impact human life (e.g., agriculture, climate, forestry, health, etc.) has therefore led to an demand for trust, which can be fostered if the methods can be interpreted and thus explained to humans. The research field of explainable artificial intelligence (XAI) provides the necessary foundations and methods. Historically, XAI has focused on the development of methods to explain the decisions and internal mechanisms of complex AI systems, with much initial research concentrating on explaining how convolutional neural networks produce image classification predictions by producing visualizations which highlight what input patterns are most influential in activating hidden units, or are most responsible for a model’s decision. In this volume, we summarize research that outlines and takes next steps towards a broader vision for explainable AI in moving beyond explaining classifiers via such methods, to include explaining other kinds of models (e.g., unsupervised and reinforcement learning models) via a diverse array of XAI techniques (e.g., question-and-answering systems, structured explanations). In addition, we also intend to move beyond simply providing model explanations to directly improving the transparency, efficiency and generalization ability of models. We hope this volume presents not only exciting research developments in explainable AI but also a guide for what next areas to focus on within this fascinating and highly relevant research field as we enter the second decade of the deep learning revolution. This volume is an outcome of the ICML 2020 workshop on “XXAI: Extending Explainable AI Beyond Deep Models and Classifiers.” © 2022, The Author(s).","Conference paper","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85128906717"
"Bousquet C.; Beltramin D.","Bousquet, Cedric (8609880700); Beltramin, Diva (57720836300)","8609880700; 57720836300","Machine Learning in Medicine: To Explain, or Not to Explain, That Is the Question","2022","Studies in Health Technology and Informatics","294","","","114","115","1","0","10.3233/SHTI220407","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131107130&doi=10.3233%2fSHTI220407&partnerID=40&md5=f31d61f8edf5e3cf0c4fa9c3523c1aab","In 2022, the Medical Informatics Europe conference created a special topic called 'Challenges of trustable AI and added-value on health' which was centered around the theme of eXplainable Artificial Intelligence. Unfortunately, two opposite views remain for biomedical applications of machine learning: accepting to use reliable but opaque models, vs. enforce models to be explainable. In this contribution we discuss these two opposite approaches and illustrate with examples the differences between them.  © 2022 European Federation for Medical Informatics (EFMI) and IOS Press.","Conference paper","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85131107130"
"Katarya R.; Sharma P.; Soni N.; Rath P.","Katarya, Rahul (35810442400); Sharma, Parth (59067232400); Soni, Nishant (57773283700); Rath, Prithish (57772766100)","35810442400; 59067232400; 57773283700; 57772766100","A Review of Interpretable Deep Learning for Neurological Disease Classification","2022","8th International Conference on Advanced Computing and Communication Systems, ICACCS 2022","","","","900","906","6","2","10.1109/ICACCS54159.2022.9785321","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133211248&doi=10.1109%2fICACCS54159.2022.9785321&partnerID=40&md5=e3b70db0638cc7acd561e38f84adb8aa","While deep learning models have been able to boast high accuracies for healthcare applications, their actual clinical use has been limited due to their black-box nature. To tackle this problem, interpretability or explainability methods are used to help provide reasoning behind the decisions of a deep learning model as well as to gain insight into their reasoning process. This is necessary due to the high-stakes nature of healthcare applications, in which every decision made must be well informed. In this paper, we showcase the various techniques that are used for interpretability in neurological disease classification. We discuss the advantages and drawbacks of various methods used to generate explanations and also discuss how to evaluate these explanations. Finally, we also discuss some of the flaws in evaluation techniques and the future directions for this field. © 2022 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85133211248"
"Colodette A.L.; Filho F.N.B.; Pinasco G.C.; de Souza Cruz S.C.; Simões S.N.","Colodette, André Louzada (57767800600); Filho, Fabiano Novaes Barcellos (57221643800); Pinasco, Gustavo Carreiro (56607214300); de Souza Cruz, Sheila Cristina (57768041000); Simões, Sérgio Nery (55489191500)","57767800600; 57221643800; 56607214300; 57768041000; 55489191500","Feature Selection for Identification of Risk Factors Associated with Infant Mortality","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13254 LNBI","","","92","102","10","0","10.1007/978-3-031-17531-2_8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142697674&doi=10.1007%2f978-3-031-17531-2_8&partnerID=40&md5=b9fead25359c93b8d4dcc67b77121dd1","In the context of infant mortality risk analyses, the application of Machine Learning techniques, like Feature Selection, can be an efficient way to increase the interpretability of data and explanation of the studied phenomenon. In this paper, we developed a Machine Learning approach to identify the main risk factors that impact the local population studied with regard to infant mortality, aiming to help professionals who deal directly with the event or with the epidemiological guidelines that may be made available from data analysis. First, we integrated the databases of the Live Birth Information System (SINASC) and the Infant Mortality Information System (SIM), between 2006 and 2019, in the city of Vitória, ES, Brazil. Then, we used feature selection methods, such as SHAP, Feature_Importance and SelectKBest, to identify the main risk factors associated with infant mortality and we compared the results obtained from applying these algorithms with the most recent results of a 2018 meta-analysis. We observed that the results achieved by the methods, especially by the SHAP method, match the results of a literature meta-analysis, in which the factors that most influenced the final outcome of mortality were Weight, APGAR, Gestational Age and Presence of Anomalies. Therefore, the use of interpretability techniques, such as SHAP, are very promising for the selection and the identification of population risk factors related to infant mortality, by using existing databases without the need for new population studies and, in addition, this knowledge can be used to help in decision making for public health. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Conference paper","Final","","Scopus","2-s2.0-85142697674"
"Fang H.S.A.; Tan N.C.; Tan W.Y.; Oei R.W.; Lee M.L.; Hsu W.","Fang, Hao Sen Andrew (57225072191); Tan, Ngiap Chuan (7202697085); Tan, Wei Ying (57225072744); Oei, Ronald Wihal (57197808793); Lee, Mong Li (7409117252); Hsu, Wynne (7402002763)","57225072191; 7202697085; 57225072744; 57197808793; 7409117252; 7402002763","Patient similarity analytics for explainable clinical risk prediction","2021","BMC Medical Informatics and Decision Making","21","1","207","","","","6","10.1186/s12911-021-01566-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109064816&doi=10.1186%2fs12911-021-01566-y&partnerID=40&md5=2898663aeb44a102e761cc7702a7ac2b","Background: Clinical risk prediction models (CRPMs) use patient characteristics to estimate the probability of having or developing a particular disease and/or outcome. While CRPMs are gaining in popularity, they have yet to be widely adopted in clinical practice. The lack of explainability and interpretability has limited their utility. Explainability is the extent of which a model’s prediction process can be described. Interpretability is the degree to which a user can understand the predictions made by a model. Methods: The study aimed to demonstrate utility of patient similarity analytics in developing an explainable and interpretable CRPM. Data was extracted from the electronic medical records of patients with type-2 diabetes mellitus, hypertension and dyslipidaemia in a Singapore public primary care clinic. We used modified K-nearest neighbour which incorporated expert input, to develop a patient similarity model on this real-world training dataset (n = 7,041) and validated it on a testing dataset (n = 3,018). The results were compared using logistic regression, random forest (RF) and support vector machine (SVM) models from the same dataset. The patient similarity model was then implemented in a prototype system to demonstrate the identification, explainability and interpretability of similar patients and the prediction process. Results: The patient similarity model (AUROC = 0.718) was comparable to the logistic regression (AUROC = 0.695), RF (AUROC = 0.764) and SVM models (AUROC = 0.766). We packaged the patient similarity model in a prototype web application. A proof of concept demonstrated how the application provided both quantitative and qualitative information, in the form of patient narratives. This information was used to better inform and influence clinical decision-making, such as getting a patient to agree to start insulin therapy. Conclusions: Patient similarity analytics is a feasible approach to develop an explainable and interpretable CRPM. While the approach is generalizable, it can be used to develop locally relevant information, based on the database it searches. Ultimately, such an approach can generate a more informative CRPMs which can be deployed as part of clinical decision support tools to better facilitate shared decision-making in clinical practice. © 2021, The Author(s).","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85109064816"
"Lima G.; Grgić-Hlača N.; Jeong J.K.; Cha M.","Lima, Gabriel (57218770114); Grgić-Hlača, Nina (57203397997); Jeong, Jin Keun (57702796300); Cha, Meeyoung (24069708400)","57218770114; 57203397997; 57702796300; 24069708400","The Conflict Between Explainable and Accountable Decision-Making Algorithms","2022","ACM International Conference Proceeding Series","","","","2103","2113","10","10","10.1145/3531146.3534628","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133024125&doi=10.1145%2f3531146.3534628&partnerID=40&md5=633ab44fc4a193e0116171f1aada2932","Decision-making algorithms are being used in important decisions, such as who should be enrolled in health care programs and be hired. Even though these systems are currently deployed in high-stakes scenarios, many of them cannot explain their decisions. This limitation has prompted the Explainable Artificial Intelligence (XAI) initiative, which aims to make algorithms explainable to comply with legal requirements, promote trust, and maintain accountability. This paper questions whether and to what extent explainability can help solve the responsibility issues posed by autonomous AI systems. We suggest that XAI systems that provide post-hoc explanations could be seen as blameworthy agents, obscuring the responsibility of developers in the decision-making process. Furthermore, we argue that XAI could result in incorrect attributions of responsibility to vulnerable stakeholders, such as those who are subjected to algorithmic decisions (i.e., patients), due to a misguided perception that they have control over explainable algorithms. This conflict between explainability and accountability can be exacerbated if designers choose to use algorithms and patients as moral and legal scapegoats. We conclude with a set of recommendations for how to approach this tension in the socio-technical process of algorithmic decision-making and a defense of hard regulation to prevent designers from escaping responsibility. © 2022 ACM.","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85133024125"
"Wang X.; Ezeana C.F.; Wang L.; Puppala M.; Huang Y.-S.; He Y.; Yu X.; Yin Z.; Zhao H.; Lai E.C.; Wong S.T.C.","Wang, Xin (58024594000); Ezeana, Chika F. (57212165594); Wang, Lin (57089607100); Puppala, Mamta (57159997100); Huang, Yan-Siang (57215329794); He, Yunjie (57221499697); Yu, Xiaohui (57157756900); Yin, Zheng (14057439800); Zhao, Hong (55619309480); Lai, Eugene C. (8224839200); Wong, Stephen T. C. (24726534400)","58024594000; 57212165594; 57089607100; 57159997100; 57215329794; 57221499697; 57157756900; 14057439800; 55619309480; 8224839200; 24726534400","Risk factors and machine learning model for predicting hospitalization outcomes in geriatric patients with dementia","2022","Alzheimer's and Dementia: Translational Research and Clinical Interventions","8","1","e12351","","","","3","10.1002/trc2.12351","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145068383&doi=10.1002%2ftrc2.12351&partnerID=40&md5=c4d89d07520a4e000e7190c4bb84f560","Introduction: Geriatric patients with dementia incur higher healthcare costs and longer hospital stays than other geriatric patients. We aimed to identify risk factors for hospitalization outcomes that could be mitigated early to improve outcomes and impact overall quality of life. Methods: We identified risk factors, that is, demographics, hospital complications, pre-admission, and post-admission risk factors including medical history and comorbidities, affecting hospitalization outcomes determined by hospital stays and discharge dispositions. Over 150 clinical and demographic factors of 15,678 encounters (8407 patients) were retrieved from our institution's data warehouse. We further narrowed them down to twenty factors through feature selection engineering by using analysis of variance (ANOVA) and Glmnet. We developed an explainable machine-learning model to predict hospitalization outcomes among geriatric patients with dementia. Results: Our model is based on stacking ensemble learning and achieved accuracy of 95.6% and area under the curve (AUC) of 0.757. It outperformed prevalent methods of risk assessment for encounters of patients with Alzheimer's disease dementia (ADD) (4993), vascular dementia (VD) (4173), Parkinson's disease with dementia (PDD) (3735), and other unspecified dementias (OUD) (2777). Top identified hospitalization outcome risk factors, mostly from medical history, include encephalopathy, number of medical problems at admission, pressure ulcers, urinary tract infections, falls, admission source, age, race, anemia, etc., with several overlaps in multi-dementia groups. Discussion: Our model identified several predictive factors that can be modified or intervened so that efforts can be made to prevent recurrence or mitigate their adverse effects. Knowledge of the modifiable risk factors would help guide early interventions for patients at high risk for poor hospitalization outcome as defined by hospital stays longer than seven days, undesirable discharge disposition, or both. The interventions include starting specific protocols on modifiable risk factors like encephalopathy, falls, and infections, where non-existent or not routine, to improve hospitalization outcomes of geriatric patients with dementia. Highlights: A total 15,678 encounters of Geriatrics with dementia with a final 20 risk factors. Developed a predictive model for hospitalization outcomes for multi-dementia types. Risk factors for each type were identified including those amenable to interventions. Top factors are encephalopathy, pressure ulcers, urinary tract infection (UTI), falls, and admission source. With accuracy of 95.6%, our ensemble predictive model outperforms other models. © 2022 The Authors. Alzheimer's & Dementia: Translational Research & Clinical Interventions published by Wiley Periodicals LLC on behalf of Alzheimer's Association.","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85145068383"
"Pfeuffer N.","Pfeuffer, Nicolas (57210111932)","57210111932","Design Principles for (X)AI-based Patient Education Systems","2022","Lecture Notes in Informatics (LNI), Proceedings - Series of the Gesellschaft fur Informatik (GI)","P-319","","","143","157","14","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136257097&partnerID=40&md5=50c49bb8e29bf6ebb52935f1211018c3","Recently, the management of chronic diseases has advanced to a prime topic for Information Systems (IS) research and practice. With increasing capability of Information Technology, patients are empowered to engage in self-management of chronic diseases connected to promises of health benefits for the individual as well as an unburdening of clinics and economic advantages for health care systems. Nevertheless, patients must be adequately educated about risks, screening and examination options to make patient self-management effective, sustainable and profitable. In this regard, Explainable Artificial Intelligence ((X)AI)-based Patient Education Systems (PES) may be an opportunity to provide patient education in an interactive, intelligible and intelligent manner. By establishing Design Principles (DP) for the engineering of effective (X)AIbased PES, instantiating them in a system prototype and evaluating the DP with the help of general practitioners, this paper contributes to the body of knowledge in designing health IS. © 2022 Gesellschaft fur Informatik (GI). All rights reserved.","Conference paper","Final","","Scopus","2-s2.0-85136257097"
"Oyeniyi O.; Dhandhukia S.S.; Sen A.; Fletcher K.K.","Oyeniyi, Oluwafeyisayo (57868078100); Dhandhukia, Shreyansh Sandip (57868418900); Sen, Amartya (56732361700); Fletcher, Kenneth K. (50861085700)","57868078100; 57868418900; 56732361700; 50861085700","A Study of Artificial Intelligence Frameworks and Their Capability to Diagnose Major Depressive Disorder","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13236 LNCS","","","3","17","14","0","10.1007/978-3-031-14135-5_1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137048853&doi=10.1007%2f978-3-031-14135-5_1&partnerID=40&md5=c06163e1576dcf040446329586e6bf8a","The accurate diagnosis of mental illness is challenging because mental illness does not result in evident physical symptoms as compared to physical illness like the common cold. As a result, no definitive medical tests exist for mental illnesses. This situation is further aggravated by the fact that many of the symptoms of various mental illnesses overlap. Further, traditional means of mental care and therapy are not easily accessible to a majority of the population in developed and developing countries alike. In addition, openly discussing mental illness in major parts of society is still considered taboo. Therefore, a plausible way to improve mental illness diagnosis and address the aforementioned challenges is by using Artificial Intelligence (AI). This paper presents a comprehensive survey of AI-enabled approaches to Major Depressive Disorder (MDD) diagnosis and outlines some future research directions and challenges in this field. The paper also presents a preliminary system architecture of an AI-enabled approach to diagnose mental health with the objectives of making the underlying system more user-centric, scalable and accessible. © 2022, Springer Nature Switzerland AG.","Conference paper","Final","","Scopus","2-s2.0-85137048853"
"Dhaliwal B.S.; Imran R.; Leung C.K.; R. Madill E.W.","Dhaliwal, Bikramjit Singh (57974737900); Imran, Rayan (58065080700); Leung, Carson K. (7402612526); R. Madill, Evan W. (58066047000)","57974737900; 58065080700; 7402612526; 58066047000","Trustworthy Explanations for Knowledge Discovered from E-Health Records","2022","2022 IEEE International Conference on E-Health Networking, Application and Services, HealthCom 2022","","","","246","251","5","5","10.1109/HealthCom54947.2022.9982786","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146296978&doi=10.1109%2fHealthCom54947.2022.9982786&partnerID=40&md5=601c943f6c326f6fe640cb9dc300f1eb","In the current era of big data, very large amounts of data are generating at a rapid rate from a wide variety of rich data sources. Electronic health (e-health) records are examples of the big data. With the technological advancements, more healthcare practice has gradually been supported by electronic processes and communication. This enables health informatics, in which computer science meets the healthcare sector to address healthcare and medical problems. Embedded in the big data are valuable information and knowledge that can be discovered by data science, data mining and machine learning techniques. Many of these techniques apply ""opaque box""approaches to make accurate predictions. However, these techniques may not be crystal clear to the users. As the users not necessarily be able to clearly view the entire knowledge discovery (e.g., prediction) process, they may not easily trust the discovered knowledge (e.g., predictions). Hence, in this paper, we present a system for providing trustworthy explanations for knowledge discovered from e-health records. Specifically, our system provides users with global explanations for the important features among the records. It also provides users with local explanations for a particular record. Evaluation results on real-life e-health records show the practicality of our system in providing trustworthy explanations to knowledge discovered (e.g., accurate predictions made).  © 2022 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85146296978"
"Chen L.; Sheu J.-T.; Tsao Y.; Chuang Y.-J.","Chen, Lichin (57219860758); Sheu, Ji-Tian (44861715400); Tsao, Yu (13608047100); Chuang, Yuh-Jue (7202686957)","57219860758; 44861715400; 13608047100; 7202686957","Deep Learning and Explainable Artificial Intelligence to Predict Patients' Choice of Hospital Levels in Urban and Rural Areas","2022","Studies in Health Technology and Informatics","290","","","734","738","4","1","10.3233/SHTI220175","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131484509&doi=10.3233%2fSHTI220175&partnerID=40&md5=43eb5cef36626075d3b8cd266576852e","Maldistribution of healthcare resources among urban and rural areas is a significant challenge worldwide. People living in rural areas may have limited access to medical resources, and often neglect their health problems or receive insufficient care services. This research uses a deep learning approach to predict patient choices regarding hospital levels (primary, secondary or tertiary hospitals) and interpret the model decision using explainable artificial intelligence. We proposed an autoencoder-deep neural network framework and trained region-based models for the urban and rural areas. The models achieve an area under the receiver operating characteristics curve (AUC) of 0.94 and 0.95, and an accuracy of 0.93 and 0.92 for the urban and rural areas, respectively. This result indicates that region-based models are effective in improving the performance. The result is potentially leading to appropriate policy planning. Further interpretation can be done to investigate the explicit differentiation of the rural and urban scenarios. © 2022 International Medical Informatics Association (IMIA) and IOS Press.","Conference paper","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85131484509"
"Morales A.S.; de Oliveira Ourique F.; Morás L.D.; Cazella S.C.","Morales, Analúcia Schiaffino (6603371672); de Oliveira Ourique, Fabrício (57925917900); Morás, Laura Derengoski (57571764300); Cazella, Silvio César (8840637800)","6603371672; 57925917900; 57571764300; 8840637800","Exploring Interpretable Machine Learning Methods and Biomarkers to Classifying Occupational Stress of the Health Workers","2022","Intelligent Systems Reference Library","121","","","105","124","19","4","10.1007/978-3-030-97516-6_6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127987086&doi=10.1007%2f978-3-030-97516-6_6&partnerID=40&md5=054c9843b9db9db073ceddfab952b383","Occupational stress is present in all professions but poses a high risk to health professionals’ performance when it becomes bad stress (distress). This chapter presents issues related to monitoring health professionals’ stress through wearable devices, focusing on research into the use of biomarkers and machine learning techniques used in the development of models that can help in decision-making related to coping with distress. Results from a literature review on these topics are reported. Challenges related to explainable Artificial Intelligence are addressed in the chapter, and challenges associated with the definition of a stress classification at different levels seek that, to identify the impacts on the health of the health professional. As the main contribution, this chapter presents a proposal for an intelligent system, the result of ongoing research, which helps in recommending actions according to the level of perceived stress, in an explainable, transparent way that is reliable for adoption by these professionals' managers. Challenges related to the intelligent system implementation itself, such as the security and privacy of user information in the foreseen layers of the Health Internet of Things solution architecture, are described for reflection. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Book chapter","Final","","Scopus","2-s2.0-85127987086"
"","","","2022 International Conference on Future Trends in Smart Communities, ICFTSC 2022","2022","2022 International Conference on Future Trends in Smart Communities, ICFTSC 2022","","","","","","251","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149122654&partnerID=40&md5=f5a9e7fdc29507907b5e00b1a0d3811e","The proceedings contain 47 papers. The topics discussed include: adopting electrical remote operation and intelligent-based system at offshore installation; anomaly prediction and remaining useful life for alternator health monitoring system; comparative analysis of different parameters used for optimization in the process of speaker and speech recognition using deep neural network; concept drift scenarios in electrical load forecasting with different generation modalities; design and development of an estimate controller for a fusion tank system; design of miniaturized s-band high isolation strip-line circulator for 5G application; effect of thickness of the poly(vinyl alcohol) passivation layer on ambipolar characteristics of graphene field-effect transistor; emotion based chatbot using deep learning; energy management algorithm in wireless sensor network for pipeline monitoring; and explainable artificial intelligence applied to deep reinforcement learning controllers for photovoltaic maximum power point tracking.","Conference review","Final","","Scopus","2-s2.0-85149122654"
"Raab D.; Fezer E.; Breitenbach J.; Baumgartl H.; Sauter D.; Buettner R.","Raab, Dominik (57218408547); Fezer, Eric (57220833918); Breitenbach, Johannes (57220835600); Baumgartl, Hermann (57204587967); Sauter, Daniel (57204584246); Buettner, Ricardo (24831738000)","57218408547; 57220833918; 57220835600; 57204587967; 57204584246; 24831738000","A Deep Learning-Based Model for Automated Quality Control in the Pharmaceutical Industry","2022","Proceedings - 2022 IEEE 46th Annual Computers, Software, and Applications Conference, COMPSAC 2022","","","","266","271","5","2","10.1109/COMPSAC54236.2022.00045","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136994647&doi=10.1109%2fCOMPSAC54236.2022.00045&partnerID=40&md5=e7c3f382e54be6d9a577cabd2cdbec75","For highly sensitive products such as pharmaceuticals, quality is a decisive factor in ensuring the therapeutic benefit that consumers expect and not jeopardizing consumers' health. So far, the quality control of pharmaceuticals is largely performed manually by qualified individuals. However, this is a time-consuming, repetitive, and error-prone process subject to natural performance fluctuations. To contribute to addressing this issue, we present an automated quality control approach for pharmaceutical capsules using a transfer learning-based convolutional neural network with a balanced accuracy of 97.27%, outperforming all current benchmarks. To increase trust in the model predictions, we incorporated two explainable artificial intelligence (XAI) methods into our approach. © 2022 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85136994647"
"Saraswat D.; Bhattacharya P.; Verma A.; Prasad V.K.; Tanwar S.; Sharma G.; Bokoro P.N.; Sharma R.","Saraswat, Deepti (57224616778); Bhattacharya, Pronaya (57200306370); Verma, Ashwin (57221908056); Prasad, Vivek Kumar (57196721184); Tanwar, Sudeep (56576145100); Sharma, Gulshan (57216326306); Bokoro, Pitshou N. (56336233900); Sharma, Ravi (57431604600)","57224616778; 57200306370; 57221908056; 57196721184; 56576145100; 57216326306; 56336233900; 57431604600","Explainable AI for Healthcare 5.0: Opportunities and Challenges","2022","IEEE Access","10","","","84486","84517","31","88","10.1109/ACCESS.2022.3197671","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136138045&doi=10.1109%2fACCESS.2022.3197671&partnerID=40&md5=db4c61445dd20c88236fe959fca1091d","In the healthcare domain, a transformative shift is envisioned towards Healthcare 5.0. It expands the operational boundaries of Healthcare 4.0 and leverages patient-centric digital wellness. Healthcare 5.0 focuses on real-time patient monitoring, ambient control and wellness, and privacy compliance through assisted technologies like artificial intelligence (AI), Internet-of-Things (IoT), big data, and assisted networking channels. However, healthcare operational procedures, verifiability of prediction models, resilience, and lack of ethical and regulatory frameworks are potential hindrances to the realization of Healthcare 5.0. Recently, explainable AI (EXAI) has been a disruptive trend in AI that focuses on the explainability of traditional AI models by leveraging the decision-making of the models and prediction outputs. The explainability factor opens new opportunities to the black-box models and brings confidence in healthcare stakeholders to interpret the machine learning (ML) and deep learning (DL) models. EXAI is focused on improving clinical health practices and brings transparency to the predictive analysis, which is crucial in the healthcare domain. Recent surveys on EXAI in healthcare have not significantly focused on the data analysis and interpretation of models, which lowers its practical deployment opportunities. Owing to the gap, the proposed survey explicitly details the requirements of EXAI in Healthcare 5.0, the operational and data collection process. Based on the review method and presented research questions, systematically, the article unfolds a proposed architecture that presents an EXAI ensemble on the computerized tomography (CT) image classification and segmentation process. A solution taxonomy of EXAI in Healthcare 5.0 is proposed, and operational challenges are presented. A supported case study on electrocardiogram (ECG) monitoring is presented that preserves the privacy of local models via federated learning (FL) and EXAI for metric validation. The case-study is supported through experimental validation. The analysis proves the efficacy of EXAI in health setups that envisions real-life model deployments in a wide range of clinical applications. © 2013 IEEE.","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85136138045"
"Selvam R.P.; Oliver A.S.; Mohan V.; Prakash N.B.; Jayasankar T.","Selvam, R. Pandi (57575841500); Oliver, A. Sheryl (56826151700); Mohan, V. (57548441500); Prakash, N.B. (56490089000); Jayasankar, T. (56826107100)","57575841500; 56826151700; 57548441500; 56490089000; 56826107100","Explainable Artificial Intelligence with Metaheuristic Feature Selection Technique for Biomedical Data Classification","2022","Intelligent Systems Reference Library","222","","","43","57","14","3","10.1007/978-981-19-1476-8_4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128186666&doi=10.1007%2f978-981-19-1476-8_4&partnerID=40&md5=7e3f33001bd9ab171908c196750c7d84","Recently, explainable artificial intelligence (XAI) becomes a hot research topic due to its decision-making abilities in several real-time applications, particularly health care. The application of XAI approaches can be used to investigate the biomedical data for effective disease diagnosis and classification. At the same time, the high dimensionality of the healthcare data poses a curse of dimensionality problem which can be solved by the design of optimization algorithms. Therefore, this paper introduces an XAI with feature selection technique for biomedical data classification (XAIMFS-BMC). The proposed XAIMFS-BMC technique intends to proficiently categorize the biomedical data into distinct classes. In addition, the XAIMFS-BMC technique involves the design of chaotic spider monkey optimization (CSMO) algorithm for effective selection of feature subsets. Moreover, the deep neural network (DNN) is exploited for medical data classification, and its efficiency can be further improved by the use of Nadam-optimizer-based hyperparameter tuning process. The performance validation of the XAIMFS-BMC technique is tested using distinct benchmark medical dataset, and the results are inspected under several aspects. The comparative results reported the supremacy of the XAIMFS-BMC technique over the other techniques in terms of different performance measures. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Book chapter","Final","","Scopus","2-s2.0-85128186666"
"Jakubowski J.; Stanisz P.; Bobek S.; Nalepa G.J.","Jakubowski, Jakub (57395510700); Stanisz, Przemysław (57148197300); Bobek, Szymon (49661044900); Nalepa, Grzegorz J. (55879229400)","57395510700; 57148197300; 49661044900; 55879229400","Anomaly detection in asset degradation process using variational autoencoder and explanations","2022","Sensors","22","1","291","","","","20","10.3390/s22010291","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122210756&doi=10.3390%2fs22010291&partnerID=40&md5=0dcbb4818a260c1798d78b077d8e01ef","Development of predictive maintenance (PdM) solutions is one of the key aspects of Industry 4.0. In recent years, more attention has been paid to data-driven techniques, which use machine learning to monitor the health of an industrial asset. The major issue in the implementation of PdM models is a lack of good quality labelled data. In the paper we present how unsupervised learning using a variational autoencoder may be used to monitor the wear of rolls in a hot strip mill, a part of a steel-making site. As an additional benchmark we use a simulated turbofan engine data set provided by NASA. We also use explainability methods in order to understand the model’s predictions. The results show that the variational autoencoder slightly outperforms the base autoencoder architecture in anomaly detection tasks. However, its performance on the real use-case does not make it a production-ready solution for industry and should be a matter of further research. Furthermore, the information obtained from the explainability model can increase the reliability of the proposed artificial intelligence-based solution. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85122210756"
"Zhu H.; Yu C.; Cangelosi A.","Zhu, Hongbo (58138332800); Yu, Chuang (57944376500); Cangelosi, Angelo (6701387796)","58138332800; 57944376500; 6701387796","Affective Human-Robot Interaction with Multimodal Explanations","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13817 LNAI","","","241","252","11","1","10.1007/978-3-031-24667-8_22","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149816463&doi=10.1007%2f978-3-031-24667-8_22&partnerID=40&md5=6442a4a7cddc8ed3e9909d9785113ca8","Facial expressions are one of the most practical and straightforward ways to communicate emotions. Facial Expression Recognition has been used in lots of fields such as human behaviour understanding and health monitoring. Deep learning models can achieve excellent performance in facial expression recognition tasks. As these deep neural networks have very complex nonlinear structures, when the model makes a prediction, it is not easy for human users to understand what is the basis for the model’s prediction. Specifically, we do not know which facial units contribute to the classification more or less. Developing affective computing models with more explainable and transparent feedback for human interactors is essential for a trustworthy human-robot interaction. Compared to “white-box” approaches, “black-box” approaches using deep neural networks, which have advantages in terms of overall accuracy but lack reliability and explainability. In this work, we introduce a multimodal affective human-robot interaction framework, with visual-based and verbal-based explanation, by Layer-Wise Relevance Propagation (LRP) and Local Interpretable Mode-Agnostic Explanation (LIME). The proposed framework has been tested on the KDEF dataset, and in human-robot interaction experiments with the Pepper robot. This experimental evaluation shows the benefits of linking deep learning emotion recognition systems with explainable strategies. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2022.","Conference paper","Final","","Scopus","2-s2.0-85149816463"
"Stojić A.; Jovanović G.; Stanišić S.; Romanić S.H.; Šoštarić A.; Udovičić V.; Perišić M.; Milićević T.","Stojić, Andreja (28168051600); Jovanović, Gordana (57225371381); Stanišić, Svetlana (56358144400); Romanić, Snježana Herceg (12778504400); Šoštarić, Andrej (6507724941); Udovičić, Vladimir (6602215128); Perišić, Mirjana (58354176300); Milićević, Tijana (56943324000)","28168051600; 57225371381; 56358144400; 12778504400; 6507724941; 6602215128; 58354176300; 56943324000","The PM2.5-bound polycyclic aromatic hydrocarbon behavior in indoor and outdoor environments, part II: Explainable prediction of benzo[a]pyrene levels","2022","Chemosphere","289","","133154","","","","6","10.1016/j.chemosphere.2021.133154","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120639864&doi=10.1016%2fj.chemosphere.2021.133154&partnerID=40&md5=4f228ed9352932c9ca516e6a65a11903","Among the polycyclic aromatic hydrocarbons (PAH), benzo[a]pyrene (B[a]P) has been considered more relevant than other species when estimating the potential exposure-related health effects and has been recognized as a marker of carcinogenic potency of air pollutant mixture. The current understanding of the factors which govern non-linear behavior of B[a]P and associated pollutants and environmental processes is insufficient and further research has to rely on the advanced analytical approach which averts the assumptions and avoids simplifications required by linear modeling methods. For the purpose of this study, we employed eXtreme Gradient Boosting (XGBoost), SHapley Additive exPlanations (SHAP) attribution method, and SHAP value fuzzy clustering to investigate the concentrations of inorganic gaseous pollutants, radon, PM2.5 and particle constituents including trace metals, ions, 16 US EPA priority PM2.5-bound PAHs and 31 meteorological variables, as key factors which shape indoor and outdoor PM2.5-bound B[a]P distribution in a university building located in the urban area of Belgrade (Serbia). According to the results, the indoor and outdoor B[a]P levels were shown to be highly correlated and mostly influenced by the concentrations of Chry, B[b]F, CO, B[a]A, I[cd]P, B[k]F, Flt, D[ah]A, Pyr, B[ghi]P, Cr, As, and PM2.5 in both indoor and outdoor environments. Besides, high B[a]P concentration events were recorded during the periods of low ambient temperature (<12 °C), unstable weather conditions with precipitation and increased soil humidity. © 2021 Elsevier Ltd","Article","Final","","Scopus","2-s2.0-85120639864"
"Uddin M.Z.; Soylu A.","Uddin, Md Zia (24482836700); Soylu, Ahmet (35243744400)","24482836700; 35243744400","Human activity recognition using wearable sensors, discriminant analysis, and long short-term memory-based neural structured learning","2021","Scientific Reports","11","1","16455","","","","64","10.1038/s41598-021-95947-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112571882&doi=10.1038%2fs41598-021-95947-y&partnerID=40&md5=f52f341f91f90a742e3b5599fcf027d2","Healthcare using body sensor data has been getting huge research attentions by a wide range of researchers because of its good practical applications such as smart health care systems. For instance, smart wearable sensor-based behavior recognition system can observe elderly people in a smart eldercare environment to improve their lifestyle and can also help them by warning about forthcoming unprecedented events such as falls or other health risk, to prolong their independent life. Although there are many ways of using distinguished sensors to observe behavior of people, wearable sensors mostly provide reliable data in this regard to monitor the individual’s functionality and lifestyle. In this paper, we propose a body sensor-based activity modeling and recognition system using time-sequential information-based deep Neural Structured Learning (NSL), a promising deep learning algorithm. First, we obtain data from multiple wearable sensors while the subjects conduct several daily activities. Once the data is collected, the time-sequential information then go through some statistical feature processing. Furthermore, kernel-based discriminant analysis (KDA) is applied to see the better clustering of the features from different activity classes by minimizing inner-class scatterings while maximizing inter-class scatterings of the samples. The robust time-sequential features are then applied with Neural Structured Learning (NSL) based on Long Short-Term Memory (LSTM), for activity modeling. The proposed approach achieved around 99% recall rate on a public dataset. It is also compared to existing different conventional machine learning methods such as typical Deep Belief Network (DBN), Convolutional Neural Network (CNN), and Recurrent Neural Network (RNN) where they yielded the maximum recall rate of 94%. Furthermore, a fast and efficient explainable Artificial Intelligence (XAI) algorithm, Local Interpretable Model-Agnostic Explanations (LIME) is used to explain and check the machine learning decisions. The robust activity recognition system can be adopted for understanding peoples' behavior in their daily life in different environments such as homes, clinics, and offices. © 2021, The Author(s).","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85112571882"
"Ayidzoe M.A.; Yu Y.; Mensah P.K.; Cai J.; Bawah F.U.","Ayidzoe, Mighty Abra (57216702958); Yu, Yongbin (16320294000); Mensah, Patrick Kwabena (57219697274); Cai, Jingye (10738940600); Bawah, Faiza Umar (57369051400)","57216702958; 16320294000; 57219697274; 10738940600; 57369051400","Visual Interpretability of Capsule Network for Medical Image analysis","2022","Turkish Journal of Electrical Engineering and Computer Sciences","30","3","","978","995","17","3","10.3906/ELK-2107-146","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128304982&doi=10.3906%2fELK-2107-146&partnerID=40&md5=dfc127b7e96f3972b0fe137dcc151fda","Deep learning (DL) models are currently not widely deployed for critical tasks such as in health. This is attributable to the”black box,” making it difficult to gain the trust of practitioners. This paper proposes the use of visualizations to enhance performance verification, improve monitoring, ensure understandability, and improve interpretability needed to gain practitioners' confidence. These are demonstrated through the development of a CapsNet model for the recognition of gastrointestinal tract infection. The gastrointestinal tract comprises several organs joined in a long tube from the mouth to the anus. It is susceptive to diseases that are difficult for medics to diagnose, since it is not easy to have physical access to the sick regions. Consequently, manual access and analysis of images of the unhealthy parts requires the skills of an expert, as it is tedious, prone to errors, and costly. Experimental results show that visualizations in the form of post-hoc interpretability can demonstrate the reliability and interpretability of the CapsNet model applied to gastrointestinal tract diseases. The outputs can also be explained to gain practitioners' confidence in paving the way for its adoption in critical areas of society. © TÜBİTAK","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85128304982"
"Fernandez-Llatas C.; Martin N.; Johnson O.; Sepulveda M.; Helm E.; Munoz-Gama J.","Fernandez-Llatas, Carlos (57204151458); Martin, Niels (56559224800); Johnson, Owen (14822120300); Sepulveda, Marcos (7005859415); Helm, Emmanuel (54791996100); Munoz-Gama, Jorge (36603094000)","57204151458; 56559224800; 14822120300; 7005859415; 54791996100; 36603094000","Building Process-Oriented Data Science Solutions for Real-World Healthcare","2022","International Journal of Environmental Research and Public Health","19","14","8427","","","","0","10.3390/ijerph19148427","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135130345&doi=10.3390%2fijerph19148427&partnerID=40&md5=fdd049ecdfa93950c5bb256f95851048","The COVID-19 pandemic has highlighted some of the opportunities, problems and barriers facing the application of Artificial Intelligence to the medical domain. It is becoming increasingly important to determine how Artificial Intelligence will help healthcare providers understand and improve the daily practice of medicine. As a part of the Artificial Intelligence research field, the Process-Oriented Data Science community has been active in the analysis of this situation and in identifying current challenges and available solutions. We have identified a need to integrate the best efforts made by the community to ensure that promised improvements to care processes can be achieved in real healthcare. In this paper, we argue that it is necessary to provide appropriate tools to support medical experts and that frequent, interactive communication between medical experts and data miners is needed to co-create solutions. Process-Oriented Data Science, and specifically concrete techniques such as Process Mining, can offer an easy to manage set of tools for developing understandable and explainable Artificial Intelligence solutions. Process Mining offers tools, methods and a data driven approach that can involve medical experts in the process of co-discovering real-world evidence in an interactive way. It is time for Process-Oriented Data scientists to collaborate more closely with healthcare professionals to provide and build useful, understandable solutions that answer practical questions in daily practice. With a shared vision, we should be better prepared to meet the complex challenges that will shape the future of healthcare. © 2022 by the authors.","Editorial","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85135130345"
"Lo Giudice M.; Mammone N.; Ieracitano C.; Aguglia U.; Mandic D.; Morabito F.C.","Lo Giudice, Michele (57219550534); Mammone, Nadia (55915193500); Ieracitano, Cosimo (57192383001); Aguglia, Umberto (57221565489); Mandic, Danilo (7006513328); Morabito, Francesco Carlo (7103166505)","57219550534; 55915193500; 57192383001; 57221565489; 7006513328; 7103166505","Explainable Deep Learning Classification of Respiratory Sound for Telemedicine Applications","2022","Communications in Computer and Information Science","1724 CCIS","","","391","403","12","1","10.1007/978-3-031-24801-6_28","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149706495&doi=10.1007%2f978-3-031-24801-6_28&partnerID=40&md5=64010d8c30eb90df848d19e08b24c492","The recent pandemic crisis combined with the explosive growth of Artificial Intellignence (AI) algorithms has highlighted the potential benefits of telemedicine for decentralised, accurate and automated clinical diagnoses. One of the most popular and essential diagnoses is the auscultation; it is non-invasive, real-time and very informative diagnoses for knowing the state of the respiratory system. To implement a possible automated auscultation analysis, the decision-making explanation of complex models (such as Deep Learning models) is crucial for trusted application in the clinical domain. In this context, we will analyse the behaviour of a Convolutional Neural Network (CNN) in classifying the largest publicly available database of respiratory sounds, originally compiled to support the scientific challenge organized at Int. Conf. on Biomedical Health Informatics (ICBHI17). It contains respiratory sounds (recorded with auscultation) of normal respiratory cycles, crackles, wheezes and both. To capture the phonetically important features of breath sounds, the Mel-Frequency Cepstrum (MFC) for short-term power spectrum representation was applied. The MFC allowed us to identify latent features without losing the temporal information so that we could easily identify the correspondence of the features to the starting sound. The MFCs were used as input to the proposed CNN who was able to classify the four above-mentioned respiratory classes with an accuracy of 72.8%. Despite interesting results, the main focus of the present study was to investigate how the CNN achieved this classification. The explainable Artificial Intelligence (xAI) technique of Gradient-weighted Class Activation Mapping (Grad-CAM) was applied. xAI made it possible to visually identify the most relevant areas, especially for the recognition of abnormal sounds, which is crucial for inspecting the correct learning of the CNN. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Conference paper","Final","","Scopus","2-s2.0-85149706495"
"Merry M.; Riddle P.; Warren J.","Merry, Michael (57202972064); Riddle, Pat (8948904700); Warren, Jim (7402212366)","57202972064; 8948904700; 7402212366","A mental models approach for defining explainable artificial intelligence","2021","BMC Medical Informatics and Decision Making","21","1","344","","","","13","10.1186/s12911-021-01703-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121008299&doi=10.1186%2fs12911-021-01703-7&partnerID=40&md5=be8ef542fc74cf408e8449b457ccd16e","Background: Wide-ranging concerns exist regarding the use of black-box modelling methods in sensitive contexts such as healthcare. Despite performance gains and hype, uptake of artificial intelligence (AI) is hindered by these concerns. Explainable AI is thought to help alleviate these concerns. However, existing definitions for explainable are not forming a solid foundation for this work. Methods: We critique recent reviews on the literature regarding: the agency of an AI within a team; mental models, especially as they apply to healthcare, and the practical aspects of their elicitation; and existing and current definitions of explainability, especially from the perspective of AI researchers. On the basis of this literature, we create a new definition of explainable, and supporting terms, providing definitions that can be objectively evaluated. Finally, we apply the new definition of explainable to three existing models, demonstrating how it can apply to previous research, and providing guidance for future research on the basis of this definition. Results: Existing definitions of explanation are premised on global applicability and don’t address the question ‘understandable by whom?’. Eliciting mental models can be likened to creating explainable AI if one considers the AI as a member of a team. On this basis, we define explainability in terms of the context of the model, comprising the purpose, audience, and language of the model and explanation. As examples, this definition is applied to regression models, neural nets, and human mental models in operating-room teams. Conclusions: Existing definitions of explanation have limitations for ensuring that the concerns for practical applications are resolved. Defining explainability in terms of the context of their application forces evaluations to be aligned with the practical goals of the model. Further, it will allow researchers to explicitly distinguish between explanations for technical and lay audiences, allowing different evaluations to be applied to each. © 2021, The Author(s).","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85121008299"
"Hellen N.; Marvin G.","Hellen, Nakayiza (57385781800); Marvin, Ggaliwango (57302525500)","57385781800; 57302525500","Explainable AI for Safe Water Evaluation for Public Health in Urban Settings","2022","2022 International Conference on Innovations in Science, Engineering and Technology, ICISET 2022","","","","227","232","5","6","10.1109/ICISET54810.2022.9775912","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131719434&doi=10.1109%2fICISET54810.2022.9775912&partnerID=40&md5=a4dcf3ebfb41216665d1956e3419395c","With the rapid improvement of human standards, the development of urban centers has significantly contributed to water contamination and environmental pollution hence compromising the safety of drinking water for public health. The ecological safety and human health have continuously lowered due to hazardous pollution factors like chemicals and pathogens. Different algorithms and blackbox Machine Learning models have been used to develop water quality/safety criteria. This work presents an Explainable Artificial Intelligence method, SHAP (SHapley Additive exPlanations) to transparently and explainably assess the most important metrics that these models use in determining water quality. The simulated results will provide theoretical support to policy makers on how to maintain water quality or safety within urban areas and improve pollution control, water and ecological management.  © 2022 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85131719434"
"Xu X.; Yan X.","Xu, Xiaoqing (57224501154); Yan, Xiangyuan (57189685482)","57224501154; 57189685482","A Convenient and Reliable Multi-Class Classification Model based on Explainable Artificial Intelligence for Alzheimer's Disease","2022","2022 IEEE International Conference on Advances in Electrical Engineering and Computer Applications, AEECA 2022","","","","671","675","4","2","10.1109/AEECA55500.2022.9918895","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141363475&doi=10.1109%2fAEECA55500.2022.9918895&partnerID=40&md5=3188b47e4ae0f1ef43bda929edcfc6ce","There is no cure for Alzheimer's disease (AD) yet. The best treatment available is early detection and early intervention. However, most patients have not been detected in time, let alone timely intervention. So, the World Health Organization (WHO) calls for an increase in the screening rate of AD. Cognitive scores (CS) have great potential in the detection of AD and are easy to obtain, but there are fewer researches based only on CS for classification for screening. Current machine learning models often have good performance but are non-transparent, which makes them difficult to be accepted by physicians. To address the above problems, we propose a solution (RN-SSAS) for data sets with small sample size, multi-category and category-imbalanced, and prove that it is more robust than Cross-Validation (CV), more importantly, it can still achieve classifications under extreme data dilemmas. Then, based on the solution and CS, a multi-class classification model for AD is established, supplemented by single and global instance interpretations based on SHapley Additive exPlanations (SHAP). Finally, our model achieves an F-measure of 0.878 based on four different cognitive scores. In conclusion, our model is convenient, reliable and trustworthy, thus it can contribute to improving the screening rate of AD.  © 2022 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85141363475"
"Laxmi Lydia E.; Anupama C.S.S.; Sharmili N.","Laxmi Lydia, E. (57196059278); Anupama, C.S.S. (57207696309); Sharmili, N. (57191575400)","57196059278; 57207696309; 57191575400","Optimal Boosting Label Weighting Extreme Learning Machine for Mental Disorder Prediction and Classification","2022","Intelligent Systems Reference Library","222","","","1","15","14","0","10.1007/978-981-19-1476-8_1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128217407&doi=10.1007%2f978-981-19-1476-8_1&partnerID=40&md5=9df8c2d89a42973cf38e55ffb06e5ed3","Explainable artificial intelligence (XAI) becomes a hot research topic in the domain of biomedical and healthcare applications. Owing to the benefits of handling massive and complicated data, XAI concept finds useful in several applications, particularly health care. With the developments of machine learning (ML) and XAI, healthcare service quality can be considerably improved. This article designs an optimal boosting label weighting extreme learning machine for mental disorder prediction and classification (OBWELM-MDC) technique. The goal of the OBWELM-MDC technique is to determine the different levels of DAS. In addition, the OBWELM-MDC technique involves the design of boosting label weighted extreme learning machine (BWELM) model for prediction process. Besides, the BWELM model can be derived by the incorporation of the label weighted extreme learning machine (LW-ELM) with boosted ensemble learning model. Moreover, the parameter tuning of the BWELM model takes place by the use of chaotic starling particle swarm optimization (CSPSO), where the inertia weight and acceleration coefficient of the PSO algorithm are modified via logistic chaotic map. The application of CSPSO algorithm has improved the predictive performance of the BWELM model. The experimental result analysis of the OBWELM-MDC technique takes place using benchmark dataset, and the results are examined under several measures. The experimental results showcased that OBWELM-MDC technique has accomplished maximum predictive outcomes over the other methods. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Book chapter","Final","","Scopus","2-s2.0-85128217407"
"Ghanvatkar S.; Rajan V.","Ghanvatkar, Suparna (57205550254); Rajan, Vaibhav (56250979000)","57205550254; 56250979000","Towards a Theory-Based Evaluation of Explainable Predictions in Healthcare","2022","International Conference on Information Systems, ICIS 2022: ""Digitization for the Next Generation""","","","","","","","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164463379&partnerID=40&md5=114c5017fac40a470c59928e48afe509","Modern Artificial Intelligence (AI) models offer high predictive accuracy but often lack interpretability with respect to reasons for predictions. Explanations for predictions are usually necessary in making high-stakes clinical decisions. Hence, many Explainable AI (XAI) techniques have been designed to generate explanations for predictions from black-box models. However, there are no rigorous metrics to evaluate these explanations, especially with respect to their usefulness to clinicians. We develop a principled method to evaluate explanations by drawing on theories from social science and accounting for specific requirements of the clinical context. As a case study, we use our metric to evaluate explanations generated by two popular XAI algorithms in the task of predicting the onset of Alzheimer's disease using genetic data. Our preliminary findings are promising and illustrate the versatility and utility of our metric. Our work contributes to the practical and theoretical development of XAI techniques and Clinical Decision Support Systems. © 2022 International Conference on Information Systems, ICIS 2022: ""Digitization for the Next Generation"". All Rights Reserved.","Conference paper","Final","","Scopus","2-s2.0-85164463379"
"Ray A.","Ray, Animesh (55435183700)","55435183700","Machine learning in postgenomic biology and personalized medicine","2022","Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery","12","2","e1451","","","","3","10.1002/widm.1451","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123476575&doi=10.1002%2fwidm.1451&partnerID=40&md5=c12434126e4130d45da9a777fe7a13bf","In recent years, machine learning (ML) has been revolutionizing biology, biomedical sciences, and gene-based agricultural technology capabilities. Massive data generated in biological sciences by rapid and deep gene sequencing and protein or other molecular structure determination, on the one hand, require data analysis capabilities using ML that are distinctly different from classical statistical methods; on the other, these large datasets are enabling the adoption of novel data-intensive ML algorithms for the solution of biological problems that until recently had relied on mechanistic model-based approaches that are computationally expensive. This review provides a bird's eye view of the applications of ML in postgenomic biology. Attempt is also made to indicate as far as possible the areas of research that are poised to make further impacts in these areas, including the importance of explainable artificial intelligence in human health. Further contributions of ML are expected to transform medicine, public health, agricultural technology, as well as to provide invaluable gene-based guidance for the management of complex environments in this age of global warming. This article is categorized under: Technologies > Machine Learning Technologies > Artificial Intelligence Technologies > Prediction. © 2022 Wiley Periodicals LLC.","Review","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85123476575"
"Silva C.; Morais A.; Ribeiro B.","Silva, Catarina (8770080300); Morais, António (57904413300); Ribeiro, Bernardete (35577774200)","8770080300; 57904413300; 35577774200","A Generic Approach to Extend Interpretability of Deep Networks","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13566 LNAI","","","488","499","11","1","10.1007/978-3-031-16474-3_40","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138687729&doi=10.1007%2f978-3-031-16474-3_40&partnerID=40&md5=7d7957916ee14257daf1e8556ac174dc","The recent advent of machine learning as a transforming technology has sparked fears about human inability to comprehend the rational of gradually more complex approaches. Interpretable Machine Learning (IML) was triggered by such concerns, with the purpose of enabling different actors to grasp the application scenarios, including trustworthiness and decision support in highly regulated sectors as those related to health and public services. YOLO (You Only Look Once) models, as other deep Convolutional Neural Network (CNN) approaches, have recently shown remarkable performance in several tasks dealing with object detection. However, interpretability of these models is still an open issue. Therefore, in this work we extend the LIME (Local Interpretable Model-agnostic Explanations) framework to be used with YOLO models. The main contribution is a public add-on to LIME that can effectively improve YOLO interpretability. Results on complex images show the potential improvement. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Conference paper","Final","","Scopus","2-s2.0-85138687729"
"Troncoso-García A.R.; Martínez-Ballesteros M.; Martínez-Álvarez F.; Troncoso A.","Troncoso-García, A.R. (57992575100); Martínez-Ballesteros, M. (35409627600); Martínez-Álvarez, F. (22035530600); Troncoso, A. (23475711600)","57992575100; 35409627600; 22035530600; 23475711600","Explainable machine learning for sleep apnea prediction","2022","Procedia Computer Science","207","","","2924","2933","9","12","10.1016/j.procs.2022.09.351","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143373691&doi=10.1016%2fj.procs.2022.09.351&partnerID=40&md5=58106a753d3caa4d44905a777aafac53","Machine and deep learning has become one of the most useful tools in the last years as a diagnosis-decision-support tool in the health area. However, it is widely known that artificial intelligence models are considered a black box and most experts experience difficulties explaining and interpreting the models and their results. In this context, explainable artificial intelligence is emerging with the aim of providing black-box models with sufficient interpretability so that models can be easily understood and further applied. Obstructive sleep apnea is a common chronic respiratory disease related to sleep. Its diagnosis nowadays is done by processing different data signals, such as electrocardiogram or respiratory rate. The waveform of the respiratory signal is of importance too. Machine learning models could be applied to the signal's analysis. Data from a polysomnography study for automatic sleep apnea detection have been used to evaluate the use of the Local Interpretable Model-Agnostic (LIME) library for explaining the health data models. Results obtained help to understand how several features have been used in the model and their influence in the quality of sleep. © 2022 The Authors. Published by Elsevier B.V.","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85143373691"
"Oprescu A.M.; Miró-Amarante G.; García-Díaz L.; Rey V.E.; Chimenea-Toscano A.; Martínez-Martínez R.; Romero-Ternero M.C.","Oprescu, A.M. (57222483543); Miró-Amarante, G. (57222478891); García-Díaz, L. (6506141151); Rey, V.E. (57222476221); Chimenea-Toscano, A. (57200193205); Martínez-Martínez, R. (57560769000); Romero-Ternero, M.C. (36662862700)","57222483543; 57222478891; 6506141151; 57222476221; 57200193205; 57560769000; 36662862700","Towards a data collection methodology for Responsible Artificial Intelligence in health: A prospective and qualitative study in pregnancy","2022","Information Fusion","83-84","","","53","78","25","19","10.1016/j.inffus.2022.03.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127526963&doi=10.1016%2fj.inffus.2022.03.011&partnerID=40&md5=44ab7502a059f6ca5a1e9902ca9871cf","A medical field that is increasingly benefiting from Artificial Intelligence applications is Gyne- cology and Obstetrics. In previous work, we exposed that Artificial Intelligence (AI) technology and obstetric control by physicians can enhance pregnancy health, leading to better pregnancy outcomes and overall better experience, also reducing any possible long-term effects that can be produced by complications. This work presents a data collection methodology for responsible AI in Health and a case study in the pregnancy domain. It is a qualitative descriptive study on the preferences and expectations expressed by pregnant women regarding responsible AI and affective computing. A 41-items structured interview was distributed among 150 pregnant pa- tients attending prenatal care at Hospital Virgen del Rocío and the Clinic Victoria Rey (Seville, Spain) during the months of October and November 2020. A substantial interest in intelligent pregnancy solutions among pregnant women has been revealed in this study. Participants with a lower level of interest reported privacy concerns and lack of trust towards AI solutions. Re- garding affective computing based intelligent solutions specifically, most participants reported positively and no significant difference was found between women having a healthy or a high risk pregnancy on this matter. Our findings also suggest that a high demand of personalized intelligent solutions exists among participants. On the topic of sharing pregnancy data with the healthcare provider in favor of scientific research, pregnant women assisting public health- care services were found to be more likely to share their data when the provider was a public healthcare system rather than a private entity. Pregnant women who are interested in using an AI pregnancy application share a strong idea that it needs to be responsible, trustworthy, useful, and safe. Likewise, we found that pregnant women would change their mind about their concerns and they would feel more confident if the intelligent solution gives explanations about the system decisions and recommendations, as XAI approach promotes. © 2022 The Author(s)","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85127526963"
"Ahmed M.; Zubair S.","Ahmed, Mohiuddin (57206975377); Zubair, Shahrin (57419533200)","57206975377; 57419533200","Explainable Artificial Intelligence in Sustainable Smart Healthcare","2022","Studies in Computational Intelligence","1025","","","265","280","15","6","10.1007/978-3-030-96630-0_12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128615527&doi=10.1007%2f978-3-030-96630-0_12&partnerID=40&md5=4792783026faadc42f5fc75449b275bf","Artificial Intelligence (AI) is the capability of a system to execute tasks similar to decisions taken by human intelligence. AI has been certainly the hotspot for Internet of Health Things (IoHT) and has brought revolutionary changes in the health community. But yet the healthcare providers and the researchers’ demand for explanation of the resulting predictions made by the system on the basis of the health data trained in the Machine Learning model was not satisfied. Thus, the field of Explainable Artificial Intelligence (XAI) has been explored by the researcher community to provide explanation to the predictions made by the machines and ensure accuracy in the absolute healthcare infrastructure. Since blindly relying on the decisions made by the machine for saving a human soul without proper understanding of the underlying logic is inappropriate, in this condition XAI assists the medical care team to understand the logic and counter check the decisions before implementing on the patient for a better cause. Our aim is to highlight the reasons of adopting XAI in the healthcare domain in this book chapter and discuss the basic concept behind it on how it can contribute towards reliant AI-based solutions to the healthcare. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Book chapter","Final","","Scopus","2-s2.0-85128615527"
"Boukobza A.; Burgun A.; Roudier B.; Tsopra R.","Boukobza, Adrien (57224207336); Burgun, Anita (7004335489); Roudier, Bertrand (56076562000); Tsopra, Rosy (55560454800)","57224207336; 7004335489; 56076562000; 55560454800","Deep Neural Networks for Simultaneously Capturing Public Topics and Sentiments during a Pandemic: Application on a COVID-19 Tweet Data Set","2022","JMIR Medical Informatics","10","5","e34306","","","","8","10.2196/34306","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133532359&doi=10.2196%2f34306&partnerID=40&md5=8caa6a83fc3ea0bdf8561365d0e985d2","Background: Public engagement is a key element for mitigating pandemics, and a good understanding of public opinion could help to encourage the successful adoption of public health measures by the population. In past years, deep learning has been increasingly applied to the analysis of text from social networks. However, most of the developed approaches can only capture topics or sentiments alone but not both together. Objective: Here, we aimed to develop a new approach, based on deep neural networks, for simultaneously capturing public topics and sentiments and applied it to tweets sent just after the announcement of the COVID-19 pandemic by the World Health Organization (WHO). Methods: A total of 1,386,496 tweets were collected, preprocessed, and split with a ratio of 80:20 into training and validation sets, respectively. We combined lexicons and convolutional neural networks to improve sentiment prediction. The trained model achieved an overall accuracy of 81% and a precision of 82% and was able to capture simultaneously the weighted words associated with a predicted sentiment intensity score. These outputs were then visualized via an interactive and customizable web interface based on a word cloud representation. Using word cloud analysis, we captured the main topics for extreme positive and negative sentiment intensity scores. Results: In reaction to the announcement of the pandemic by the WHO, 6 negative and 5 positive topics were discussed on Twitter. Twitter users seemed to be worried about the international situation, economic consequences, and medical situation. Conversely, they seemed to be satisfied with the commitment of medical and social workers and with the collaboration between people. Conclusions: We propose a new method based on deep neural networks for simultaneously extracting public topics and sentiments from tweets. This method could be helpful for monitoring public opinion during crises such as pandemics. ©Adrien Boukobza, Anita Burgun, Bertrand Roudier, Rosy Tsopra.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85133532359"
"Guo C.; Shang Z.; Ren J.; Zhao Z.; Wang S.; Chen X.","Guo, Chang (58149070900); Shang, Zuogang (58028374100); Ren, Jiaxin (57398334300); Zhao, Zhibin (57192212778); Wang, Shibin (55786336400); Chen, Xuefeng (57205480544)","58149070900; 58028374100; 57398334300; 57192212778; 55786336400; 57205480544","Instance-Wise Causal Feature Selection Explainer for Rotating Machinery Fault Diagnosis","2022","2022 International Conference on Sensing, Measurement and Data Analytics in the Era of Artificial Intelligence, ICSMD 2022 - Proceedings","","","","","","","1","10.1109/ICSMD57530.2022.10058059","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150433127&doi=10.1109%2fICSMD57530.2022.10058059&partnerID=40&md5=8a600cea0824cf05fa0ff54fb4973ad7","Artificial neural networks in prognostics and health management (PHM), especially in intelligent fault diagnosis (IFD) have made great progress but possess black-box nature, leading to lack of interpretability and weak robustness when facing complex environment variations. When environment changes, the model tends to make wrong decisions leading to a cost, especially for major equipment if easily trusted by the users. Researchers have made studies on eXplainable Artificial Intelligence (XAI) based IFD to better understand the models. Most of them express their interpretability in the way of drawing gradient-based saliency maps to show where the model focuses on, which is of little consideration for causal effect and not sparse enough without quantitative metrics. To address these issues, we design an XAI method that utilizes a neural network as an instance-wise feature selector to select frequency bands that have stronger causal strength with the diagnosis result than others and further explain the diagnosis model. We quantify causal strength with the relative entropy distance (RED) and treat the simplified RED as the objective function for the optimization of the selector model. Finally, our experiments demonstrate the superiority of our method over another algorithm L2X measured by post-hoc accuracy (PHA), variant average causal effect (ACE), and vision plots.  © 2022 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85150433127"
"Ferdousi R.; Mabruba N.; Laamarti F.; El Saddik A.; Yang C.","Ferdousi, Rahatara (57203067582); Mabruba, Nabila (57210563285); Laamarti, Fedwa (56398557900); El Saddik, Abdulmotaleb (35431360000); Yang, Chunsheng (55995410600)","57203067582; 57210563285; 56398557900; 35431360000; 55995410600","Non-invasive Anemia Detection from Conjunctival Images","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13497 LNCS","","","189","201","12","0","10.1007/978-3-031-22061-6_14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145160780&doi=10.1007%2f978-3-031-22061-6_14&partnerID=40&md5=9fbc95c8125c1183225b959079eeb066","Anemia is a worldwide health issue. To diagnose anemia, blood must be drawn to examine the hemoglobin level. The procedure is time-consuming and labor-intensive. The existing Artificial Intelligence (AI)-based anemia detection methods in literature have shortcomings, including, i) specially designed data collection device, ii) manual feature extraction, iii) small data size for training the model, and iv)user’s trust in AI prediction. In this paper, we aim to provide a non-invasive model of anemia detection from visible signs. We trained a CNN model on eye-membrane image data collected from real patients and open image sources. Our model predicts anemic patients with good accuracy at 98%. In addition, we proposed the explainable AI method as a part of the non-invasive diagnosis to enhance the user’s trust in the CNN model’s prediction. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Conference paper","Final","","Scopus","2-s2.0-85145160780"
"Srinivasu P.N.; Sandhya N.; Jhaveri R.H.; Raut R.","Srinivasu, Parvathaneni Naga (57200093122); Sandhya, N. (6506940095); Jhaveri, Rutvij H. (55201717100); Raut, Roshani (56405465100)","57200093122; 6506940095; 55201717100; 56405465100","From Blackbox to Explainable AI in Healthcare: Existing Tools and Case Studies","2022","Mobile Information Systems","2022","","8167821","","","","45","10.1155/2022/8167821","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133128224&doi=10.1155%2f2022%2f8167821&partnerID=40&md5=565c7c075acd2d1462b32df2721f936e","Introduction. Artificial intelligence (AI) models have been employed to automate decision-making, from commerce to more critical fields directly affecting human lives, including healthcare. Although the vast majority of these proposed AI systems are considered black box models that lack explainability, there is an increasing trend of attempting to create medical explainable Artificial Intelligence (XAI) systems using approaches such as attention mechanisms and surrogate models. An AI system is said to be explainable if humans can tell how the system reached its decision. Various XAI-driven healthcare approaches and their performances in the current study are discussed. The toolkits used in local and global post hoc explainability and the multiple techniques for explainability pertaining the Rational, Data, and Performance explainability are discussed in the current study. Methods. The explainability of the artificial intelligence model in the healthcare domain is implemented through the Local Interpretable Model-Agnostic Explanations and Shapley Additive Explanations for better comprehensibility of the internal working mechanism of the original AI models and the correlation among the feature set that influences decision of the model. Results. The current state-of-the-art XAI-based and future technologies through XAI are reported on research findings in various implementation aspects, including research challenges and limitations of existing models. The role of XAI in the healthcare domain ranging from the earlier prediction of future illness to the disease's smart diagnosis is discussed. The metrics considered in evaluating the model's explainability are presented, along with various explainability tools. Three case studies about the role of XAI in the healthcare domain with their performances are incorporated for better comprehensibility. Conclusion. The future perspective of XAI in healthcare will assist in obtaining research insight in the healthcare domain. © 2022 Parvathaneni Naga Srinivasu et al.","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85133128224"
"Ostellino S.; Benso A.; Politano G.","Ostellino, Sofia (57705209000); Benso, Alfredo (7006074596); Politano, Gianfranco (26655038700)","57705209000; 7006074596; 26655038700","The integration of clinical data in the assessment of multiple sclerosis – A review","2022","Computer Methods and Programs in Biomedicine","221","","106900","","","","2","10.1016/j.cmpb.2022.106900","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130623290&doi=10.1016%2fj.cmpb.2022.106900&partnerID=40&md5=95932b64c3255341d3a5555b87ac618d","Background and Objectives: Multiple Sclerosis (MS) is a neurological disease associated with various and heterogeneous clinical characteristics. Given its complex nature and its unpredictable evolution over time, there isn't an established and exhaustive clinical protocol (or tool) for its diagnosis nor for monitoring its progression. Instead, different clinical exams and physical/psychological evaluations need to be taken into account. The Expanded Disability Status Scale (EDSS) is the most used clinical scale, but it suffers from several limitations. Developing computational solutions for the identification of bio-markers of disease progression that overcome the downsides of currently used scales is crucial and is gaining interest in current literature and research. Methods: This Review focuses on the importance of approaching MS diagnosis and monitoring by investigating correlations between cognitive impairment and clinical data that refer to different MS domains. We review papers that integrate heterogeneous data and analyse them with statistical methods to understand their applicability into more advanced computational tools. Particular attention is paid to the impact that computational approaches can have on personalized-medicine. Results: Personalized medicine for neuro-degenerative diseases is an unmet clinical need which can be addressed using computational approaches able to efficiently integrate heterogeneous clinical data extracted from both private and publicly available electronic health databases. Conclusions: Reliable and explainable Artificial Intelligence are computational approaches required to understand the complex and demonstrated interactions between MS manifestations as well as to provide reliable predictions on the disease evolution, representing a promising research field. © 2022","Review","Final","","Scopus","2-s2.0-85130623290"
"Banja J.D.; Hollstein R.D.; Bruno M.A.","Banja, John D. (56068504700); Hollstein, Rolf Dieter (57470543600); Bruno, Michael A. (35794552900)","56068504700; 57470543600; 35794552900","When Artificial Intelligence Models Surpass Physician Performance: Medical Malpractice Liability in an Era of Advanced Artificial Intelligence","2022","Journal of the American College of Radiology","19","7","","816","820","4","14","10.1016/j.jacr.2021.11.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125480211&doi=10.1016%2fj.jacr.2021.11.014&partnerID=40&md5=c5e225d659ba4aecab820fd0879d4a65","It seems inevitable that diagnostic and recommender artificial intelligence models will ultimately reach a point when they outperform human clinicians. Just as antibiotics displaced a host of medicinals for treating infections, the superior performance of such models will force their adoption. This article contemplates certain ethical and legal implications bearing on that adoption, especially because they involve a clinician's exposure to allegations of malpractice. The article discusses four relevant considerations: (1) the imperative of using explainable artificial intelligence models in clinical care, (2) specific strategies for diminishing liability when a clinician agrees or disagrees with a model's findings or recommendations but the patient nevertheless experiences a poor outcome, (3) relieving liability through legislation or regulation, and (4) comprehending such models as “persons” and therefore as potential defendants in legal proceedings. We conclude with observations on clinician–vendor relationships and argue that, although advanced artificial intelligence models have not yet arrived, clinicians must begin considering their implications now. © 2022 American College of Radiology","Article","Final","","Scopus","2-s2.0-85125480211"
"Kinger S.; Kulkarni V.","Kinger, Shakti (55782658100); Kulkarni, Vrushali (55487102600)","55782658100; 55487102600","Explainability of Deep Learning-Based System in Health Care","2022","Smart Innovation, Systems and Technologies","281","","","619","633","14","4","10.1007/978-981-16-9447-9_47","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130406267&doi=10.1007%2f978-981-16-9447-9_47&partnerID=40&md5=cbdc6c90c66b11ff8fdbeb79cf6d14ea","Ocular disease is an eye disease that reduces the eye’s ability to work normally. Early ocular disease detection is important to avoid blindness caused by some of the diseases like cataracts, glaucoma, diabetes, age-related macular degeneration (AMD), etc. Artificial intelligence (AI) techniques have been used to build systems for the speedy diagnosis of such diseases. In recent years, the deep neural network (DNN) has shown remarkable success in this area. But the black box nature of such systems has created questions on the use of DNN in a high-risk system like health care. Explainable AI (XAI) is a suite of methods and techniques that provides explanations of predictions made by AI systems. This helps to achieve accountability, transparency, and debugging of the model in the healthcare domain. In this paper, we have proposed to develop an ocular disease classification model and an XAI method that can be used to explain the classification of eye diseases, from eye fundus images. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Conference paper","Final","","Scopus","2-s2.0-85130406267"
"","","","9th International Work-Conference on Bioinformatics and Biomedical Engineering, IWBBIO 2022","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13347 LNBI","","","","","922","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133165209&partnerID=40&md5=eb8db11cc84177d75b5e01c4371d7cd2","The proceedings contain 75 papers. The special focus in this conference is on Bioinformatics and Biomedical Engineering. The topics include: Automated TTC Image-Based Analysis of Mouse Brain Lesions; PET-Neuroimaging and Neuropsychological Study for Early Cognitive Impairment in Parkinson’s Disease; architecture and Calibration of a Multi-channel Electrical Impedance Myographer; advanced Incremental Attribute Learning Clustering Algorithm for Medical and Healthcare Applications; Assessment of Inflammation in Non-calcified Artery Plaques with Dynamic 18F-FDG-PET/CT: CT Alone, Does-It Detect the Vulnerable Plaque?; Comparative Analysis of the Spatial Structure Chloroplasts and Cyanobacteria Photosynthetic Systems I and II Genes; Unsupervised Classification of Some Bacteria with 16S RNA Genes; modern Approaches to Cancer Treatment; a Service for Flexible Management and Analysis of Heterogeneous Clinical Data; linear Predictive Modeling for Immune Metabolites Related to Other Metabolites; reconfigurable Arduino Shield for Biosignal Acquisition; smart Watch for Smart Health Monitoring: A Literature Review; Data Quality Enhancement for Machine Learning on Wearable ECGs; Measurable Difference Between Malignant and Benign Tumor of the Thyroid Gland Recognizable Using Echogenicity Index in Ultrasound B-MODE Imaging: An Experimental Blind Study; initial Prototype of Low-Cost Stool Monitoring System for Early Detection of Diseases; Cerebral Activation in Subjects with Developmental Coordination Disorder: A Pilot Study with PET Imaging; on the Use of Explainable Artificial Intelligence for the Differential Diagnosis of Pigmented Skin Lesions; estimating Frontal Body Landmarks from Thermal Sensors Using Residual Neural Networks; NMF for Quality Control of Multi-modal Retinal Images for Diagnosis of Diabetes Mellitus and Diabetic Retinopathy; radiomic-Based Lung Nodule Classification in Low-Dose Computed Tomography; modelling of Arbitrary Shaped Channels and Obstacles by Distance Function; Segmentation of Brain MR Images Using Quantum Inspired Firefly Algorithm with Mutation; a Deep Learning Framework for the Prediction of Conversion to Alzheimer Disease.","Conference review","Final","","Scopus","2-s2.0-85133165209"
"Vodencarevic A.; Weingärtner M.; Caro J.J.; Ukalovic D.; Zimmermann-Rittereiser M.; Schwab S.; Kolominsky-Rabas P.","Vodencarevic, Asmir (49964935000); Weingärtner, Michael (6602660568); Caro, J. Jaime (56158366900); Ukalovic, Dubravka (57770435500); Zimmermann-Rittereiser, Marcus (57190185225); Schwab, Stefan (57194322437); Kolominsky-Rabas, Peter (6701484066)","49964935000; 6602660568; 56158366900; 57770435500; 57190185225; 57194322437; 6701484066","Prediction of Recurrent Ischemic Stroke Using Registry Data and Machine Learning Methods: The Erlangen Stroke Registry","2022","Stroke","53","7","","2299","2306","7","13","10.1161/STROKEAHA.121.036557","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133103053&doi=10.1161%2fSTROKEAHA.121.036557&partnerID=40&md5=f94dccdea04dbf7bb88c68623c1d925f","Background: There have been multiple efforts toward individual prediction of recurrent strokes based on structured clinical and imaging data using machine learning algorithms. Some of these efforts resulted in relatively accurate prediction models. However, acquiring clinical and imaging data is typically possible at provider sites only and is associated with additional costs. Therefore, we developed recurrent stroke prediction models based solely on data easily obtained from the patient at home. Methods: Data from 384 patients with ischemic stroke were obtained from the Erlangen Stroke Registry. Patients were followed at 3 and 12 months after first stroke and then annually, for about 2 years on average. Multiple machine learning algorithms were applied to train predictive models for estimating individual risk of recurrent stroke within 1 year. Double nested cross-validation was utilized for conservative performance estimation and models' learning capabilities were assessed by learning curves. Predicted probabilities were calibrated, and relative variable importance was assessed using explainable artificial intelligence techniques. Results: The best model achieved the area under the curve of 0.70 (95% CI, 0.64-0.76) and relatively good probability calibration. The most predictive factors included patient's family and housing circumstances, rehabilitative measures, age, high calorie diet, systolic and diastolic blood pressures, percutaneous endoscopic gastrotomy, number of family doctor's home visits, and patient's mental state. Conclusions: Developing fairly accurate models for individual risk prediction of recurrent ischemic stroke within 1 year solely based on registry data is feasible. Such models could be applied in a home setting to provide an initial risk assessment and identify high-risk patients early. © 2022 Lippincott Williams and Wilkins. All rights reserved.","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85133103053"
"Marvin G.; Nakatumba-Nabende J.; Hellen N.; Alam M.G.R.","Marvin, Ggaliwango (57302525500); Nakatumba-Nabende, Joyce (57197759951); Hellen, Nakayiza (57385781800); Alam, Md. Golam Rabiul (57675622100)","57302525500; 57197759951; 57385781800; 57675622100","Responsible Artificial Intelligence for Preterm Birth Prediction in Vulnerable Populations","2022","Proceedings of IEEE Asia-Pacific Conference on Computer Science and Data Engineering, CSDE 2022","","","","","","","0","10.1109/CSDE56538.2022.10089301","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153682829&doi=10.1109%2fCSDE56538.2022.10089301&partnerID=40&md5=842e6d512adecc4bc8c9f631433c68b5","Newborn and child mortality prevention is one of the prioritized Sustainable Development Goals (SDG) targets of the World Health Organization (WHO) expected by 2030. This SDG Target 3.2 has prompted for a lot of solutions for the most affected regions like Sub-Saharan Africa, Central and Southern Asia in order to stop preventable deaths of children under the age of 5 years and newborns although available solutions are not yet sufficient to achieve the Goal. The 4th Industrial revolution has further advanced the need for a reliable interdisciplinary approach that leverages technological advancements in achieving the SDG Target 3.2. In this work, we present a trustworthy Artificial Intelligence (AI) solution that blends with the data driven emerging technologies to reduce this global burden by transparently and interpretably predicting preterm births for patients and physicians for predictive and preventive action towards lowering neonatal deaths and increasing child survival. This AI solution can globally improve maternal and child healthcare among nations the run curative healthcare systems. We used Random Forest and KNeighbors and obtained an accuracy of 100% and 78% with respectively with Synthetic Minority Oversampling Technique (SMOTE) and Adaptive Synthetic (ADASYN) class balancing techniques. With interpretability of the random forest algorithm, we can responsibly improve AI technology adoption for maternal and child health and provide useful automated data driven insights to maternal healthcare management stakeholders and policy makers for a sustainable healthcare system in developing countries.  © 2022 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85153682829"
"Chakraborty S.; Mittermaier S.; Carbonelli C.; Servadei L.","Chakraborty, Sanghamitra (58018278100); Mittermaier, Simon (57218448884); Carbonelli, Cecilia (56376383900); Servadei, Lorenzo (57194048204)","58018278100; 57218448884; 56376383900; 57194048204","Explainable AI for Gas Sensors","2022","Proceedings of IEEE Sensors","2022-October","","","","","","3","10.1109/SENSORS52175.2022.9967180","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144093402&doi=10.1109%2fSENSORS52175.2022.9967180&partnerID=40&md5=4f4d534866db37a3c190c5681262e0fa","Releasing harmful pollutants like ozone and nitrogen dioxide gas into the atmosphere has been a serious concern in recent times. Such gases endanger the health of humans as well as other species and cause damage to the environment. As a result, it has become vital to monitor the air quality around us. With technological advancement, low-cost chemical gas sensors equipped with machine learning or deep learning algorithms can be employed to detect these gases and their concentrations. However, with the use of such algorithms, there comes a challenge to understanding why they made certain predictions in human terms. This paper aims to address this difficulty by adopting different methodologies of explainable artificial intelligence (XAI) for gas sensors. These methods in turn help understand the reasoning behind the predictions made by the models and at the same time facilitate the characterization of the sensor behavior. © 2022 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85144093402"
"Connie T.; Tan Y.F.; Goh M.K.O.; Hon H.W.; Kadim Z.; Wong L.P.","Connie, Tee (6508018670); Tan, Yee Fan (57278565300); Goh, Michael Kah Ong (36117903000); Hon, Hock Woon (55583168200); Kadim, Zulaikha (36717665100); Wong, Li Pei (24726309400)","6508018670; 57278565300; 36117903000; 55583168200; 36717665100; 24726309400","Explainable health prediction from facial features with transfer learning","2022","Journal of Intelligent and Fuzzy Systems","42","3","","2491","2503","12","3","10.3233/JIFS-211737","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124670766&doi=10.3233%2fJIFS-211737&partnerID=40&md5=3f9e3f9a26f1fe6906281ead9d7b6219","In the recent years, Artificial Intelligence (AI) has been widely deployed in the healthcare industry. The new AI technology enables efficient and personalized healthcare systems for the public. In this paper, transfer learning with pre-Trained VGGFace model is applied to identify sick symptoms based on the facial features of a person. As the deep learning model's operation is unknown for making a decision, this paper investigates the use of Explainable AI (XAI) techniques for soliciting explanations for the predictions made by the model. Various XAI techniques including Integrated Gradient, Explainable region-based AI (XRAI) and Local Interpretable Model-Agnostic Explanations (LIME) are studied. XAI is crucial to increase the model's transparency and reliability for practical deployment. Experimental results demonstrate that the attribution method can give proper explanations for the decisions made by highlighting important attributes in the images. The facial features that account for positive and negative classes predictions are highlighted appropriately for effective visualization. XAI can help to increase accountability and trustworthiness of the healthcare system as it provides insights for understanding how a conclusion is derived from the AI model. © 2022-IOS Press. All rights reserved.","Article","Final","","Scopus","2-s2.0-85124670766"
"Nor A.K.M.; Pedapati S.R.; Muhammad M.; Leiva V.","Nor, Ahmad Kamal Mohd (57222669255); Pedapati, Srinivasa Rao (57191863218); Muhammad, Masdi (36662781700); Leiva, Víctor (22953630400)","57222669255; 57191863218; 36662781700; 22953630400","Abnormality Detection and Failure Prediction Using Explainable Bayesian Deep Learning: Methodology and Case Study with Industrial Data","2022","Mathematics","10","4","554","","","","26","10.3390/math10040554","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124729775&doi=10.3390%2fmath10040554&partnerID=40&md5=a1656ffe9430765b994ee7e07163c7ff","Mistrust, amplified by numerous artificial intelligence (AI) related incidents, is an issue that has caused the energy and industrial sectors to be amongst the slowest adopter of AI methods. Central to this issue is the black-box problem of AI, which impedes investments and is fast becoming a legal hazard for users. Explainable AI (XAI) is a recent paradigm to tackle such an issue. Being the backbone of the industry, the prognostic and health management (PHM) domain has recently been introduced into XAI. However, many deficiencies, particularly the lack of explanation assessment methods and uncertainty quantification, plague this young domain. In the present paper, we elaborate a framework on explainable anomaly detection and failure prognostic employing a Bayesian deep learning model and Shapley additive explanations (SHAP) to generate local and global explanations from the PHM tasks. An uncertainty measure of the Bayesian model is utilized as a marker for anomalies and expands the prognostic explanation scope to include the model’s confidence. In addition, the global explanation is used to improve prognostic performance, an aspect neglected from the handful of studies on PHM-XAI. The quality of the explanation is examined employing local accuracy and consistency properties. The elaborated framework is tested on real-world gas turbine anomalies and synthetic turbofan failure prediction data. Seven out of eight of the tested anomalies were successfully identified. Additionally, the prognostic outcome showed a 19% improvement in statistical terms and achieved the highest prognostic score amongst best published results on the topic. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85124729775"
"Deshpande N.M.; Gite S.; Pradhan B.; Assiri M.E.","Deshpande, Nilkanth Mukund (57221078240); Gite, Shilpa (56656365900); Pradhan, Biswajeet (12753037900); Assiri, Mazen Ebraheem (55896877200)","57221078240; 56656365900; 12753037900; 55896877200","Explainable Artificial Intelligence–A New Step towards the Trust in Medical Diagnosis with AI Frameworks: A Review","2022","CMES - Computer Modeling in Engineering and Sciences","133","3","","843","872","29","7","10.32604/cmes.2022.021225","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138813363&doi=10.32604%2fcmes.2022.021225&partnerID=40&md5=38e73a3afc23f87596124d8408f88d67","Machine learning (ML) has emerged as a critical enabling tool in the sciences and industry in recent years. Today’s machine learning algorithms can achieve outstanding performance on an expanding variety of complex tasks–thanks to advancements in technique, the availability of enormous databases, and improved computing power. Deep learning models are at the forefront of this advancement. However, because of their nested nonlinear structure, these strong models are termed as “black boxes,” as they provide no information about how they arrive at their conclusions. Such a lack of transparencies may be unacceptable in many applications, such as the medical domain. A lot of emphasis has recently been paid to the development of methods for visualizing, explaining, and interpreting deep learning models. The situation is substantially different in safety-critical applications. The lack of transparency of machine learning techniques may be limiting or even disqualifying issue in this case. Significantly, when single bad decisions can endanger human life and health (e.g., autonomous driving, medical domain) or result in significant monetary losses (e.g., algorithmic trading), depending on an unintelligible data-driven system may not be an option. This lack of transparency is one reason why machine learning in sectors like health is more cautious than in the consumer, e-commerce, or entertainment industries. Explainability is the term introduced in the preceding years. The AI model’s black box nature will become explainable with these frameworks. Especially in the medical domain, diagnosing a particular disease through AI techniques would be less adapted for commercial use. These models’ explainable natures will help them commercially in diagnosis decisions in the medical field. This paper explores the different frameworks for the explainability of AI models in the medical field. The available frameworks are compared with other parameters, and their suitability for medical fields is also discussed. © 2022 Tech Science Press. All rights reserved.","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85138813363"
"Khalid M.; Khattak H.A.; Ahmad A.; Chan Bukhari S.A.","Khalid, Mutahira (58253780700); Khattak, Hasan Ali (57208818911); Ahmad, Arsalan (55811204500); Chan Bukhari, Syed Ahmad (57196354764)","58253780700; 57208818911; 55811204500; 57196354764","Explainable Prediction of Medical Codes through Automated Knowledge Graph Curation Framework","2022","Proceedings of 2022 19th International Bhurban Conference on Applied Sciences and Technology, IBCAST 2022","","","","331","336","5","1","10.1109/IBCAST54850.2022.9990551","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146497778&doi=10.1109%2fIBCAST54850.2022.9990551&partnerID=40&md5=612ef25ef09db5c35a55ef9d7a2ea14f","Medical Coding (MC) converts medical diagnoses, procedures, equipment, and services into alphanumeric codes. Automated Medical Code prediction systems use Machine Learning and Deep Learning techniques. They could save insurance companies, Government agencies, and medical staff from the hassle of reading lengthy summaries to prepare insurance claims for reimbursement of expenses and fees. If not wholly correct, these machine-understandable codes can still help the medical coders to ease their task of predicting accurate medical codes. Despite being helpful, labor-saving, and efficient, these decision support systems fizzle out due to scarcity of domain knowledge. Knowledge Graphs containing a network of concepts, their meta-data, and a hierarchy of relationships in specific domains can help build the applications of Computer Assisted Coding (CAC) with much more accurate and precise results glued with Explainability. Unfortunately, domain-specific Knowledge Graphs (KG) creation is a grueling task, as it requires healthcare experts' intervention to annotate the medical concepts. Our proposed approach is to create a framework for the Automated Generation of Knowledge Graphs, which is yet a manual and time-consuming process, with the help of Natural Language Processing, Ontology-based information retrieval, and a semantic enrichment process. The created domain-specific Knowledge graph is then fused with a Deep Learning model. Predictions made by the Deep Learning model were compared before and after the consolidation of the domain-specific Knowledge Graph. We created a web-based application to save users from the complexity of the Deep learning model and knowledge graphs. Results proved the significance of the combination of Knowledge graphs and Artificial Intelligence. Produced results yield increased precision and accuracy with fewer false-positive results.  © 2022 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85146497778"
"Lin Y.-C.; Chen T.-C.T.","Lin, Yu-Cheng (34868423300); Chen, Tin-Chih Toly (8298017700)","34868423300; 8298017700","Type-II fuzzy approach with explainable artificial intelligence for nature-based leisure travel destination selection amid the COVID-19 pandemic","2022","Digital Health","8","","","","","","15","10.1177/20552076221106322","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132280800&doi=10.1177%2f20552076221106322&partnerID=40&md5=ea63335859bc4cf81b918c38df1c83e6","During the coronavirus disease 2019 (COVID-19) pandemic, it is difficult for travelers to choose suitable nature-based leisure travel destinations because many factors are related to health risks and are highly uncertain. This research proposes a type-II fuzzy approach with explainable artificial intelligence to overcome this difficulty. First, an innovative type-II alpha-cut operations fuzzy collaborative intelligence method was used to derive the fuzzy priorities of factors critical for nature-based leisure travel destination selection. Subsequently, a type-II fuzzy Vise Kriterijumska Optimizacija I Kompromisno Resenje method, which is also novel, was employed to evaluate and compare the overall performance of nature-based leisure travel destinations. Furthermore, several measures were taken to enhance the explainability of the selection process and result. The effectiveness of the proposed type-II fuzzy approach was evaluated in a regional experiment conducted in Taichung City, Taiwan, during the COVID-19 pandemic. © The Author(s) 2022.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85132280800"
"Sosa-Espadas C.E.; Cetina-Aguilar M.; Soladrero J.A.; Darias J.M.; Brito-Borges E.E.; Cuevas-Cuevas N.L.; Orozco-Del-Castillo M.G.","Sosa-Espadas, Cristian E. (58266764200); Cetina-Aguilar, Manuel (58265687900); Soladrero, Jose A. (58266139500); Darias, Jesus M. (57369149300); Brito-Borges, Esteban E. (57220024321); Cuevas-Cuevas, Nora L. (57205404939); Orozco-Del-Castillo, Mauricio G. (36802492900)","58266764200; 58265687900; 58266139500; 57369149300; 57220024321; 57205404939; 36802492900","Applying explanation methods for the iterative refinement of an ANN-based depression screening tool","2022","CEUR Workshop Proceedings","3389","","","129","140","11","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159777989&partnerID=40&md5=0a6120f6b58965bce0e6ff692aa3f97a","Depression, as one of many mental health disorders, is a serious health and economy problem, affecting over 300 million people of all ages. The diagnosis of depression is a very complex and time-consuming task for mental health professionals, who usually rely on self-report questionnaires as a screening process. However, the items used in these questionnaires are sometimes subjective, particularly to certain demographics, and could require much time and effort from the patient. In recent years, artificial intelligence techniques, such as artificial neural networks, have been commonly used to screen for depression, however, they operate as black-box models, i.e., they lack explainability and interpretability which are mandatory in health-related fields. In this work we propose not only an artificial neural network, but also a set of explainable artificial intelligence techniques to refine a large set of items from a psychological questionnaire into a more concise, explainable one. © 2022 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0). CEUR Workshop Proceedings (CEUR-WS.org)","Conference paper","Final","","Scopus","2-s2.0-85159777989"
"","","","20th International Conference of the Italian Association for Artificial Intelligence, AIxIA 2021","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13196 LNAI","","","","","715","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135069387&partnerID=40&md5=b054f9046ed69c8c92a1687246fec28f","The proceedings contain 48 papers. The special focus in this conference is on Italian Association for Artificial Intelligence. The topics include: Effective Analysis of Industry-Relevant Cyber-Physical Systems via Statistical Model Checking; A Comparative Study of AI Search Methods for Personalised Cancer Therapy Synthesis in COPASI; explainable Artificial Intelligence for Technology Policy Making Using Attribution Networks; robust Optimization Models For Local Flexibility Characterization of Virtual Power Plants; Continuous Defect Prediction in CI/CD Pipelines: A Machine Learning-Based Framework; Tafl-ES: Exploring Evolution Strategies for Asymmetrical Board Games; knowledge-Based Neural Pre-training for Intelligent Document Management; detection Accuracy for Evaluating Compositional Explanations of Units; Generating Local Textual Explanations for CNNs: A Semantic Approach Based on Knowledge Graphs; Highlighting the Importance of Reducing Research Bias and Carbon Emissions in CNNs; deep Learning of Recurrence Texture in Physiological Signals; adversarial Machine Learning in e-Health: Attacking a Smart Prescription System; siamese Networks with Transfer Learning for Change Detection in Sentinel-2 Images; learned Sorted Table Search and Static Indexes in Small Model Space; domino Saliency Metrics: Improving Existing Channel Saliency Metrics with Structural Information; Neural QBAFs: Explaining Neural Networks Under LRP-Based Argumentation Frameworks; Enhancing Telepresence Robots with AI: Combining Services to Personalize and React; exploration-Intensive Distractors: Two Environment Proposals and a Benchmarking; Clustering-Based Interpretation of Deep ReLU Network; supporting Trustworthy Artificial Intelligence via Bayesian Argumentation; EEG-Based BCIs for Elderly Rehabilitation Enhancement Exploiting Artificial Data; Static, Dynamic and Acceleration Features for CNN-Based Speech Emotion Recognition; arabCeleb: Speaker Recognition in Arabic; Human Detection in Drone Images Using YOLO for Search-and-Rescue Operations; vision-Based Holistic Scene Understanding for Context-Aware Human-Robot Interaction; A Relevance-Based CNN Trimming Method for Low-Resources Embedded Vision; A Sound (But Incomplete) Polynomial Translation from Discretised PDDL+ to Numeric Planning; Misogynous MEME Recognition: A Preliminary Study; punctuation Restoration in Spoken Italian Transcripts with Transformers.","Conference review","Final","","Scopus","2-s2.0-85135069387"
"Hilal A.M.; ISSAOUI I.; Obayya M.; Al-Wesabi F.N.; NEMRI N.; Hamza M.A.; Al Duhayyim M.; Zamani A.S.","Hilal, Anwer Mustafa (57202837434); ISSAOUI, Imène (57505036500); Obayya, Marwa (6505869929); Al-Wesabi, Fahd N. (57211901842); NEMRI, Nadhem (54420791100); Hamza, Manar Ahmed (57223407265); Al Duhayyim, Mesfer (57204360566); Zamani, Abu Sarwar (57295189700)","57202837434; 57505036500; 6505869929; 57211901842; 54420791100; 57223407265; 57204360566; 57295189700","Modeling of explainable artificial intelligence for biomedical mental disorder diagnosis","2022","Computers, Materials and Continua","71","2","","3853","3867","14","5","10.32604/cmc.2022.022663","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120753822&doi=10.32604%2fcmc.2022.022663&partnerID=40&md5=c80d232067674d86e68144ae235ec91e","The abundant existence of both structured and unstructured data and rapid advancement of statistical models stressed the importance of introducing Explainable Artificial Intelligence (XAI), a process that explains how prediction is done in AI models. Biomedicalmental disorder, i.e.,Autism Spectral Disorder (ASD) needs to be identified and classified at early stage itself in order to reduce health crisis.With this background, the current paper presents XAI-based ASD diagnosis (XAI-ASD) model to detect and classify ASD precisely. The proposed XAI-ASD technique involves the design of Bacterial Foraging Optimization (BFO)-based Feature Selection (FS) technique. In addition, Whale Optimization Algorithm (WOA) with Deep Belief Network (DBN) model is also applied for ASD classification process in which the hyperparameters of DBN model are optimally tuned with the help of WOA. In order to ensure a better ASD diagnostic outcome, a series of simulation process was conducted on ASD dataset. © 2022 Tech Science Press. All rights reserved.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85120753822"
"Hu Q.; Gois F.N.B.; Costa R.; Zhang L.; Yin L.; Magaia N.; de Albuquerque V.H.C.","Hu, Qinhua (16177668300); Gois, Francisco Nauber B. (57220176589); Costa, Rafael (57645340600); Zhang, Lijuan (57203122218); Yin, Ling (36700696500); Magaia, Naercio (50262438700); de Albuquerque, Victor Hugo C. (56962603300)","16177668300; 57220176589; 57645340600; 57203122218; 36700696500; 50262438700; 56962603300","Explainable artificial intelligence-based edge fuzzy images for COVID-19 detection and identification","2022","Applied Soft Computing","123","","108966","","","","27","10.1016/j.asoc.2022.108966","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134069814&doi=10.1016%2fj.asoc.2022.108966&partnerID=40&md5=294bb087948d3e208e008f5ddf6cba63","The COVID-19 pandemic continues to wreak havoc on the world's population's health and well-being. Successful screening of infected patients is a critical step in the fight against it, with radiology examination using chest radiography being one of the most important screening methods. For the definitive diagnosis of COVID-19 disease, reverse-transcriptase polymerase chain reaction remains the gold standard. Currently available lab tests may not be able to detect all infected individuals; new screening methods are required. We propose a Multi-Input Transfer Learning COVID-Net fuzzy convolutional neural network to detect COVID-19 instances from torso X-ray, motivated by the latter and the open-source efforts in this research area. Furthermore, we use an explainability method to investigate several Convolutional Networks COVID-Net forecasts in an effort to not only gain deeper insights into critical factors associated with COVID-19 instances, but also to aid clinicians in improving screening. We show that using transfer learning and pre-trained models, we can detect it with a high degree of accuracy. Using X-ray images, we chose four neural networks to predict its probability. Finally, in order to achieve better results, we considered various methods to verify the techniques proposed here. As a result, we were able to create a model with an AUC of 1.0 and accuracy, precision, and recall of 0.97. The model was quantized for use in Internet of Things devices and maintained a 0.95 percent accuracy. © 2022 Elsevier B.V.","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85134069814"
"Solanes A.; Radua J.","Solanes, Aleix (57194048427); Radua, Joaquim (34868485200)","57194048427; 34868485200","Advances in Using MRI to Estimate the Risk of Future Outcomes in Mental Health - Are We Getting There?","2022","Frontiers in Psychiatry","13","","fpsyt-13-826111","","","","0","10.3389/fpsyt.2022.826111","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128835635&doi=10.3389%2ffpsyt.2022.826111&partnerID=40&md5=1a862f8a73be1d498f934c41d3a650ff","[No abstract available]","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85128835635"
"Nor A.K.M.; Pedapati S.R.; Muhammad M.; Leiva V.","Nor, Ahmad Kamal Mohd (57222669255); Pedapati, Srinivasa Rao (57191863218); Muhammad, Masdi (36662781700); Leiva, Víctor (22953630400)","57222669255; 57191863218; 36662781700; 22953630400","Overview of explainable artificial intelligence for prognostic and health management of industrial assets based on preferred reporting items for systematic reviews and meta-analyses","2021","Sensors","21","23","8020","","","","35","10.3390/s21238020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120329402&doi=10.3390%2fs21238020&partnerID=40&md5=dee7a67e890f6f17a8fddce1fa70481a","Surveys on explainable artificial intelligence (XAI) are related to biology, clinical trials, fintech management, medicine, neurorobotics, and psychology, among others. Prognostics and health management (PHM) is the discipline that links the studies of failure mechanisms to system lifecycle management. There is a need, which is still absent, to produce an analytical compilation of PHM-XAI works. In this paper, we use preferred reporting items for systematic reviews and meta-analyses (PRISMA) to present a state of the art on XAI applied to PHM of industrial assets. This work provides an overview of the trend of XAI in PHM and answers the question of accuracy versus explainability, considering the extent of human involvement, explanation assessment, and uncertainty quantification in this topic. Research articles associated with the subject, since 2015 to 2021, were selected from five databases following the PRISMA methodology, several of them related to sensors. The data were extracted from selected articles and examined obtaining diverse findings that were synthesized as follows. First, while the discipline is still young, the analysis indicates a growing acceptance of XAI in PHM. Second, XAI offers dual advantages, where it is assimilated as a tool to execute PHM tasks and explain diagnostic and anomaly detection activities, implying a real need for XAI in PHM. Third, the review shows that PHM-XAI papers provide interesting results, suggesting that the PHM performance is unaffected by the XAI. Fourth, human role, evaluation metrics, and uncertainty management are areas requiring further attention by the PHM community. Adequate assessment metrics to cater to PHM needs are requested. Finally, most case studies featured in the considered articles are based on real industrial data, and some of them are related to sensors, showing that the available PHM-XAI blends solve real-world challenges, increasing the confidence in the artificial intelligence models’ adoption in the industry. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Review","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85120329402"
"Ieracitano C.; Mammone N.; Versaci M.; Varone G.; Ali A.-R.; Armentano A.; Calabrese G.; Ferrarelli A.; Turano L.; Tebala C.; Hussain Z.; Sheikh Z.; Sheikh A.; Sceni G.; Hussain A.; Morabito F.C.","Ieracitano, Cosimo (57192383001); Mammone, Nadia (55915193500); Versaci, Mario (6601979970); Varone, Giuseppe (57215087723); Ali, Abder-Rahman (56341279800); Armentano, Antonio (57497226300); Calabrese, Grazia (58711529900); Ferrarelli, Anna (57499432800); Turano, Lorena (57220925471); Tebala, Carmela (57213624662); Hussain, Zain (57220730154); Sheikh, Zakariya (57195149794); Sheikh, Aziz (7202522962); Sceni, Giuseppe (6504742844); Hussain, Amir (19734290900); Morabito, Francesco Carlo (7103166505)","57192383001; 55915193500; 6601979970; 57215087723; 56341279800; 57497226300; 58711529900; 57499432800; 57220925471; 57213624662; 57220730154; 57195149794; 7202522962; 6504742844; 19734290900; 7103166505","A fuzzy-enhanced deep learning approach for early detection of Covid-19 pneumonia from portable chest X-ray images","2022","Neurocomputing","481","","","202","215","13","85","10.1016/j.neucom.2022.01.055","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123886564&doi=10.1016%2fj.neucom.2022.01.055&partnerID=40&md5=63857a5f5176da2143359957a9eff211","The Covid-19 pandemic is the defining global health crisis of our time. Chest X-Rays (CXR) have been an important imaging modality for assisting in the diagnosis and management of hospitalised Covid-19 patients. However, their interpretation is time intensive for radiologists. Accurate computer aided systems can facilitate early diagnosis of Covid-19 and effective triaging. In this paper, we propose a fuzzy logic based deep learning (DL) approach to differentiate between CXR images of patients with Covid-19 pneumonia and with interstitial pneumonias not related to Covid-19. The developed model here, referred to as CovNNet, is used to extract some relevant features from CXR images, combined with fuzzy images generated by a fuzzy edge detection algorithm. Experimental results show that using a combination of CXR and fuzzy features, within a deep learning approach by developing a deep network inputed to a Multilayer Perceptron (MLP), results in a higher classification performance (accuracy rate up to 81%), compared to benchmark deep learning approaches. The approach has been validated through additional datasets which are continously generated due to the spread of the virus and would help triage patients in acute settings. A permutation analysis is carried out, and a simple occlusion methodology for explaining decisions is also proposed. The proposed pipeline can be easily embedded into present clinical decision support systems. © 2022 Elsevier B.V.","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85123886564"
"Krzysiak R.; Nguyen S.; Chen Y.","Krzysiak, Rafal (57355576200); Nguyen, Shalyn (58078908100); Chen, Yang Quan (7601439185)","57355576200; 58078908100; 7601439185","XAIoT-The Future of Wearable Internet of Things","2022","MESA 2022 - 18th IEEE/ASME International Conference on Mechatronic and Embedded Systems and Applications, Proceedings","","","","","","","6","10.1109/MESA55290.2022.10004460","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146862937&doi=10.1109%2fMESA55290.2022.10004460&partnerID=40&md5=5f62a08554c8ea8980d94834a1f7fc62","The addition of Machine Learning and other Artificial Intelligent (AI) algorithms has expanded the capabilities of the Internet of Things (IoT) framework. However, the Artificial Internet of Things (AIoT), has also brought forward many issues with the combination, mainly lack of explanations and transparency. This lack of explanations of what AI models are doing is problematic in many fields, especially in the medical field. Explainable Artificial Intelligence (XAI) allows users to be given more in depth knowledge with the background processes of the model prediction. Enabling XAI into the IoT framework would develop a system that would not only capture Big Data more effectively, but also allow the system to be more transparent and explainable, causing wider adoption of the framework. This review focuses on current work done in AIoT and how it can be vastly improved with XAIoT. We propose an Explainable Artificial Intelligent Internet of Things (XAIoT) framework for monitoring physiological health using a smart watch.  © 2022 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85146862937"
"Luckey D.; Fritz H.; Legatiuk D.; Peralta Abadía J.J.; Walther C.; Smarsly K.","Luckey, Daniel (57218249738); Fritz, Henrieke (57218247050); Legatiuk, Dmitrii (55837249400); Peralta Abadía, José Joaquín (57312888700); Walther, Christian (56844907900); Smarsly, Kay (8880230300)","57218249738; 57218247050; 55837249400; 57312888700; 56844907900; 8880230300","Explainable Artificial Intelligence to Advance Structural Health Monitoring","2022","Structural Integrity","21","","","331","346","15","6","10.1007/978-3-030-81716-9_16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117883229&doi=10.1007%2f978-3-030-81716-9_16&partnerID=40&md5=40644b6e4f354c466ce747f99043089e","In recent years, structural health monitoring (SHM) applications have significantly been enhanced, driven by advancements in artificial intelligence (AI) and machine learning (ML), a subcategory of AI. Although ML algorithms allow detecting patterns and features in sensor data that would otherwise remain undetected, the generally opaque inner processes and black-box character of ML algorithms are limiting the application of ML to SHM. Incomprehensible decision-making processes often result in doubts and mistrust in ML algorithms, expressed by engineers and stakeholders. In an attempt to increase trust in ML algorithms, explainable artificial intelligence (XAI) aims to provide explanations of decisions made by black-box ML algorithms. However, there is a lack of XAI approaches that meet all requirements of SHM applications. This chapter provides a review of ML and XAI approaches relevant to SHM and proposes a conceptual XAI framework pertinent to SHM applications. First, ML algorithms relevant to SHM are categorized. Next, XAI approaches, such as transparent models and model-specific explanations, are presented and categorized to identify XAI approaches appropriate for being implemented in SHM applications. Finally, based on the categorization of ML algorithms and the presentation of XAI approaches, the conceptual XAI framework is introduced. It is expected that the proposed conceptual XAI framework will provide a basis for improving ML acceptance and transparency and therefore increase trust in ML algorithms implemented in SHM applications. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Book chapter","Final","","Scopus","2-s2.0-85117883229"
"Prajod P.; Huber T.; André E.","Prajod, Pooja (57212552269); Huber, Tobias (57214634721); André, Elisabeth (7006887726)","57212552269; 57214634721; 7006887726","Using Explainable AI to Identify Differences Between Clinical and Experimental Pain Detection Models Based on Facial Expressions","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13141 LNCS","","","311","322","11","3","10.1007/978-3-030-98358-1_25","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127055291&doi=10.1007%2f978-3-030-98358-1_25&partnerID=40&md5=e017c489fd6274554544b87533376d54","Most of the currently available pain datasets use two types of pain stimuli - people with clinically diagnosed conditions (e.g. surgery) performing tasks that cause them pain (we call this clinical pain) and pain caused by external stimuli such as heat or electricity (we call this experimental pain). In high-risk domains like healthcare, understanding the decisions and limitations of various types of pain recognition models is pivotal for the acceptance of the technology. In this paper, we present a process based on Explainable Artificial Intelligence techniques to investigate the differences in the learned representations of models trained on experimental pain (BioVid heat pain dataset) and clinical pain (UNBC shoulder pain dataset). To this end, we first train two convolutional neural networks - one for each dataset - to automatically discern between pain and no pain. Next, we perform a cross-dataset evaluation, i.e., evaluate the performance of the heat pain model on images from the shoulder pain dataset and vice versa. Then, we use Layer-wise Relevance Propagation to analyze which parts of the images in our test sets were relevant for each pain model. Based on this analysis, we rely on the visual inspection by a human observer to generate hypotheses about learned concepts that distinguish the two models. Finally, we test those hypotheses quantitatively utilizing concept embedding analysis methods. Through this process, we identify (1) a concept which the clinical pain model is more strongly relying on and, (2) a concept which the experimental pain model is paying more attention to. Finally, we discuss how both of these concepts are involved in known pain patterns and can be attributed to behavioral differences found in the datasets. © 2022, Springer Nature Switzerland AG.","Conference paper","Final","","Scopus","2-s2.0-85127055291"
"Raza A.; Tran K.P.; Koehl L.; Li S.","Raza, Ali (57215077057); Tran, Kim Phuc (56666926900); Koehl, Ludovic (17434471300); Li, Shujun (57219848979)","57215077057; 56666926900; 17434471300; 57219848979","Designing ECG monitoring healthcare system with federated transfer learning and explainable AI","2022","Knowledge-Based Systems","236","","107763","","","","75","10.1016/j.knosys.2021.107763","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120445086&doi=10.1016%2fj.knosys.2021.107763&partnerID=40&md5=bf9900e5139c5e01cee6e22bc255e8a8","Deep learning plays a vital role in classifying different arrhythmias using electrocardiography (ECG) data. Nevertheless, training deep learning models normally requires a large amount of data and can lead to privacy concerns. Unfortunately, a large amount of healthcare data cannot be easily collected from a single silo. Additionally, deep learning models are like black-box, with no explainability of the predicted results, which is often required in clinical healthcare. This limits the application of deep learning in real-world health systems. In this paper, to address the above-mentioned challenges, we design a novel end-to-end framework in a federated setting for ECG-based healthcare using explainable artificial intelligence (XAI) and deep convolutional neural networks (CNN). The federated setting is used to solve challenges such as data availability and privacy concerns. Furthermore, the proposed framework effectively classifies different arrhythmias using an autoencoder and a classifier, both based on a CNN. Additionally, we propose an XAI-based module on top of the proposed classifier for interpretability of the classification results, which helps clinical practitioners to interpret the predictions of the classifier and to make quick and reliable decisions. The proposed framework was trained and tested using the baseline Massachusetts Institute of Technology - Boston's Beth Israel Hospital (MIT-BIH) Arrhythmia database. The trained classifier outperformed existing work by achieving accuracy up to 94.5% and 98.9% for arrhythmia detection using noisy and clean data, respectively, with five-fold cross-validation. We also propose a new communication cost reduction method to reduce the communication costs and to enhance the privacy of users’ data in the federated setting. While the proposed framework was tested and validated for ECG classification, it is general enough to be extended to many other healthcare applications. © 2021 Elsevier B.V.","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85120445086"
"Brakefield W.S.; Ammar N.; Shaban-Nejad A.","Brakefield, Whitney S. (57220065523); Ammar, Nariman (57217244894); Shaban-Nejad, Arash (13405260500)","57220065523; 57217244894; 13405260500","An Urban Population Health Observatory for Disease Causal Pathway Analysis and Decision Support: Underlying Explainable Artificial Intelligence Model","2022","JMIR Formative Research","6","7","e36055","","","","4","10.2196/36055","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135903902&doi=10.2196%2f36055&partnerID=40&md5=0a16a8dc59e74e9fd0873eb16df248ee","Background: Many researchers have aimed to develop chronic health surveillance systems to assist in public health decision-making. Several digital health solutions created lack the ability to explain their decisions and actions to human users. Objective: This study sought to (1) expand our existing Urban Population Health Observatory (UPHO) system by incorporating a semantics layer; (2) cohesively employ machine learning and semantic/logical inference to provide measurable evidence and detect pathways leading to undesirable health outcomes; (3) provide clinical use case scenarios and design case studies to identify socioenvironmental determinants of health associated with the prevalence of obesity, and (4) design a dashboard that demonstrates the use of UPHO in the context of obesity surveillance using the provided scenarios. Methods: The system design includes a knowledge graph generation component that provides contextual knowledge from relevant domains of interest. This system leverages semantics using concepts, properties, and axioms from existing ontologies. In addition, we used the publicly available US Centers for Disease Control and Prevention 500 Cities data set to perform multivariate analysis. A cohesive approach that employs machine learning and semantic/logical inference reveals pathways leading to diseases. Results: In this study, we present 2 clinical case scenarios and a proof-of-concept prototype design of a dashboard that provides warnings, recommendations, and explanations and demonstrates the use of UPHO in the context of obesity surveillance, treatment, and prevention. While exploring the case scenarios using a support vector regression machine learning model, we found that poverty, lack of physical activity, education, and unemployment were the most important predictive variables that contribute to obesity in Memphis, TN. Conclusions: The application of UPHO could help reduce health disparities and improve urban population health. The expanded UPHO feature incorporates an additional level of interpretable knowledge to enhance physicians, researchers, and health officials' informed decision-making at both patient and community levels. © 2022 by the Author(s).","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85135903902"
"De Oliveira A.C.; Diniz E.J.S.; Teixeira S.; Teles A.S.","De Oliveira, Adonias C. (57190287304); Diniz, Evandro J.S. (57580940800); Teixeira, Silmar (23104699800); Teles, Ariel S. (56439261000)","57190287304; 57580940800; 23104699800; 56439261000","How can machine learning identify suicidal ideation from user's texts? Towards the explanation of the Boamente system","2022","Procedia Computer Science","206","","","141","150","9","1","10.1016/j.procs.2022.09.093","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143610636&doi=10.1016%2fj.procs.2022.09.093&partnerID=40&md5=5ac553469f7e7037f94415e3ce79535f","Suicidal Ideation (SI) is characterized by a desire to die and, in many cases, even planning a suicide with writing notes or farewell letters. As a digital phenotyping-based application, the Boamente tool remotely detects patterns that indicate SI by passively collecting texts typed by patients under supervision of a mental health professional. The system uses Artificial Intelligence (AI) techniques in the process of classifying texts as positive or negative for suicidal ideation. However, there is no explanation yet on how texts influence the outputs of Machine/Deep Learning (ML/DL) models. This study aims to start the process of explaining how words and sentences of the collected texts influence the outputs of the classification models. The following classical ML algorithms were trained and models explained in this study: C-Support Vector Classification (SVC), Extra Trees, and Random Forest machine learning. The ELI5 method was used for the local and global interpretability of the algorithms. Features of each class were analyzed with their respective weight values on the output of each algorithm in the global interpretability. Classifications of three samples from the dataset were explained during the local interpretability step. After analyzing the features of each class, we were able to identify that the Extra Trees and Random Forest classifiers were equally influenced by features composed of one term or two terms. SVC was more influenced by features composed of two terms than just one term. In general, features including the features ""suicide"", ""desire to kill oneself""and ""sadness""had a higher importance value to indicate positive for SI.  © 2022 The Author(s).","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85143610636"
"Mensah P.K.; Ayidzoe M.A.","Mensah, Patrick Kwabena (57219697274); Ayidzoe, Mighty Abra (57216702958)","57219697274; 57216702958","Overview of CapsNet Performance Evaluation Methods for Image Classification using a Dual Input Capsule Network as a Case Study","2022","International Journal of Computing and Digital Systems","12","1","","29","43","14","2","10.12785/ijcds/120104","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136142953&doi=10.12785%2fijcds%2f120104&partnerID=40&md5=1f2519ef705a9cc6d1cf8e11fd069399","Performance evaluation is a critical part of deep learning (DL) that requires careful conduct to enhance confidence and reliability. Several metrics exist to evaluate DL models, however, choosing one for a given model is not trivial, since it is not a one-fit-all solution. Practically, accuracy is the most popularly used evaluation metric for capsule networks (CapsNets). This is problematic for sensitive applications (e.g. health), since accuracy is overly optimistic in the presence of class imbalance, and does not permit the exact reporting of a model’s risk of bias and potential usefulness. This paper, therefore, aims at demonstrating the usefulness of other metrics for performance evaluation as well as interpretability through the implementation of a custom capsule model. The metrics are effective in measuring the real performance of the models in terms of accuracy (93.03% for proposed model), number of parameters (≈ 4 million fewer for proposed model), ability to scale and fail-safe, and the effectiveness of the routing process when evaluated on the datasets. Evaluating a CapsNet model with all these metrics has the potential to enhance the practitioner’s confidence and also improve model understandability and reliability. © 2022 University of Bahrain. All rights reserved.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85136142953"
"Marvin G.; Alam M.G.R.","Marvin, Ggaliwango (57302525500); Alam, Md.Golam Rabiul (57675622100)","57302525500; 57675622100","Explainable Augmented Intelligence and Deep Transfer Learning for Pediatric Pulmonary Health Evaluation","2022","2022 International Conference on Innovations in Science, Engineering and Technology, ICISET 2022","","","","272","277","5","7","10.1109/ICISET54810.2022.9775845","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131711780&doi=10.1109%2fICISET54810.2022.9775845&partnerID=40&md5=fac2b271361dbc5857946914c244bf7c","Biomedical Instrumentation is one of the fastest health emerging innovative technologies with proven contribution towards interdisciplinary medicine, it helps physicians to diagnose complex medical problems and provide treatment to patients precisely and safely. With this technological trend, explainable artificial intelligence, biomedical image processing and augmented intelligence can provide a tool that can help pediatricians, pulmonology and otolaryngology physicians, epidemiologists and pediatric practitioners to interpretably and reliably diagnose chronic and acute respiratory disorders in children, adolescents and infants. Unfortunately, the reliability of digital image processing for pulmonary disease diagnosis often depends on availability of large chest X-ray image datasets. This work presents a reliable interpretable deep transfer learning approach for pediatric pulmonary health evaluation regardless of the scarcity and limited annotated pediatric chest X-ray Image dataset sizes. This approach leverages a combination of computer vision tools and techniques to reduce child morbidity and mortality through predictive and preventive medicine with reduced surveillance risks and affordability in low resource settings. With open datasets, the deep neural networks classified the generated augmented images into 4 classes namely; Normal, Covid-19, Tuberculosis and Pneumonia at an accuracy of 97%, 97%, 70%, and 73% respectively with recall of 100% for Pneumonia and overall accuracy of 79% at only 10 epochs for both regular and transferred learning.  © 2022 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85131711780"
"Uddin M.Z.; Dysthe K.K.; Følstad A.; Brandtzaeg P.B.","Uddin, Md Zia (24482836700); Dysthe, Kim Kristoffer (57223998208); Følstad, Asbjørn (55890812000); Brandtzaeg, Petter Bae (56644160400)","24482836700; 57223998208; 55890812000; 56644160400","Deep learning for prediction of depressive symptoms in a large textual dataset","2022","Neural Computing and Applications","34","1","","721","744","23","52","10.1007/s00521-021-06426-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113630004&doi=10.1007%2fs00521-021-06426-4&partnerID=40&md5=8515081c54feeb7c3e74e22377b7e786","Depression is a common illness worldwide with potentially severe implications. Early identification of depressive symptoms is a crucial first step towards assessment, intervention, and relapse prevention. With an increase in data sets with relevance for depression, and the advancement of machine learning, there is a potential to develop intelligent systems to detect symptoms of depression in written material. This work proposes an efficient approach using Long Short-Term Memory (LSTM)-based Recurrent Neural Network (RNN) to identify texts describing self-perceived symptoms of depression. The approach is applied on a large dataset from a public online information channel for young people in Norway. The dataset consists of youth’s own text-based questions on this information channel. Features are then provided from a one-hot process on robust features extracted from the reflection of possible symptoms of depression pre-defined by medical and psychological experts. The features are better than conventional approaches, which are mostly based on the word frequencies (i.e., some topmost frequent words are chosen as features from the whole text dataset and applied to model the underlying events in any text message) rather than symptoms. Then, a deep learning approach is applied (i.e., RNN) to train the time-sequential features discriminating texts describing depression symptoms from posts with no such descriptions (non-depression posts). Finally, the trained RNN is used to automatically predict depression posts. The system is compared against conventional approaches where it achieved superior performance than others. The linear discriminant space clearly reveals the robustness of the features by generating better clustering than other traditional features. Besides, since the features are based on the possible symptoms of depression, the system may generate meaningful explanations of the decision from machine learning models using an explainable Artificial Intelligence (XAI) algorithm called Local Interpretable Model-Agnostic Explanations (LIME). The proposed depression symptom feature-based approach shows superior performance compared to the traditional general word frequency-based approaches where frequency of the features gets more importance than the specific symptoms of depression. Although the proposed approach is applied on a Norwegian dataset, a similar robust approach can be applied on other depression datasets developed in other languages with proper annotations and symptom-based feature extraction. Thus, the depression prediction approach can be adopted to contribute to develop better mental health care technologies such as intelligent chatbots. © 2021, The Author(s).","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85113630004"
"Nazar M.; Alam M.M.; Yafi E.; Su'Ud M.M.","Nazar, Mobeen (57202076376); Alam, Muhammad Mansoor (57535594200); Yafi, Eiad (36806985300); Su'Ud, Mazliham Mohd (36701624700)","57202076376; 57535594200; 36806985300; 36701624700","A Systematic Review of Human-Computer Interaction and Explainable Artificial Intelligence in Healthcare with Artificial Intelligence Techniques","2021","IEEE Access","9","","","153316","153348","32","79","10.1109/ACCESS.2021.3127881","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119430656&doi=10.1109%2fACCESS.2021.3127881&partnerID=40&md5=48b8c8ad4f320230a1a6fd0c3642eba7","Artificial intelligence (AI) is one of the emerging technologies. In recent decades, artificial intelligence (AI) has gained widespread acceptance in a variety of fields, including virtual support, healthcare, and security. Human-Computer Interaction (HCI) is a field that has been combining AI and human-computer engagement over the past several years in order to create an interactive intelligent system for user interaction. AI, in conjunction with HCI, is being used in a variety of fields by employing various algorithms and employing HCI to provide transparency to the user, allowing them to trust the machine. The comprehensive examination of both the areas of AI and HCI, as well as their subfields, has been explored in this work. The main goal of this article was to discover a point of intersection between the two fields. The understanding of Explainable Artificial Intelligence (XAI), which is a linking point of HCI and XAI, was gained through a literature review conducted in this research. The literature survey encompassed themes identified in the literature (such as XAI and its areas, major XAI aims, and XAI problems and challenges). The study's other major focus was on the use of AI, HCI, and XAI in healthcare. The poll also addressed the shortcomings in XAI in healthcare, as well as the field's future potential. As a result, the literature indicates that XAI in healthcare is still a novel subject that has to be explored more in the future.  © 2013 IEEE.","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85119430656"
"","","","2nd International Conference on Neural Computing for Advanced Applications, NCAA 2021","2021","Communications in Computer and Information Science","1449","","","","","769","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115168368&partnerID=40&md5=e5279c27b5a48578f88153fb75f247c1","The proceedings contain 54 papers. The special focus in this conference is on Neural Computing for Advanced Applications. The topics include: Sample Reduction Using ℓ1 -Norm Twin Bounded Support Vector Machine; spreading Dynamics Analysis for Railway Networks; learning to Collocate Fashion Items from Heterogeneous Network Using Structural and Textual Features; building Energy Performance Certificate Labelling Classification Based on Explainable Artificial Intelligence; cross Languages One-Versus-All Speech Emotion Classifier; a Hybrid Machine Learning Approach for Customer Loyalty Prediction; LDA-Enhanced Federated Learning for Image Classification with Missing Modality; A Data Enhancement Method for Gene Expression Profile Based on Improved WGAN-GP; examining and Predicting Teacher Professional Development by Machine Learning Methods; systematic Analysis of Joint Entity and Relation Extraction Models in Identifying Overlapping Relations; A Hybrid Approach to Risk Analysis for Critical Failures of Machinery Spaces on Unmanned Ships by Fuzzy AHP; a New Health Indicator Construction Approach and Its Application in Remaining Useful Life Prediction of Bearings; an Improved Reinforcement Learning for Security-Constrained Economic Dispatch of Battery Energy Storage in Microgrids; self-supervised Learning Advance Fault Diagnosis of Rotating Machinery; a Method for Imbalanced Fault Diagnosis Based on Self-attention Generative Adversarial Network; self-supervised Contrastive Representation Learning for Machinery Fault Diagnosis; SGWnet: An Interpretable Convolutional Neural Network for Mechanical Fault Intelligent Diagnosis; GFU-Net: A Deep Learning Approach for Automatic Metal Crack Detection; an Improved Cluster Load Balancing Scheduling Algorithm; an Improved Cloud Particles Optimizer for Function Optimization; abnormality Detection and Identification Algorithm for High-Speed Freight Train Body; meta-feature Extraction for Multi-objective Optimization Problems.","Conference review","Final","","Scopus","2-s2.0-85115168368"
"Yang M.; Liu C.; Wang X.; Li Y.; Gao H.; Liu X.; Li J.","Yang, Meicheng (57215599996); Liu, Chengyu (55680778400); Wang, Xingyao (57204673604); Li, Yuwen (57192954817); Gao, Hongxiang (57218222476); Liu, Xing (56446058600); Li, Jianqing (55971506700)","57215599996; 55680778400; 57204673604; 57192954817; 57218222476; 56446058600; 55971506700","An Explainable Artificial Intelligence Predictor for Early Detection of Sepsis","2020","Critical Care Medicine","48","11","","E1091","E1096","5","44","10.1097/CCM.0000000000004550","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092802032&doi=10.1097%2fCCM.0000000000004550&partnerID=40&md5=dbc8c0c9c3ddfe385e2d8bc681a626a9","Objectives: Early detection of sepsis is critical in clinical practice since each hour of delayed treatment has been associated with an increase in mortality due to irreversible organ damage. This study aimed to develop an explainable artificial intelligence model for early predicting sepsis by analyzing the electronic health record data from ICU provided by the PhysioNet/Computing in Cardiology Challenge 2019. Design: Retrospective observational study. Setting: We developed our model on the shared ICUs publicly data and verified on the full hidden populations for challenge scoring. Patients: Public database included 40,336 patients' electronic health records sourced from Beth Israel Deaconess Medical Center (hospital system A) and Emory University Hospital (hospital system B). A total of 24,819 patients from hospital systems A, B, and C (an unidentified hospital system) were sequestered as full hidden test sets. Interventions: None. Measurements and Main Results: A total of 168 features were extracted on hourly basis. Explainable artificial intelligence sepsis predictor model was trained to predict sepsis in real time. Impact of each feature on hourly sepsis prediction was explored in-depth to show the interpretability. The algorithm demonstrated the final clinical utility score of 0.364 in this challenge when tested on the full hidden test sets, and the scores on three separate test sets were 0.430, 0.422, and-0.048, respectively. Conclusions: Explainable artificial intelligence sepsis predictor model achieves superior performance for predicting sepsis risk in a real-time way and provides interpretable information for understanding sepsis risk in ICU. © 2020 Lippincott Williams and Wilkins. All rights reserved.","Article","Final","","Scopus","2-s2.0-85092802032"
"Leung C.K.; Fung D.L.X.; Mai D.; Wen Q.; Tran J.; Souza J.","Leung, Carson K. (7402612526); Fung, Daryl L.X. (57219031234); Mai, Daniel (57261739100); Wen, Qi (57211167399); Tran, Jason (57262032900); Souza, Joglas (57203537633)","7402612526; 57219031234; 57261739100; 57211167399; 57262032900; 57203537633","Explainable Data Analytics for Disease and Healthcare Informatics","2021","ACM International Conference Proceeding Series","","","","65","74","9","8","10.1145/3472163.3472175","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115056656&doi=10.1145%2f3472163.3472175&partnerID=40&md5=6dac80df18449c72153af39bf9585413","With advancements in technology, huge volumes of valuable data have been generated and collected at a rapid velocity from a wide variety of rich data sources. Examples of these valuable data include healthcare and disease data such as privacy-preserving statistics on patients who suffered from diseases like the coronavirus disease 2019 (COVID-19). Analyzing these data can be for social good. For instance, data analytics on the healthcare and disease data often leads to the discovery of useful information and knowledge about the disease. Explainable artificial intelligence (XAI) further enhances the interpretability of the discovered knowledge. Consequently, the explainable data analytics helps people to get a better understanding of the disease, which may inspire them to take part in preventing, detecting, controlling and combating the disease. In this paper, we present an explainable data analytics system for disease and healthcare informatics. Our system consists of two key components. The predictor component analyzes and mines historical disease and healthcare data for making predictions on future data. Although huge volumes of disease and healthcare data have been generated, volumes of available data may vary partially due to privacy concerns. So, the predictor makes predictions with different methods. It uses random forest With sufficient data and neural network-based few-shot learning (FSL) with limited data. The explainer component provides the general model reasoning and a meaningful explanation for specific predictions. As a database engineering application, we evaluate our system by applying it to real-life COVID-19 data. Evaluation results show the practicality of our system in explainable data analytics for disease and healthcare informatics. © 2021 ACM.","Conference paper","Final","","Scopus","2-s2.0-85115056656"
"de Sousa I.P.; Vellasco M.M.B.R.; da Silva E.C.","de Sousa, Iam Palatnik (56313836500); Vellasco, Marley Maria Bernardes Rebuzzi (56200580000); da Silva, Eduardo Costa (14009474700)","56313836500; 56200580000; 14009474700","Local interpretable model-agnostic explanations for classification of lymph node metastases","2019","Sensors (Switzerland)","19","13","2969","","","","59","10.3390/s19132969","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069942136&doi=10.3390%2fs19132969&partnerID=40&md5=1406675df103b091733150c4e9ff61c4","An application of explainable artificial intelligence on medical data is presented. There is an increasing demand in machine learning literature for such explainable models in health-related applications. This work aims to generate explanations on how a Convolutional Neural Network (CNN) detects tumor tissue in patches extracted from histology whole slide images. This is achieved using the “locally-interpretable model-agnostic explanations” methodology. Two publicly-available convolutional neural networks trained on the Patch Camelyon Benchmark are analyzed. Three common segmentation algorithms are compared for superpixel generation, and a fourth simpler parameter-free segmentation algorithm is proposed. The main characteristics of the explanations are discussed, as well as the key patterns identified in true positive predictions. The results are compared to medical annotations and literature and suggest that the CNN predictions follow at least some aspects of human expert knowledge. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85069942136"
"Carrieri A.P.; Haiminen N.; Maudsley-Barton S.; Gardiner L.-J.; Murphy B.; Mayes A.E.; Paterson S.; Grimshaw S.; Winn M.; Shand C.; Hadjidoukas P.; Rowe W.P.M.; Hawkins S.; MacGuire-Flanagan A.; Tazzioli J.; Kenny J.G.; Parida L.; Hoptroff M.; Pyzer-Knapp E.O.","Carrieri, Anna Paola (56318750300); Haiminen, Niina (8359433300); Maudsley-Barton, Sean (57201288793); Gardiner, Laura-Jayne (56049441900); Murphy, Barry (57193274885); Mayes, Andrew E. (7103244121); Paterson, Sarah (7102053418); Grimshaw, Sally (55862006200); Winn, Martyn (57203031960); Shand, Cameron (57203126513); Hadjidoukas, Panagiotis (6506489389); Rowe, Will P. M. (56680758000); Hawkins, Stacy (7203056477); MacGuire-Flanagan, Ashley (57222171513); Tazzioli, Jane (57222163177); Kenny, John G. (9841572500); Parida, Laxmi (6701798270); Hoptroff, Michael (55847455500); Pyzer-Knapp, Edward O. (56015175000)","56318750300; 8359433300; 57201288793; 56049441900; 57193274885; 7103244121; 7102053418; 55862006200; 57203031960; 57203126513; 6506489389; 56680758000; 7203056477; 57222171513; 57222163177; 9841572500; 6701798270; 55847455500; 56015175000","Explainable AI reveals changes in skin microbiome composition linked to phenotypic differences","2021","Scientific Reports","11","1","4565","","","","46","10.1038/s41598-021-83922-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101749836&doi=10.1038%2fs41598-021-83922-6&partnerID=40&md5=e119d41df379e4b15a3b06abdbf66678","Alterations in the human microbiome have been observed in a variety of conditions such as asthma, gingivitis, dermatitis and cancer, and much remains to be learned about the links between the microbiome and human health. The fusion of artificial intelligence with rich microbiome datasets can offer an improved understanding of the microbiome’s role in human health. To gain actionable insights it is essential to consider both the predictive power and the transparency of the models by providing explanations for the predictions. We combine the collection of leg skin microbiome samples from two healthy cohorts of women with the application of an explainable artificial intelligence (EAI) approach that provides accurate predictions of phenotypes with explanations. The explanations are expressed in terms of variations in the relative abundance of key microbes that drive the predictions. We predict skin hydration, subject's age, pre/post-menopausal status and smoking status from the leg skin microbiome. The changes in microbial composition linked to skin hydration can accelerate the development of personalized treatments for healthy skin, while those associated with age may offer insights into the skin aging process. The leg microbiome signatures associated with smoking and menopausal status are consistent with previous findings from oral/respiratory tract microbiomes and vaginal/gut microbiomes respectively. This suggests that easily accessible microbiome samples could be used to investigate health-related phenotypes, offering potential for non-invasive diagnosis and condition monitoring. Our EAI approach sets the stage for new work focused on understanding the complex relationships between microbial communities and phenotypes. Our approach can be applied to predict any condition from microbiome samples and has the potential to accelerate the development of microbiome-based personalized therapeutics and non-invasive diagnostics. © 2021, The Author(s).","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85101749836"
"Fong R.; Vedaldi A.","Fong, Ruth (57200621318); Vedaldi, Andrea (14036614600)","57200621318; 14036614600","Explanations for Attributing Deep Neural Network Predictions","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11700 LNCS","","","149","167","18","37","10.1007/978-3-030-28954-6_8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072842217&doi=10.1007%2f978-3-030-28954-6_8&partnerID=40&md5=d98cbd72919a375b993e3313bc29fc7d","Given the recent success of deep neural networks and their applications to more high impact and high risk applications, like autonomous driving and healthcare decision-making, there is a great need for faithful and interpretable explanations of “why” an algorithm is making a certain prediction. In this chapter, we introduce 1. Meta-Predictors as Explanations, a principled framework for learning explanations for any black box algorithm, and 2. Meaningful Perturbations, an instantiation of our paradigm applied to the problem of attribution, which is concerned with attributing what features of an input (i.e., regions of an input image) are responsible for a model’s output (i.e., a CNN classifier’s object class prediction). We first introduced these contributions in [8]. We also briefly survey existing visual attribution methods and highlight how they faith to be both faithful and interpretable. © Springer Nature Switzerland AG 2019.","Book chapter","Final","","Scopus","2-s2.0-85072842217"
"Alves M.A.; Castro G.Z.; Oliveira B.A.S.; Ferreira L.A.; Ramírez J.A.; Silva R.; Guimarães F.G.","Alves, Marcos Antonio (57210123593); Castro, Giulia Zanon (57222658821); Oliveira, Bruno Alberto Soares (57221874506); Ferreira, Leonardo Augusto (56883726900); Ramírez, Jaime Arturo (7401783344); Silva, Rodrigo (55202175900); Guimarães, Frederico Gadelha (7005645510)","57210123593; 57222658821; 57221874506; 56883726900; 7401783344; 55202175900; 7005645510","Explaining machine learning based diagnosis of COVID-19 from routine blood tests with decision trees and criteria graphs","2021","Computers in Biology and Medicine","132","","104335","","","","60","10.1016/j.compbiomed.2021.104335","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103648098&doi=10.1016%2fj.compbiomed.2021.104335&partnerID=40&md5=cffb20fb9e602d2728c65847b6c627ad","The sudden outbreak of coronavirus disease 2019 (COVID-19) revealed the need for fast and reliable automatic tools to help health teams. This paper aims to present understandable solutions based on Machine Learning (ML) techniques to deal with COVID-19 screening in routine blood tests. We tested different ML classifiers in a public dataset from the Hospital Albert Einstein, São Paulo, Brazil. After cleaning and pre-processing the data has 608 patients, of which 84 are positive for COVID-19 confirmed by RT-PCR. To understand the model decisions, we introduce (i) a local Decision Tree Explainer (DTX) for local explanation and (ii) a Criteria Graph to aggregate these explanations and portrait a global picture of the results. Random Forest (RF) classifier achieved the best results (accuracy 0.88, F1–score 0.76, sensitivity 0.66, specificity 0.91, and AUROC 0.86). By using DTX and Criteria Graph for cases confirmed by the RF, it was possible to find some patterns among the individuals able to aid the clinicians to understand the interconnection among the blood parameters either globally or on a case-by-case basis. The results are in accordance with the literature and the proposed methodology may be embedded in an electronic health record system. © 2021 Elsevier Ltd","Article","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85103648098"
"Barda A.J.; Horvat C.M.; Hochheiser H.","Barda, Amie J. (57194499184); Horvat, Christopher M. (56565364600); Hochheiser, Harry (6602502080)","57194499184; 56565364600; 6602502080","A qualitative research framework for the design of user-centered displays of explanations for machine learning model predictions in healthcare","2020","BMC Medical Informatics and Decision Making","20","1","257","","","","49","10.1186/s12911-020-01276-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092601699&doi=10.1186%2fs12911-020-01276-x&partnerID=40&md5=f34d0534504ebb2a21be35b7a1941497","Background: There is an increasing interest in clinical prediction tools that can achieve high prediction accuracy and provide explanations of the factors leading to increased risk of adverse outcomes. However, approaches to explaining complex machine learning (ML) models are rarely informed by end-user needs and user evaluations of model interpretability are lacking in the healthcare domain. We used extended revisions of previously-published theoretical frameworks to propose a framework for the design of user-centered displays of explanations. This new framework served as the basis for qualitative inquiries and design review sessions with critical care nurses and physicians that informed the design of a user-centered explanation display for an ML-based prediction tool. Methods: We used our framework to propose explanation displays for predictions from a pediatric intensive care unit (PICU) in-hospital mortality risk model. Proposed displays were based on a model-agnostic, instance-level explanation approach based on feature influence, as determined by Shapley values. Focus group sessions solicited critical care provider feedback on the proposed displays, which were then revised accordingly. Results: The proposed displays were perceived as useful tools in assessing model predictions. However, specific explanation goals and information needs varied by clinical role and level of predictive modeling knowledge. Providers preferred explanation displays that required less information processing effort and could support the information needs of a variety of users. Providing supporting information to assist in interpretation was seen as critical for fostering provider understanding and acceptance of the predictions and explanations. The user-centered explanation display for the PICU in-hospital mortality risk model incorporated elements from the initial displays along with enhancements suggested by providers. Conclusions: We proposed a framework for the design of user-centered displays of explanations for ML models. We used the proposed framework to motivate the design of a user-centered display of an explanation for predictions from a PICU in-hospital mortality risk model. Positive feedback from focus group participants provides preliminary support for the use of model-agnostic, instance-level explanations of feature influence as an approach to understand ML model predictions in healthcare and advances the discussion on how to effectively communicate ML model information to healthcare providers.  © 2020 The Author(s).","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85092601699"
"Malhi A.; Kampik T.; Pannu H.; Madhikermi M.; Framling K.","Malhi, Avleen (56663163300); Kampik, Timotheus (56728735900); Pannu, Husanbir (57193135499); Madhikermi, Manik (56048426400); Framling, Kary (6506103412)","56663163300; 56728735900; 57193135499; 56048426400; 6506103412","Explaining Machine Learning-Based Classifications of In-Vivo Gastral Images","2019","2019 Digital Image Computing: Techniques and Applications, DICTA 2019","","","8945986","","","","20","10.1109/DICTA47822.2019.8945986","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078699959&doi=10.1109%2fDICTA47822.2019.8945986&partnerID=40&md5=f8e41b10153458e69e48c3acc4150bb9","This paper proposes an explainable machine learning tool that can potentially be used for decision support in medical image analysis scenarios. For a decision-support system it is important to be able to reverse-engineer the impact of features on the final decision outcome. In the medical domain, such functionality is typically required to allow applying machine learning to clinical decision making. In this paper, we present initial experiments that have been performed on in-vivo gastral images obtained from capsule endoscopy. Quantitative analysis has been performed to evaluate the utility of the proposed method. Convolutional neural networks have been used for training the validating of the image data set to provide the bleeding classifications. The visual explanations have been provided in the images to help health professionals trust the black box predictions. While the paper focuses on the in-vivo gastral image use case, most findings are generalizable. © 2019 IEEE.","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85078699959"
"Ward I.R.; Wang L.; Lu J.; Bennamoun M.; Dwivedi G.; Sanfilippo F.M.","Ward, Isaac Ronald (57211607330); Wang, Ling (58456763400); Lu, Juan (57260107600); Bennamoun, Mohammed (7004376121); Dwivedi, Girish (10340469500); Sanfilippo, Frank M (9736813100)","57211607330; 58456763400; 57260107600; 7004376121; 10340469500; 9736813100","Explainable artificial intelligence for pharmacovigilance: What features are important when predicting adverse outcomes? Explainable artificial intelligence for pharmacovigilance","2021","Computer Methods and Programs in Biomedicine","212","","106415","","","","19","10.1016/j.cmpb.2021.106415","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117921160&doi=10.1016%2fj.cmpb.2021.106415&partnerID=40&md5=1129b36640e04f5926a53caa9da2082e","Background and Objective. Explainable Artificial Intelligence (XAI) has been identified as a viable method for determining the importance of features when making predictions using Machine Learning (ML) models. In this study, we created models that take an individual's health information (e.g. their drug history and comorbidities) as inputs, and predict the probability that the individual will have an Acute Coronary Syndrome (ACS) adverse outcome. Methods. Using XAI, we quantified the contribution that specific drugs had on these ACS predictions, thus creating an XAI-based technique for pharmacovigilance monitoring, using ACS as an example of the adverse outcome to detect. Individuals aged over 65 who were supplied Musculo-skeletal system (anatomical therapeutic chemical (ATC) class M) or Cardiovascular system (ATC class C) drugs between 1993 and 2009 were identified, and their drug histories, comorbidities, and other key features were extracted from linked Western Australian datasets. Multiple ML models were trained to predict if these individuals would have an ACS related adverse outcome (i.e., death or hospitalisation with a discharge diagnosis of ACS), and a variety of ML and XAI techniques were used to calculate which features — specifically which drugs — led to these predictions. Results. The drug dispensing features for rofecoxib and celecoxib were found to have a greater than zero contribution to ACS related adverse outcome predictions (on average), and it was found that ACS related adverse outcomes can be predicted with 72% accuracy. Furthermore, the XAI libraries LIME and SHAP were found to successfully identify both important and unimportant features, with SHAP slightly outperforming LIME. Conclusions. ML models trained on linked administrative health datasets in tandem with XAI algorithms can successfully quantify feature importance, and with further development, could potentially be used as pharmacovigilance monitoring techniques. © 2021 Elsevier B.V.","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85117921160"
"Stanišić S.; Perišić M.; Jovanović G.; Maletić D.; Vudragović D.; Vranić A.; Stojić A.","Stanišić, Svetlana (56358144400); Perišić, Mirjana (58354176300); Jovanović, Gordana (57225371381); Maletić, Dimitrije (15728244200); Vudragović, Dušan (18038664000); Vranić, Ana (57219516504); Stojić, Andreja (28168051600)","56358144400; 58354176300; 57225371381; 15728244200; 18038664000; 57219516504; 28168051600","What Information on Volatile Organic Compounds Can Be Obtained from the Data of a Single Measurement Site Through the Use of Artificial Intelligence?","2021","Studies in Computational Intelligence","973","","","207","225","18","3","10.1007/978-3-030-72711-6_12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112019980&doi=10.1007%2f978-3-030-72711-6_12&partnerID=40&md5=041a8a4e855759b3a409d4fe1847373b","Increasing air pollutant concentrations over the last few decades have been a focus of contemporary scientific research due to adverse effects on public health, the environment and climate change. In this chapter, we used an innovative integrated methodology for spatio-temporal characterization of sources and concentration forecasts of toxic, mutagenic and carcinogenic representatives of volatile organic species—benzene, toluene, ethylbenzene and xylene, commonly referred to as BTEX. The methodology is based on receptor-oriented air circulation modeling and artificial intelligence implemented through machine learning and explainable artificial intelligence methods. The study covered two years of data obtained from a single monitoring station located at 54a Despota Stefana Boulevard (44 ∘ 49’68” N, 20 ∘ 28’04” E). This station was selected from the local and state network for air quality monitoring in the territory of Belgrade. The receptor-oriented modeling was effective for classifying sources of BTEX and the assessment of BTEX concentrations in the Belgrade urban area surrounding the receptor site that was not regularly monitored. The correlations and ratios between BTEX compounds were used for estimating their interrelationships and presence in the air, which contributed to the identification of their origin. Also, this study evaluated the possibilities of BTEX spatio-temporal forecasts based on the integrated methodology. For this purpose, XGBoost was efficient at forecasting BTEX levels, with estimated errors (6–15%) significantly below the uncertainty obtained by conventional models for the evaluation of average annual pollutant concentrations. The results suggest that temperature, wind speed and wind direction represented the main parameters which explain the spatio-temporal distribution of BTEX, while the impact of other factors showed significant variations depending on the locations of the receptor and the compound. © 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Book chapter","Final","","Scopus","2-s2.0-85112019980"
"Hong C.W.; Lee C.; Lee K.; Ko M.-S.; Hur K.","Hong, Chang Woo (57216618761); Lee, Changmin (57216649036); Lee, Kwangsuk (57205595406); Ko, Min-Seung (57211030307); Hur, Kyeon (8880164600)","57216618761; 57216649036; 57205595406; 57211030307; 8880164600","Explainable artificial intelligence for the remaining useful life prognosis of the turbofan engines","2020","Proceedings of the 3rd IEEE International Conference on Knowledge Innovation and Invention 2020, ICKII 2020","","","9318912","144","147","3","11","10.1109/ICKII50300.2020.9318912","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100581180&doi=10.1109%2fICKII50300.2020.9318912&partnerID=40&md5=dfc79cd937de467b95afa4694b35fc35","This paper proposes a deep-stacked neural network to prognose the remaining useful life of the turbofan engines and analyze results using explainable artificial intelligence. The proposed model prognoses the remaining useful life of the turbofan engines accurately by properly stacking a one-dimensional convolutional neural network (1D-CNN), long short-term memory (LSTM), and bidirectional LSTM algorithms. This model also uses dropout technique and residual network to enhance the prediction accuracy. The Explainable artificial intelligence analyzes the prognostic results of RUL. Using SHAP (SHapely Addictive exPlanation), this model can analyze features that have significantly influenced RUL prediction. The SHAP force plot and decision plot can help decision-makers of the turbofan engine properly manage the maintenance by showing the influence of sensors. © 2020 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85100581180"
"Shang Y.; Tian Y.; Zhou M.; Zhou T.; Lyu K.; Wang Z.; Xin R.; Liang T.; Zhu S.; Li J.","Shang, Yong (57200260587); Tian, Yu (56461757000); Zhou, Min (57198736758); Zhou, Tianshu (55450273700); Lyu, Kewei (57224311947); Wang, Zhixiao (57217624541); Xin, Ran (57224319006); Liang, Tingbo (7202019213); Zhu, Shiqiang (7404391617); Li, Jingsong (35766898100)","57200260587; 56461757000; 57198736758; 55450273700; 57224311947; 57217624541; 57224319006; 7202019213; 7404391617; 35766898100","EHR-Oriented Knowledge Graph System: Toward Efficient Utilization of Non-Used Information Buried in Routine Clinical Practice","2021","IEEE Journal of Biomedical and Health Informatics","25","7","9444689","2463","2475","12","16","10.1109/JBHI.2021.3085003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107345830&doi=10.1109%2fJBHI.2021.3085003&partnerID=40&md5=ab58b4b64d92788a3a4704b53e215d66","Non-used clinical information has negative implications on healthcare quality. Clinicians pay priority attention to clinical information relevant to their specialties during routine clinical practices but may be insensitive or less concerned about information showing disease risks beyond their specialties, resulting in delayed and missed diagnoses or improper management. In this study, we introduced an electronic health record (EHR)-oriented knowledge graph system to efficiently utilize non-used information buried in EHRs. EHR data were transformed into a semantic patient-centralized information model under the ontology structure of a knowledge graph. The knowledge graph then creates an EHR data trajectory and performs reasoning through semantic rules to identify important clinical findings within EHR data. A graphical reasoning pathway illustrates the reasoning footage and explains the clinical significance for clinicians to better understand the neglected information. An application study was performed to evaluate unconsidered chronic kidney disease (CKD) reminding for non-nephrology clinicians to identify important neglected information. The study covered 71,679 patients in non-nephrology departments. The system identified 2,774 patients meeting CKD diagnosis criteria and 10,377 patients requiring high attention. A follow-up study of 5,439 patients showed that 82.1% of patients who met the diagnosis criteria and 61.4% of patients requiring high attention were confirmed to be CKD positive during follow-up research. The application demonstrated that the proposed approach is feasible and effective in clinical information utilization. Additionally, it's valuable as an explainable artificial intelligence to provide interpretable recommendations for specialist physicians to understand the importance of non-used data and make comprehensive decisions.  © 2013 IEEE.","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85107345830"
"Omri N.; Masry Z.A.; Mairot N.; Giampiccolo S.; Zerhouni N.","Omri, Nabil (57202969413); Masry, Zeina Al (56989876800); Mairot, Nicolas (57210378605); Giampiccolo, Sylvian (57210371386); Zerhouni, Noureddine (7003536461)","57202969413; 56989876800; 57210378605; 57210371386; 7003536461","X-PHM: Prognostics and health management knowledge-based framework for SME","2021","Procedia CIRP","104","","","1595","1600","5","2","10.1016/j.procir.2021.11.269","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121596365&doi=10.1016%2fj.procir.2021.11.269&partnerID=40&md5=72223d68ae8daad15738a17d99cf5e01","Prognostics and Health Management (PHM) is an emerging concept based on industrial data management. The implementation of PHM in small and medium-sized enterprises (SMEs) is currently limited due to data accessibility difficulties. In order to overcome this pitfall, one could formalize the operators' knowledge and integrate it in the SME's information system. Thus, the implementation of the PHM will be based on this information system associating data with knowledge. To this end, we propose a collaborative PHM approach (X-PHM) to ensure the extraction of operators' knowledge and its integration into the PHM process. The decision resulting from this approach is restituted with a concern of explainability. This paper details the proposed approach while focusing on the data management process and its integration in explainable decisions. This new framework is applied in a French SME to understand its production process and facilitate its digital transformation. © 2021 Elsevier B.V.. All rights reserved.","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85121596365"
"Jang S.-I.; Girard M.J.A.; Thiery A.H.","Jang, Se-In (57317669100); Girard, Michaël J.A. (24721263100); Thiery, Alexandre H. (55780359100)","57317669100; 24721263100; 55780359100","Explainable diabetic retinopathy classification based on neural-symbolic learning","2021","CEUR Workshop Proceedings","2986","","","104","113","9","2","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118246567&partnerID=40&md5=dc7ab1b47b2f5560a4da1feee718e4a3","In this paper, we propose an explainable diabetic retinopathy (ExplainDR) classification model based on neural-symbolic learning. To gain explainability, a high-level symbolic representation should be considered in decision making. Specifically, we introduce a human-readable symbolic representation, which follows a taxonomy style of diabetic retinopathy characteristics related to eye health conditions to achieve explainability. We then include human-readable features obtained from the symbolic representation in the disease prediction. Experimental results on a diabetic retinopathy classification dataset show that our proposed ExplainDR method exhibits promising performance when compared to that from state-of-the-art methods applied to the IDRiD dataset, while also providing interpretability and explainability. © CEUR Workshop Proceedings 2021.","Conference paper","Final","","Scopus","2-s2.0-85118246567"
"Khan P.F.; Meehan K.","Khan, Pathan Faisal (57222073562); Meehan, Kevin (57219441369)","57222073562; 57219441369","Diabetes prognosis using white-box machine learning framework for interpretability of results","2021","2021 IEEE 11th Annual Computing and Communication Workshop and Conference, CCWC 2021","","","9375927","1501","1506","5","3","10.1109/CCWC51732.2021.9375927","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103446830&doi=10.1109%2fCCWC51732.2021.9375927&partnerID=40&md5=e1adb897fec6f493c8313a97fdd0e4b3","Artificial intelligence solutions in the healthcare sector are a fundamental phenomenon. It has enabled medical practitioners to perform high quality and precision treatments to prevent diseases or cure a patient. While it is essential to use such solutions, it is also more important to make these solutions transparent to medical professionals. Doctors rely on the cause behind a prognosis rather than just the binary result. This study provides an insight into the feasibility and importance of explainable artificial intelligence solutions for the healthcare sector. A case-study on diabetes in Pima Indian females aids this research motive. The study has maintained good explainability of the predictions and high accuracy by the machine learning models used. This study used a white-box machine learning framework, local interpretable model-agnostic explanations, to prove the cause. The framework successfully interpreted case-by-case predictions of some machine learning models. The machine learning models, while being interpretable, also provided high accuracy in prediction. The highest accuracy, 80.5%, was shown by a random forest model. The study found out glucose levels as the most contributing factors for the outcome of diabetes. The results from this study can be used by researchers to reevaluate their position on white-box machine-learning solutions in the healthcare sector. © 2021 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85103446830"
"Zhou Y.; Boussard M.; Delaborde A.","Zhou, Yongxin (57226595395); Boussard, Matthieu (57204166287); Delaborde, Agnes (58680788000)","57226595395; 57204166287; 58680788000","Towards an XAI-Assisted Third-Party Evaluation of AI Systems: Illustration on Decision Trees","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12688 LNAI","","","158","172","14","0","10.1007/978-3-030-82017-6_10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113333532&doi=10.1007%2f978-3-030-82017-6_10&partnerID=40&md5=db57674cad21e4523b43981b1f61d564","We explored the potential contribution of eXplainable Artificial Intelligence (XAI) for the evaluation of Artificial Intelligence (AI), in a context where such an evaluation is performed by independent third-party evaluators, for example in the objective of certification. The experimental approach of this paper is based on “explainable by design” decision trees that produce predictions on health data and bank data. Results presented in this paper show that the explanations could be used by the evaluators to identify the parameters used in decision making and their levels of importance. The explanations would thus make it possible to orient the constitution of the evaluation corpus, to explore the rules followed for decision-making and to identify potentially critical relationships between different parameters. In addition, the explanations make it possible to inspect the presence of bias in the database and in the algorithm. These first results lay the groundwork for further additional research in order to generalize the conclusions of this paper to different XAI methods. © 2021, Springer Nature Switzerland AG.","Conference paper","Final","","Scopus","2-s2.0-85113333532"
"Meske C.; Bunde E.","Meske, Christian (55806873000); Bunde, Enrico (57218312382)","55806873000; 57218312382","Transparency and trust in human-AI-interaction: The role of model-agnostic explanations in computer vision-based decision support","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12217 LNCS","","","54","69","15","40","10.1007/978-3-030-50334-5_4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088749501&doi=10.1007%2f978-3-030-50334-5_4&partnerID=40&md5=148e3c6a2b5a8b08ce89bbea3b044259","Computer Vision, and hence Artificial Intelligence-based extraction of information from images, has increasingly received attention over the last years, for instance in medical diagnostics. While the algorithms’ complexity is a reason for their increased performance, it also leads to the ‘black box’ problem, consequently decreasing trust towards AI. In this regard, “Explainable Artificial Intelligence” (XAI) allows to open that black box and to improve the degree of AI transparency. In this paper, we first discuss the theoretical impact of explainability on trust towards AI, followed by showcasing how the usage of XAI in a health-related setting can look like. More specifically, we show how XAI can be applied to understand why Computer Vision, based on deep learning, did or did not detect a disease (malaria) on image data (thin blood smear slide images). Furthermore, we investigate, how XAI can be used to compare the detection strategy of two different deep learning models often used for Computer Vision: Convolutional Neural Network and Multi-Layer Perceptron. Our empirical results show that i) the AI sometimes used questionable or irrelevant data features of an image to detect malaria (even if correctly predicted), and ii) that there may be significant discrepancies in how different deep learning models explain the same prediction. Our theoretical discussion highlights that XAI can support trust in Computer Vision systems, and AI systems in general, especially through an increased understandability and predictability. © Springer Nature Switzerland AG 2020.","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85088749501"
"Gobbo B.","Gobbo, Beatrice (57210260894)","57210260894","Explaining AI Through Critical Reflection Artifacts: On the Role of Communication Design Within XAI","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12585 LNCS","","","184","188","4","0","10.1007/978-3-030-68007-7_12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102617350&doi=10.1007%2f978-3-030-68007-7_12&partnerID=40&md5=efac0a9d8483112c21c9494d1ac6038c","Artificial intelligence algorithms – and data that feed them – are increasingly imbued with agency and impact and are empowered to make decisions in our lives in a wide variety of domains: from search engines, information filtering, political campaigns, health to the prediction of criminal recidivism or loan repayment. Indeed, algorithms are difficult to understand, and explaining “how they exercise their power and influence” and how a given input (whether or not consciously released) is transformed into an output. In the computer science field, techniques of explainable artificial intelligence (XAI) have been developed for disclosing and studying algorithmic models, using data visualization as visual language to let experts explore their inner workings. However, current research on machine learning explainability empowers the creators of machine learning models but is not addressing the needs of people affected by them. This paper leverages on communication and information design methods (or competences) to expand the explainable machine learning field of action towards the general public. © 2021, Springer Nature Switzerland AG.","Conference paper","Final","","Scopus","2-s2.0-85102617350"
"Pnevmatikakis A.; Kanavos S.; Matikas G.; Kostopoulou K.; Cesario A.; Kyriazakos S.","Pnevmatikakis, Aristodemos (6507698316); Kanavos, Stathis (57222471649); Matikas, George (57224263908); Kostopoulou, Konstantina (57222465713); Cesario, Alfredo (7006465542); Kyriazakos, Sofoklis (6602529684)","6507698316; 57222471649; 57224263908; 57222465713; 7006465542; 6602529684","Risk assessment for personalized health insurance based on real-world data","2021","Risks","9","3","46","1","15","14","16","10.3390/risks9030046","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102726395&doi=10.3390%2frisks9030046&partnerID=40&md5=4d8a4c0a891dbbdc40251ceeccdf5f6a","The way one leads their life is considered an important factor in health. In this paper we propose a system to provide risk assessment based on behavior for the health insurance sector. To do so we built a platform to collect real-world data that enumerate different aspects of behavior, and a simulator to augment actual data with synthetic. Using the data, we built classifiers to predict variations in important quantities for the lifestyle of a person. We offer a risk assessment service to the health insurance professionals by manipulating the classifier predictions in the long-term. We also address virtual coaching by using explainable Artificial Intelligence (AI) techniques on the classifier itself to gain insights on the advice to be offered to insurance customers. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85102726395"
"Abadía J.J.P.; Fritz H.; Dadoulis G.; Dragos K.; Smarsly K.","Abadía, José Joaquín Peralta (57312888700); Fritz, Henrieke (57218247050); Dadoulis, Georgios (57217491055); Dragos, Kosmas (56928464600); Smarsly, Kay (8880230300)","57312888700; 57218247050; 57217491055; 56928464600; 8880230300","Automated decision making in structural health monitoring using explainable artificial intelligence","2021","EG-ICE 2021 Workshop on Intelligent Computing in Engineering, Proceedings","","","","432","441","9","3","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134227634&partnerID=40&md5=56788345ff3823d918d79e042d04f0a0","The need for processing large amounts of data from modern structural health monitoring (SHM) systems has been fostering interdisciplinary SHM strategies employing artificial intelligence (AI) algorithms for detecting damage. However, the opacity of several AI algorithms hinders their widespread adoption in SHM practice. To enhance the trust of practitioners in AI algorithms, this paper proposes an explainable artificial intelligence (XAI) approach for SHM. The approach builds upon the capabilities of unsupervised learning algorithms for detecting outliers indicative of structural damage in structural response data. Moreover, features in the data governing outlier detection are “explained” to the user, thus ensuring transparency in decision making. The XAI-SHM approach is validated via simulations of a pedestrian bridge that may or may not include damage. Results show that the XAI-SHM approach is capable of distinguishing between damage and random fluctuations of structural properties, while decisions made by the XAI-SHM approach are clearly explained. © 2021 Universitätsverlag der Technischen Universität Berlin. All Rights Reserved.","Conference paper","Final","","Scopus","2-s2.0-85134227634"
"Cho E.; Son J.; Iordanov V.B.; Jun S.; Baek J.; Lee S.; Lee C.; Choi J.-M.; Gobana F.W.; Park S.","Cho, Eunyoung (57205301446); Son, Jeany (56112184000); Iordanov, Vladimirov Blagovest (23092152200); Jun, Sungwoo (57213189514); Baek, Jaeuk (57199177370); Lee, SoYeon (7601413373); Lee, Changeun (38163044200); Choi, Jin-Mo (57219646375); Gobana, Feyissa Woyano (57188758394); Park, Sangjoon (57191670772)","57205301446; 56112184000; 23092152200; 57213189514; 57199177370; 7601413373; 38163044200; 57219646375; 57188758394; 57191670772","Toward Effective IT Services in Defence Talent Management Platform","2020","International Conference on ICT Convergence","2020-October","","9289499","1605","1608","3","0","10.1109/ICTC49870.2020.9289499","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098932785&doi=10.1109%2fICTC49870.2020.9289499&partnerID=40&md5=150be04d860ab9bb3526cb33523c9b48","There are various problems in society that can be solved by IT technology, and we would like to provide a technique to mitigate safety incidents and accidents that occur continuously in military life. As the current use of smart devices by soldiers has had a positive effect, it becomes possible to provide specialized care services such as personalized health care, psychology care, physical strength check-up and feedback by AI coaches, based on the requirements of the soldiers. These services can provide a reliable and safe environment for personnel fulfilling the duty of defense. However, in order to figure out the psychological factors of problems such as barracks accidents caused by special environments of soldiers, those services need an AI technology with the level of explainable Artificial Intelligence (XAI) or Artificial Wisdom (AW) beyond handling a psychological feedback. Therefore, in this paper, we propose IT platform services that are able to support the needs currently raised to deal with problems that arise in the mid- to long-term life of the specific group as universal social welfare issues. © 2020 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85098932785"
"Ghassemi M.; Oakden-Rayner L.; Beam A.L.","Ghassemi, Marzyeh (56305414400); Oakden-Rayner, Luke (57190029830); Beam, Andrew L (36452687100)","56305414400; 57190029830; 36452687100","The false hope of current approaches to explainable artificial intelligence in health care","2021","The Lancet Digital Health","3","11","","e745","e750","5","420","10.1016/S2589-7500(21)00208-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121190788&doi=10.1016%2fS2589-7500%2821%2900208-9&partnerID=40&md5=bc8efac19321a390d50efd67a82ffba7","The black-box nature of current artificial intelligence (AI) has caused some to question whether AI must be explainable to be used in high-stakes scenarios such as medicine. It has been argued that explainable AI will engender trust with the health-care workforce, provide transparency into the AI decision making process, and potentially mitigate various kinds of bias. In this Viewpoint, we argue that this argument represents a false hope for explainable AI and that current explainability methods are unlikely to achieve these goals for patient-level decision support. We provide an overview of current explainability techniques and highlight how various failure cases can cause problems for decision making for individual patients. In the absence of suitable explainability methods, we advocate for rigorous internal and external validation of AI models as a more direct means of achieving the goals often associated with explainability, and we caution against having explainability be a requirement for clinically deployed models. © 2021 The Author(s). Published by Elsevier Ltd. This is an Open Access article under the CC BY 4.0 license","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85121190788"
"Collecchia G.","Collecchia, Giampaolo (6506292685)","6506292685","Let’s open the black box: eXplainable Artificial Intelligence (XAI); [Apriamo la scatola nera: l’intelligenza artificiale spiegabile]","2021","Recenti Progressi in Medicina","112","11","","709","710","1","0","10.1701/3696.36848","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121960701&doi=10.1701%2f3696.36848&partnerID=40&md5=903fc9819520cb5d14017060f4ef7579","The so-called “opacity” of artificial intelligence (AI), the black box model, raises concerns about the increasing use in relevant and critical areas of our society such as health, assistance, autonomous driving, military technologies, the economy, finance, justice, insurance. The need to create models that make it possible to understand, trust and therefore govern the emerging generation of these systems is increasingly felt, while maintaining a high level of performance. This is why a new discipline is being developed, eXplainable AI (XAI), which can be defined as a set of tools and techniques used to make the operation of AI increasingly transparent and easy to understand. © 2021 Il Pensiero Scientifico Editore s.r.l.. All rights reserved.","Article","Final","","Scopus","2-s2.0-85121960701"
"Keller N.; Jenny M.A.; Spies C.A.; Herzog S.M.","Keller, Niklas (56166933400); Jenny, Mirjam A. (55479321400); Spies, Claudia A. (7005143930); Herzog, Stefan M. (16070110000)","56166933400; 55479321400; 7005143930; 16070110000","Augmenting Decision Competence in Healthcare Using AI-based Cognitive Models","2020","2020 IEEE International Conference on Healthcare Informatics, ICHI 2020","","","9374376","","","","6","10.1109/ICHI48887.2020.9374376","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103167626&doi=10.1109%2fICHI48887.2020.9374376&partnerID=40&md5=a45406ddd7bf12cb66d7237a09042bba","In many critical decisions, such as medicine, transparency of the underlying decision process is critical. This extends to decision processes that are supported by artificial intelligence. Rather than using a post-hoc explainability approach from explainable AI research (SHAP or LIME), we develop and test an intrinsically transparent and intuitively interpretable model developed from cognitive science, fast-And-frugal trees, in a comparative analysis with state-of-The-Art machine learning models. The resultant decision support can be easily implemented as laminated pocket card, augmenting the decision competence of physicians rather than replacing it. © 2020 IEEE.","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85103167626"
"Kim J.; Lee S.; Hwang E.; Ryu K.S.; Jeong H.; Lee J.W.; Hwangbo Y.; Choi K.S.; Cha H.S.","Kim, Junetae (57189001212); Lee, Sangwon (57214838599); Hwang, Eugene (57542512000); Ryu, Kwang Sun (50263181500); Jeong, Hanseok (58101282200); Lee, Jae Wook (56025142500); Hwangbo, Yul (57147438700); Choi, Kui Son (36760529200); Cha, Hyo Soung (55943223800)","57189001212; 57214838599; 57542512000; 50263181500; 58101282200; 56025142500; 57147438700; 36760529200; 55943223800","Limitations of deep learning attention mechanisms in clinical research: Empirical case study based on the Korean diabetic disease setting","2020","Journal of Medical Internet Research","22","12","e18418","","","","13","10.2196/18418","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097752131&doi=10.2196%2f18418&partnerID=40&md5=8d50f7e958a0e9a2a455f68bbefdf30a","Background: Despite excellent prediction performance, noninterpretability has undermined the value of applying deep-learning algorithms in clinical practice. To overcome this limitation, attention mechanism has been introduced to clinical research as an explanatory modeling method. However, potential limitations of using this attractive method have not been clarified to clinical researchers. Furthermore, there has been a lack of introductory information explaining attention mechanisms to clinical researchers. Objective: The aim of this study was to introduce the basic concepts and design approaches of attention mechanisms. In addition, we aimed to empirically assess the potential limitations of current attention mechanisms in terms of prediction and interpretability performance. Methods: First, the basic concepts and several key considerations regarding attention mechanisms were identified. Second, four approaches to attention mechanisms were suggested according to a two-dimensional framework based on the degrees of freedom and uncertainty awareness. Third, the prediction performance, probability reliability, concentration of variable importance, consistency of attention results, and generalizability of attention results to conventional statistics were assessed in the diabetic classification modeling setting. Fourth, the potential limitations of attention mechanisms were considered. Results: Prediction performance was very high for all models. Probability reliability was high in models with uncertainty awareness. Variable importance was concentrated in several variables when uncertainty awareness was not considered. The consistency of attention results was high when uncertainty awareness was considered. The generalizability of attention results to conventional statistics was poor regardless of the modeling approach. Conclusions: The attention mechanism is an attractive technique with potential to be very promising in the future. However, it may not yet be desirable to rely on this method to assess variable importance in clinical settings. Therefore, along with theoretical studies enhancing attention mechanisms, more empirical studies investigating potential limitations should be encouraged. © Junetae Kim, Sangwon Lee, Eugene Hwang, Kwang Sun Ryu, Hanseok Jeong, Jae Wook Lee, Yul Hwangbo, Kui Son Choi, Hyo Soung Cha. Originally published in the Journal of Medical Internet Research (http://www.jmir.org), 16.12.2020. This is an open-access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work, first published in the Journal of Medical Internet Research, is properly cited. The complete bibliographic information, a link to the original publication on http://www.jmir.org/, as well as this copyright and license information must be included.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85097752131"
"Nahmias D.O.; Kontson K.L.","Nahmias, David O. (57202955293); Kontson, Kimberly L. (55841179400)","57202955293; 55841179400","Easy Perturbation EEG Algorithm for Spectral Importance (easyPEASI): A Simple Method to Identify Important Spectral Features of EEG in Deep Learning Models","2020","Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining","","","","2398","2406","8","18","10.1145/3394486.3403289","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090411045&doi=10.1145%2f3394486.3403289&partnerID=40&md5=90c03d5399548b5efb7964657e51cd17","Efforts into understanding neurological differences between populations is an active area of research. Deep learning has recently shown promising results using EEG as input to distinguish recordings of subjects based on neurological activity. However, only about one quarter of these studies investigate the underlying neurophysiological implications. This work proposes and validates a method to investigate frequency bands important to EEG-driven deep learning models. Easy perturbation EEG algorithm for spectral importance (easyPEASI) is simpler than previous methods and requires only perturbations to input data. We validate easyPEASI on EEG pathology classification using the Temple University Health EEG Corpus. easyPEASI is further applied to characterize the effects of patients' medications on brain rhythms. We investigate classifications of patients taking one of two anticonvulsant medications, Dilantin (phenytoin) and Keppra (levetiracetam), and subjects taking no medications. We find that for recordings of subjects with clinically-determined normal EEG that these medications effect the Theta and Alpha band most significantly. For recordings with clinically-determined abnormal EEG these medications affected the Delta, Theta, and Alpha bands most significantly. We also find the Beta band to be affected differently by the two medications. Results found here show promise for a method of obtaining explainable artificial intelligence and interpretable models from EEG-driven deep learning through a simpler more accessible method perturbing only input data. Overall, this work provides a fast, easy, and reproducible method to automatically determine salient spectral features of neural activity that have been learned by machine learning models, such as deep learning. © 2020 Owner/Author.","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85090411045"
"Panigutti C.; Perotti A.; Panisson A.; Bajardi P.; Pedreschi D.","Panigutti, Cecilia (57194345147); Perotti, Alan (58321306400); Panisson, André (9133831400); Bajardi, Paolo (35114469000); Pedreschi, Dino (6603935985)","57194345147; 58321306400; 9133831400; 35114469000; 6603935985","FairLens: Auditing black-box clinical decision support systems","2021","Information Processing and Management","58","5","102657","","","","41","10.1016/j.ipm.2021.102657","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108332873&doi=10.1016%2fj.ipm.2021.102657&partnerID=40&md5=5baef6cbf23a8b6c54abb95fffcbc14d","The pervasive application of algorithmic decision-making is raising concerns on the risk of unintended bias in AI systems deployed in critical settings such as healthcare. The detection and mitigation of model bias is a very delicate task that should be tackled with care and involving domain experts in the loop. In this paper we introduce FairLens, a methodology for discovering and explaining biases. We show how this tool can audit a fictional commercial black-box model acting as a clinical decision support system (DSS). In this scenario, the healthcare facility experts can use FairLens on their historical data to discover the biases of the model before incorporating it into the clinical decision flow. FairLens first stratifies the available patient data according to demographic attributes such as age, ethnicity, gender and healthcare insurance; it then assesses the model performance on such groups highlighting the most common misclassifications. Finally, FairLens allows the expert to examine one misclassification of interest by explaining which elements of the affected patients’ clinical history drive the model error in the problematic group. We validate FairLens’ ability to highlight bias in multilabel clinical DSSs introducing a multilabel-appropriate metric of disparity and proving its efficacy against other standard metrics. © 2021","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85108332873"
"El Mir H.; Perinpanayagam S.","El Mir, Haroun (57217630678); Perinpanayagam, Suresh (36617783100)","57217630678; 36617783100","Certification Approach for Physics Informed Machine Learning and its Application in Landing Gear Life Assessment","2021","AIAA/IEEE Digital Avionics Systems Conference - Proceedings","2021-October","","","","","","3","10.1109/DASC52595.2021.9594374","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122805935&doi=10.1109%2fDASC52595.2021.9594374&partnerID=40&md5=31d7d7acac938176475604380c76815d","The efficacy of fatigue life approximation methodologies for Landing Gear systems is studied and compared to the ongoing Structural Health Monitoring techniques being researched, which will forecast failures based on the system's specific life and withstanding abilities, ranging from creating a digital simulation model to applying neural network technologies, in order to simulate and approximate locations and levels of failure along the structure. Explainable Artificial Intelligence allows for the ease-of-integration of Deep Neural Network data into Predictive Maintenance, which is a procedure focused on the health of a system and its efficient upkeep via the use of sensor-based data. Test data from a flight includes a multitude of conditions and varying parameters such as the surface of the landing strip as well as the aircraft itself, requiring the use of Deep Neural Network models for damage assessment and failure anticipation, where compliance to standards is a major question raised, as the EASA AI roadmap is followed, as well as the ICAO and FAA. This paper additionally discusses the challenges faced with respect to standardizing the Explainable AI methodologies and their parameters specifically for the case of Landing Gear.  © 2021 IEEE.","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85122805935"
"Palatnik de Sousa I.; Maria Bernardes Rebuzzi Vellasco M.; Costa da Silva E.","Palatnik de Sousa, Iam (56313836500); Maria Bernardes Rebuzzi Vellasco, Marley (56200580000); Costa da Silva, Eduardo (14009474700)","56313836500; 56200580000; 14009474700","Local Interpretable Model-Agnostic Explanations for Classification of Lymph Node Metastases","2019","Sensors (Basel, Switzerland)","19","13","","","","","33","10.3390/s19132969","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069324122&doi=10.3390%2fs19132969&partnerID=40&md5=63616bc512d6a72e38075c4465623035","An application of explainable artificial intelligence on medical data is presented. There is an increasing demand in machine learning literature for such explainable models in health-related applications. This work aims to generate explanations on how a Convolutional Neural Network (CNN) detects tumor tissue in patches extracted from histology whole slide images. This is achieved using the ""locally-interpretable model-agnostic explanations"" methodology. Two publicly-available convolutional neural networks trained on the Patch Camelyon Benchmark are analyzed. Three common segmentation algorithms are compared for superpixel generation, and a fourth simpler parameter-free segmentation algorithm is proposed. The main characteristics of the explanations are discussed, as well as the key patterns identified in true positive predictions. The results are compared to medical annotations and literature and suggest that the CNN predictions follow at least some aspects of human expert knowledge.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85069324122"
"Lindsay L.; Coleman S.; Kerr D.; Taylor B.; Moorhead A.","Lindsay, Leeanne (57207735162); Coleman, Sonya (7201402808); Kerr, Dermot (24376886200); Taylor, Brian (7403036731); Moorhead, Anne (36174297600)","57207735162; 7201402808; 24376886200; 7403036731; 36174297600","Explainable Artificial Intelligence for Falls Prediction","2020","Communications in Computer and Information Science","1244 CCIS","","","76","84","8","4","10.1007/978-981-15-6634-9_8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089246941&doi=10.1007%2f978-981-15-6634-9_8&partnerID=40&md5=b02a210e21ca675bbf2205a795c43e72","With a rapidly ageing population, it is likely that we will encounter an older adult falling. Falls can cause death, serious injury or harm, loss of confidence and loss of independence. Falling can happen to any of us, however those over 65 years of age can be classified as a group of adults who are more vulnerable and at increased risk of falling. This paper focuses on applying explainable artificial intelligence techniques, in the form of decision trees, to healthcare data in order to predict the risk of falling in older adults. These decision trees could potentially be introduced for health and social care professionals to help aid their judgements when making decisions. © 2020, Springer Nature Singapore Pte Ltd.","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85089246941"
"Pesquita C.","Pesquita, Catia (24467133700)","24467133700","Towards semantic integration for explainable artificial intelligence in the biomedical domain","2021","HEALTHINF 2021 - 14th International Conference on Health Informatics; Part of the 14th International Joint Conference on Biomedical Engineering Systems and Technologies, BIOSTEC 2021","","","","747","753","6","14","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103858298&partnerID=40&md5=6178138bc7873ba7deef470d3053b03a","Explainable artificial intelligence typically focuses on data-based explanations, lacking the semantic context needed to produce human-centric explanations. This is especially relevant in healthcare and life sciences where the heterogeneity in both data sources and user expertise, and the underlying complexity of the domain and applications poses serious challenges. The Semantic Web represents an unparalleled opportunity in this area: it provides large amounts of freely available data in the form of Knowledge Graphs, which link data to ontologies, and can thus act as background knowledge for building explanations closer to human conceptualizations. In particular, knowledge graphs support the computation of semantic similarity between objects, providing an understanding of why certain objects are considered similar or different. This is a basic aspect of explainability and is at the core of many machine learning applications. However, when data covers multiple domains, it may be necessary to integrate different ontologies to cover the full semantic landscape of the underlying data. We propose a methodology for semantic explanations in the biomedical domain that is based on the semantic annotation and integration of heterogenous data into a common semantic landscape that supports semantic similarity assessments. This methodology builds upon state of the art semantic web technologies and produces post-hoc explanations that are independent of the machine learning method employed. Copyright © 2021 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved","Conference paper","Final","","Scopus","2-s2.0-85103858298"
"Ammar N.; Shaban-Nejad A.","Ammar, Nariman (57217244894); Shaban-Nejad, Arash (13405260500)","57217244894; 13405260500","Explainable artificial intelligence recommendation system by leveraging the semantics of adverse childhood experiences: Proof-of-concept prototype development","2020","JMIR Medical Informatics","8","11","e18752","","","","32","10.2196/18752","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097463874&doi=10.2196%2f18752&partnerID=40&md5=9ef23f6a3d11660a8287945846b0bd4e","Background: The study of adverse childhood experiences and their consequences has emerged over the past 20 years. Although the conclusions from these studies are available, the same is not true of the data. Accordingly, it is a complex problem to build a training set and develop machine-learning models from these studies. Classic machine learning and artificial intelligence techniques cannot provide a full scientific understanding of the inner workings of the underlying models. This raises credibility issues due to the lack of transparency and generalizability. Explainable artificial intelligence is an emerging approach for promoting credibility, accountability, and trust in mission-critical areas such as medicine by combining machine-learning approaches with explanatory techniques that explicitly show what the decision criteria are and why (or how) they have been established. Hence, thinking about how machine learning could benefit from knowledge graphs that combine “common sense” knowledge as well as semantic reasoning and causality models is a potential solution to this problem. Objective: In this study, we aimed to leverage explainable artificial intelligence, and propose a proof-of-concept prototype for a knowledge-driven evidence-based recommendation system to improve mental health surveillance. Methods: We used concepts from an ontology that we have developed to build and train a question-answering agent using the Google DialogFlow engine. In addition to the question-answering agent, the initial prototype includes knowledge graph generation and recommendation components that leverage third-party graph technology. Results: To showcase the framework functionalities, we here present a prototype design and demonstrate the main features through four use case scenarios motivated by an initiative currently implemented at a children’s hospital in Memphis, Tennessee. Ongoing development of the prototype requires implementing an optimization algorithm of the recommendations, incorporating a privacy layer through a personal health library, and conducting a clinical trial to assess both usability and usefulness of the implementation. Conclusions: This semantic-driven explainable artificial intelligence prototype can enhance health care practitioners’ ability to provide explanations for the decisions they make. ©Nariman Ammar, Arash Shaban-Nejad.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85097463874"
"Lira D.B.; Xavier F.; Digiampietri L.A.","Lira, Diego Bezerra (57226057579); Xavier, Fernando (57205662218); Digiampietri, Luciano Antonio (8843300600)","57226057579; 57205662218; 8843300600","Combining clustering and classification algorithms for automatic bot detection: A case study on posts about COVID-19","2021","ACM International Conference Proceeding Series","","","3466970","","","","0","10.1145/3466933.3466970","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110325604&doi=10.1145%2f3466933.3466970&partnerID=40&md5=d66e55e7b9b15c55ef125f1335e2e06a","In the last decade, there has been a great insertion of bots in several social media. Among the potentially harmful effects of these software agents, there are: the spread of computer viruses and different internet scams, and the spread of fake news, with emphasis on political-electoral and public health-related news. This work presents a new approach for bots' detection on Twitter, combining the use of feature selection, clustering, and classification algorithms. The proposed approach was compared with more conventional ones (for example, without the use of clustering) and the premise used in this work proved to be true: the use of clustering, together with the features selection, allowed the production of better classification models in order to identify not only the bots who have an activity profile considered non-human (extremely active on Twitter) but also other bots whose profiles are more similar to humans' ones. The best results of automatic detection of bots reached an overall accuracy of 96.8% and F1 score equal to 0.622. As an additional advantage, these values were achieved by decision-tree models, which can be considered explainable artificial intelligence models. © 2021 ACM.","Conference paper","Final","","Scopus","2-s2.0-85110325604"
"Balcombe L.; De Leo D.","Balcombe, Luke (56375240700); De Leo, Diego (7006128644)","56375240700; 7006128644","Digital mental health challenges and the horizon ahead for solutions","2021","JMIR Mental Health","8","3","e26811","","","","37","10.2196/26811","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103667590&doi=10.2196%2f26811&partnerID=40&md5=33000c9c7bf101f0c3b42aebb04f3e48","The demand outstripping supply of mental health resources during the COVID-19 pandemic presents opportunities for digital technology tools to fill this new gap and, in the process, demonstrate capabilities to increase their effectiveness and efficiency. However, technology-enabled services have faced challenges in being sustainably implemented despite showing promising outcomes in efficacy trials since the early 2000s. The ongoing failure of these implementations has been addressed in reconceptualized models and frameworks, along with various efforts to branch out among disparate developers and clinical researchers to provide them with a key for furthering evaluative research. However, the limitations of traditional research methods in dealing with the complexities of mental health care warrant a diversified approach. The crux of the challenges of digital mental health implementation is the efficacy and evaluation of existing studies. Web-based interventions are increasingly used during the pandemic, allowing for affordable access to psychological therapies. However, a lagging infrastructure and skill base has limited the application of digital solutions in mental health care. Methodologies need to be converged owing to the rapid development of digital technologies that have outpaced the evaluation of rigorous digital mental health interventions and strategies to prevent mental illness. The functions and implications of human-computer interaction require a better understanding to overcome engagement barriers, especially with predictive technologies. Explainable artificial intelligence is being incorporated into digital mental health implementation to obtain positive and responsible outcomes. Investment in digital platforms and associated apps for real-time screening, tracking, and treatment offer the promise of cost-effectiveness in vulnerable populations. Although machine learning has been limited by study conduct and reporting methods, the increasing use of unstructured data has strengthened its potential. Early evidence suggests that the advantages outweigh the disadvantages of incrementing such technology. The limitations of an evidence-based approach require better integration of decision support tools to guide policymakers with digital mental health implementation. There is a complex range of issues with effectiveness, equity, access, and ethics (eg, privacy, confidentiality, fairness, transparency, reproducibility, and accountability), which warrant resolution. Evidence-informed policies, development of eminent digital products and services, and skills to use and maintain these solutions are required. Studies need to focus on developing digital platforms with explainable artificial intelligence–based apps to enhance resilience and guide the treatment decisions of mental health practitioners. Investments in digital mental health should ensure their safety and workability. End users should encourage the use of innovative methods to encourage developers to effectively evaluate their products and services and to render them a worthwhile investment. Technology-enabled services in a hybrid model of care are most likely to be effective (eg, specialists using these services among vulnerable, at-risk populations but not severe cases of mental ill health). © Luke Balcombe, Diego De Leo.","Review","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85103667590"
"Roessner V.; Rothe J.; Kohls G.; Schomerus G.; Ehrlich S.; Beste C.","Roessner, Veit (57201040114); Rothe, Josefine (55841548100); Kohls, Gregor (55883640600); Schomerus, Georg (13405272500); Ehrlich, Stefan (23100034000); Beste, Christian (23468528700)","57201040114; 55841548100; 55883640600; 13405272500; 23100034000; 23468528700","Taming the chaos?! Using eXplainable Artificial Intelligence (XAI) to tackle the complexity in mental health research","2021","European Child and Adolescent Psychiatry","30","8","","1143","1146","3","15","10.1007/s00787-021-01836-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109746656&doi=10.1007%2fs00787-021-01836-0&partnerID=40&md5=3bba28e69097967630a42530e26493c4","[No abstract available]","Editorial","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85109746656"
"Hatwell J.; Gaber M.M.; Atif Azad R.M.","Hatwell, Julian (57217070625); Gaber, Mohamed Medhat (8927664800); Atif Azad, R. Muhammad (57219337833)","57217070625; 8927664800; 57219337833","Ada-WHIPS: Explaining AdaBoost classification with applications in the health sciences","2020","BMC Medical Informatics and Decision Making","20","1","250","","","","45","10.1186/s12911-020-01201-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092304414&doi=10.1186%2fs12911-020-01201-2&partnerID=40&md5=1928b65de2dba0a70da8ee87aae23ae9","Background: Computer Aided Diagnostics (CAD) can support medical practitioners to make critical decisions about their patients' disease conditions. Practitioners require access to the chain of reasoning behind CAD to build trust in the CAD advice and to supplement their own expertise. Yet, CAD systems might be based on black box machine learning models and high dimensional data sources such as electronic health records, magnetic resonance imaging scans, cardiotocograms, etc. These foundations make interpretation and explanation of the CAD advice very challenging. This challenge is recognised throughout the machine learning research community. eXplainable Artificial Intelligence (XAI) is emerging as one of the most important research areas of recent years because it addresses the interpretability and trust concerns of critical decision makers, including those in clinical and medical practice. Methods: In this work, we focus on AdaBoost, a black box model that has been widely adopted in the CAD literature. We address the challenge-to explain AdaBoost classification-with a novel algorithm that extracts simple, logical rules from AdaBoost models. Our algorithm, Adaptive-Weighted High Importance Path Snippets (Ada-WHIPS), makes use of AdaBoost's adaptive classifier weights. Using a novel formulation, Ada-WHIPS uniquely redistributes the weights among individual decision nodes of the internal decision trees of the AdaBoost model. Then, a simple heuristic search of the weighted nodes finds a single rule that dominated the model's decision. We compare the explanations generated by our novel approach with the state of the art in an experimental study. We evaluate the derived explanations with simple statistical tests of well-known quality measures, precision and coverage, and a novel measure stability that is better suited to the XAI setting. Results: Experiments on 9 CAD-related data sets showed that Ada-WHIPS explanations consistently generalise better (mean coverage 15%-68%) than the state of the art while remaining competitive for specificity (mean precision 80%-99%). A very small trade-off in specificity is shown to guard against over-fitting which is a known problem in the state of the art methods. Conclusions: The experimental results demonstrate the benefits of using our novel algorithm for explaining CAD AdaBoost classifiers widely found in the literature. Our tightly coupled, AdaBoost-specific approach outperforms model-agnostic explanation methods and should be considered by practitioners looking for an XAI solution for this class of models. © 2020 The Author(s).","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85092304414"
"Yoo H.; Park R.C.; Chung K.","Yoo, Hyun (56072166700); Park, Roy C. (55681405300); Chung, Kyungyong (25927027500)","56072166700; 55681405300; 25927027500","IoT-based health big-data process technologies: A survey","2021","KSII Transactions on Internet and Information Systems","15","3","","974","992","18","37","10.3837/tiis.2021.03.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104250660&doi=10.3837%2ftiis.2021.03.009&partnerID=40&md5=2660f8667117b4f521858c437729063e","Recently, the healthcare field has undergone rapid changes owing to the accumulation of health big data and the development of machine learning. Data mining research in the field of healthcare has different characteristics from those of other data analyses, such as the structural complexity of the medical data, requirement for medical expertise, and security of personal medical information. Various methods have been implemented to address these issues, including the machine learning model and cloud platform. However, the machine learning model presents the problem of opaque result interpretation, and the cloud platform requires more in-depth research on security and efficiency. To address these issues, this paper presents a recent technology for Internet-of-Things-based (IoT-based) health big data processing. We present a cloud-based IoT health platform and health big data processing technology that reduces the medical data management costs and enhances safety. We also present a data mining technology for health-risk prediction, which is the core of healthcare. Finally, we propose a study using explainable artificial intelligence that enhances the reliability and transparency of the decision-making system, which is called the black box model owing to its lack of transparency. © 2021 Korean Society for Internet Information. All rights reserved.","Review","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85104250660"
"Leung C.K.; Jiang F.; Zhang Y.","Leung, Carson K. (7402612526); Jiang, Fan (36104297200); Zhang, Yibin (57211690912)","7402612526; 36104297200; 57211690912","Explainable machine learning and mining of influential patterns from sparse web","2020","Proceedings - 2020 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology, WI-IAT 2020","","","9457808","829","836","7","17","10.1109/WIIAT50758.2020.00128","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105944242&doi=10.1109%2fWIIAT50758.2020.00128&partnerID=40&md5=dd78fa54737bf0ec55849660e5f56930","With advancements in modern technology in the current era, very large volumes of big data have been generated and collected in numerous real-life applications. These have formed a connected world comprising webs of agents, data, people, things and trust. Some of these webs have also emerged in health and smart living. As valuable information and knowledge is embedded in these rich sets of webs, web intelligence is in demand. In this paper, we focus on a data science task of web usage mining. In particular, we present a web intelligent solution to conduct explainable machine learning and mining of influential patterns from sparse web. It provides a compressed representation of sparse web, discovers influential websites and/or web pages that are frequently browsed or surfed by web surfers, and recommends these influential websites and/or web pages to other web surfers. Evaluation results show the effectiveness (especially, in data compression), interpretability and practicality of our solution. © 2020 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85105944242"
"Schoonderwoerd T.A.J.; Jorritsma W.; Neerincx M.A.; van den Bosch K.","Schoonderwoerd, Tjeerd A.J. (57210144415); Jorritsma, Wiard (55914859100); Neerincx, Mark A. (9133405200); van den Bosch, Karel (35103709000)","57210144415; 55914859100; 9133405200; 35103709000","Human-centered XAI: Developing design patterns for explanations of clinical decision support systems","2021","International Journal of Human Computer Studies","154","","102684","","","","85","10.1016/j.ijhcs.2021.102684","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108794824&doi=10.1016%2fj.ijhcs.2021.102684&partnerID=40&md5=7957c1e57e652d7896599445148e1e73","Much of the research on eXplainable Artificial Intelligence (XAI) has centered on providing transparency of machine learning models. More recently, the focus on human-centered approaches to XAI has increased. Yet, there is a lack of practical methods and examples on the integration of human factors into the development processes of AI-generated explanations that humans prove to uptake for better performance. This paper presents a case study of an application of a human-centered design approach for AI-generated explanations. The approach consists of three components: Domain analysis to define the concept & context of explanations, Requirements elicitation & assessment to derive the use cases & explanation requirements, and the consequential Multi-modal interaction design & evaluation to create a library of design patterns for explanations. In a case study, we adopt the DoReMi-approach to design explanations for a Clinical Decision Support System (CDSS) for child health. In the requirements elicitation & assessment, a user study with experienced paediatricians uncovered what explanations the CDSS should provide. In the interaction design & evaluation, a second user study tested the consequential interaction design patterns. This case study provided a first set of user requirements and design patterns for an explainable decision support system in medical diagnosis, showing how to involve expert end users in the development process and how to develop, more or less, generic solutions for general design problems in XAI. © 2021 The Authors","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85108794824"
"Arrotta L.","Arrotta, Luca (57218835832)","57218835832","Multi-inhabitant and explainable Activity Recognition in Smart Homes","2021","Proceedings - IEEE International Conference on Mobile Data Management","2021-June","","9474845","264","266","2","0","10.1109/MDM52706.2021.00054","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112363997&doi=10.1109%2fMDM52706.2021.00054&partnerID=40&md5=5b5bc3f50d20a4a6e4cd2f83f1252be8","The sensor-based detection of Activities of Daily Living (ADLs) in smart home environments can be exploited to provide healthcare applications, like remotely monitoring fragile subjects living in their habitations. However, ADLs recognition methods have been mainly investigated with a focus on singleinhabitant scenarios. The major problem in multi-inhabitant settings is data association: Assigning to each resident the environmental sensors' events that he/she triggered. Furthermore, Deep Learning (DL) solutions have been recently explored for ADLs recognition, with promising results. Nevertheless, the main drawbacks of these methods are their need for large amounts of training data, and their lack of interpretability. This paper summarizes some contributions of my Ph.D. research, in which we are designing explainable multi-inhabitant approaches for ADLs recognition. We have already investigated a hybrid knowledge-and data-driven solution that exploits the high-level context of each resident to perform data association. Currently, we are studying semi-supervised techniques to mitigate the data scarcity issue, and explainable Artificial Intelligence (XAI) methods to make DL classifiers for ADLs more transparent. © 2021 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85112363997"
"Moreno-Sanchez P.A.","Moreno-Sanchez, Pedro A. (57219909354)","57219909354","Development of an Explainable Prediction Model of Heart Failure Survival by Using Ensemble Trees","2020","Proceedings - 2020 IEEE International Conference on Big Data, Big Data 2020","","","9378460","4902","4910","8","29","10.1109/BigData50022.2020.9378460","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103815191&doi=10.1109%2fBigData50022.2020.9378460&partnerID=40&md5=82dbef59d9fea48bda3bf4ca82a0b752","Cardiovascular diseases (CVD) are the leading cause of death globally. Heart failure prediction, one of the CVD manifestations, has become a priority for doctors, however, up to date clinical practice usually has failed to reach high accuracy in such tasks. Machine learning offers advantages not only for clinical prediction but also for feature ranking improving the interpretation of the outputs by clinical professionals. Thus, the concept of eXplainable Artificial Intelligence (XAI) is aimed to cope with the lack of explainability of machine learning models in the healthcare domain, in this case, and provide healthcare professionals with patient-tailored decision-making tools that improve treatments and diagnostics. This paper presents a heart failure survival prediction model development by using ensemble trees machine learning techniques. Extreme Gradient Boosting (XGBoost) is demonstrated as the classifier with most accurate results (83% accuracy with unseen data) over the other ensemble trees options. Moreover, a features selection preprocessing is made in order to assess which relevant features contribute to the model's results. Next, in terms of improving the explainability of the model developed, a study of features importance is carried out showing the ""follow up time period""feature as the most relevant. Finally, a quantitative evaluation of the interpretability and fidelity of the model developed is performed obtaining a balanced ratio between these two indicators. © 2020 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85103815191"
"Biswas M.; Kaiser M.S.; Mahmud M.; Al Mamun S.; Hossain M.S.; Rahman M.A.","Biswas, Milon (57188669162); Kaiser, M. Shamim (56446362000); Mahmud, Mufti (35173453700); Al Mamun, Shamim (58276770300); Hossain, Md. Shahadat (55871122455); Rahman, Muhammad Arifur (55889568700)","57188669162; 56446362000; 35173453700; 58276770300; 55871122455; 55889568700","An XAI Based Autism Detection: The Context Behind the Detection","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12960 LNAI","","","448","459","11","29","10.1007/978-3-030-86993-9_40","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115827443&doi=10.1007%2f978-3-030-86993-9_40&partnerID=40&md5=f4949577baa05180ebd84bc6d3dc0e9e","With the rapid growth of the Internet of Healthcare Things, a massive amount of data is generated by a broad variety of medical devices. Because of the complex relationship in large-scale healthcare data, researchers who bring a revolution in the healthcare industry embrace Artificial Intelligence (AI). In certain cases, it has been reported that AI can do better than humans at performing healthcare tasks. The data-driven black-box model, on the other hand, does not appeal to healthcare professionals as it is not transparent, and any biasing can hamper the performance the prediction model for the real-life operation. In this paper, we proposed an AI model for early detection of autism in children. Then we showed why AI with explainability is important. This paper provides examples focused on the Autism Spectrum Disorder dataset (Autism screening data for toddlers by Dr Fadi Fayez Thabtah) and discussed why explainability approaches should be used when using AI systems in healthcare. © 2021, Springer Nature Switzerland AG.","Conference paper","Final","","Scopus","2-s2.0-85115827443"
"Al-Taie Z.; Liu D.; Mitchem J.B.; Papageorgiou C.; Kaifi J.T.; Warren W.C.; Shyu C.-R.","Al-Taie, Zainab (57193262661); Liu, Danlu (57201723392); Mitchem, Jonathan B (7003579053); Papageorgiou, Christos (23006135400); Kaifi, Jussuf T. (6506590387); Warren, Wesley C. (7202784659); Shyu, Chi-Ren (7005681170)","57193262661; 57201723392; 7003579053; 23006135400; 6506590387; 7202784659; 7005681170","Explainable artificial intelligence in high-throughput drug repositioning for subgroup stratifications with interventionable potential","2021","Journal of Biomedical Informatics","118","","103792","","","","10","10.1016/j.jbi.2021.103792","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107949586&doi=10.1016%2fj.jbi.2021.103792&partnerID=40&md5=4363de29a8df9b7e0b3a2509ce200890","Enabling precision medicine requires developing robust patient stratification methods as well as drugs tailored to homogeneous subgroups of patients from a heterogeneous population. Developing de novo drugs is expensive and time consuming with an ultimately low FDA approval rate. These limitations make developing new drugs for a small portion of a disease population unfeasible. Therefore, drug repositioning is an essential alternative for developing new drugs for a disease subpopulation. This shows the importance of developing data-driven approaches that find druggable homogeneous subgroups within the disease population and reposition the drugs for these subgroups. In this study, we developed an explainable AI approach for patient stratification and drug repositioning. Contrast pattern mining and network analysis were used to discover homogeneous subgroups within a disease population. For each subgroup, a biomedical network analysis was done to find the drugs that are most relevant to a given subgroup of patients. The set of candidate drugs for each subgroup was ranked using an aggregated drug score assigned to each drug. The proposed method represents a human-in-the-loop framework, where medical experts use the data-driven results to generate hypotheses and obtain insights into potential therapeutic candidates for patients who belong to a subgroup. Colorectal cancer (CRC) was used as a case study. Patients' phenotypic and genotypic data was utilized with a heterogeneous knowledge base because it gives a multi-view perspective for finding new indications for drugs outside of their original use. Our analysis of the top candidate drugs for the subgroups identified by medical experts showed that most of these drugs are cancer-related, and most of them have the potential to be a CRC regimen based on studies in the literature. © 2021 The Author(s)","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85107949586"
"Larasati R.; Liddo A.D.; Motta E.","Larasati, Retno (57197729397); Liddo, Anna De (13008594300); Motta, Enrico (7006092143)","57197729397; 13008594300; 7006092143","AI Healthcare System Interface: Explanation Design for Non-Expert User Trust","2021","CEUR Workshop Proceedings","2903","","","","","","4","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110537156&partnerID=40&md5=7503134c25fb33e3b4c04db92fc19c8a","Research indicates that non-expert users tend to either over-trust or distrust AI systems. This raises concerns when AI is applied to healthcare, where a patient trusting the advice of an unreliable system, or completely distrusting a reliable one, can lead to fatal incidents or missed healthcare opportunities. Previous research indicated that explanations can help users to make appropriate judgements on AI Systems' trust, but how to design AI explanation interfaces for non-expert users in a medical support scenarios is still an open research challenge. This paper explores a stage-based participatory design process to develop a trustworthy explanation interface for non-experts in an AI medical support scenario. A trustworthy explanation is an explanation that helps users to make considered judgments on trusting (or not) and AI system for their healthcare. The objective of this paper was to identify the explanation components that can effectively inform the design of a trustworthy explanation interface. To achieve that, we undertook three data collections, examining experts' and non-experts' perceptions of AI medical support system's explanations. We then developed a User Mental Model, an Expert Mental Model, and a Target Mental Model of explanation, describing how non-expert and experts understand explanations, how their understandings differ, and how it can be combined. Based on the Target Mental Model, we then propose a set of 14 explanation design guidelines for trustworthy AI Healthcare System explanation, that take into account non-expert users needs, medical experts practice, and AI experts understanding. © 2020  Copyright © 2021 for this paper by its authors.","Conference paper","Final","","Scopus","2-s2.0-85110537156"
"Fellous J.-M.; Sapiro G.; Rossi A.; Mayberg H.; Ferrante M.","Fellous, Jean-Marc (7003806123); Sapiro, Guillermo (7005450011); Rossi, Andrew (7403474875); Mayberg, Helen (7006007091); Ferrante, Michele (57206662476)","7003806123; 7005450011; 7403474875; 7006007091; 57206662476","Explainable Artificial Intelligence for Neuroscience: Behavioral Neurostimulation","2019","Frontiers in Neuroscience","13","","1346","","","","80","10.3389/fnins.2019.01346","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077335140&doi=10.3389%2ffnins.2019.01346&partnerID=40&md5=e2389a13dd1c6f249124ddfaad761e07","The use of Artificial Intelligence and machine learning in basic research and clinical neuroscience is increasing. AI methods enable the interpretation of large multimodal datasets that can provide unbiased insights into the fundamental principles of brain function, potentially paving the way for earlier and more accurate detection of brain disorders and better informed intervention protocols. Despite AI’s ability to create accurate predictions and classifications, in most cases it lacks the ability to provide a mechanistic understanding of how inputs and outputs relate to each other. Explainable Artificial Intelligence (XAI) is a new set of techniques that attempts to provide such an understanding, here we report on some of these practical approaches. We discuss the potential value of XAI to the field of neurostimulation for both basic scientific inquiry and therapeutic purposes, as well as, outstanding questions and obstacles to the success of the XAI approach. © Copyright © 2019 Fellous, Sapiro, Rossi, Mayberg and Ferrante.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85077335140"
"Galli A.; Moscato V.; Sperlí G.; Santo A.D.","Galli, Antonio (57210585022); Moscato, Vincenzo (8300789300); Sperlí, Giancarlo (55510822500); Santo, Aniello De (56435909200)","57210585022; 8300789300; 55510822500; 56435909200","An explainable artificial intelligence methodology for hard disk fault prediction","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12391 LNCS","","","403","413","10","5","10.1007/978-3-030-59003-1_26","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091522529&doi=10.1007%2f978-3-030-59003-1_26&partnerID=40&md5=03a3d32caf862ea179e21b8c41530191","Failure rates of Hard Disk Drives (HDDs) are high and often due to a variety of different conditions. Thus, there is increasing demand for technologies dedicated to anticipating possible causes of failure, so to allow for preventive maintenance operations. In this paper, we propose a framework to predict HDD health status according to a long short-term memory (LSTM) model. We also employ eXplainable Artificial Intelligence (XAI) tools, to provide effective explanations of the model decisions, thus making the final results more useful to human decision-making processes. We extensively evaluate our approach on standard data-sets, proving its feasibility for real world applications. © Springer Nature Switzerland AG 2020.","Conference paper","Final","","Scopus","2-s2.0-85091522529"
"Wang W.; van Lint C.L.; Brinkman W.-P.; Rövekamp T.J.M.; van Dijk S.; van der Boog P.; Neerincx M.A.","Wang, Wenxin (55632555300); van Lint, Céline L. (56829139700); Brinkman, Willem-Paul (8707580500); Rövekamp, Ton J. M. (24825712000); van Dijk, Sandra (7004624443); van der Boog, Paul (6602543972); Neerincx, Mark A. (9133405200)","55632555300; 56829139700; 8707580500; 24825712000; 7004624443; 6602543972; 9133405200","Guided or factual computer support for kidney patients with different experience levels and medical health situations: preferences and usage","2019","Health and Technology","9","3","","329","342","13","4","10.1007/s12553-019-00295-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065720963&doi=10.1007%2fs12553-019-00295-7&partnerID=40&md5=f2e3e41c16e542488143a80b1eb989e3","Personalization of eHealth systems is a promising technique for improving patients’ adherence. This paper explores the possibility of personalisation based on the patients’ medical health situation and on their health literacy. The study is set within the context of a self-management support system (SMSS) for renal transplant patients. A SMSS is designed with layering, nudging, emphaticizing, and focusing principles. It has two communication styles: (1) a guided style that provided more interpretation support and addressed emotional needs; and (2) a factual style that showed only measurement history, medical information, and recommendations. To evaluate the design, 49 renal transplant patients with three different experience levels participated in a lab study, in which they used the system in imaginary scenarios to deal with three medical health situations (alright, mild concern, and concern). A 96% understanding and 87% adherence rate was observed, with a significant interaction effect on adherence between patient group and health situation. Furthermore, compared to recently transplanted patients, not recently transplanted patients were relatively more positive towards the factual than the guided communication style in the “alright” condition. Furthermore, additional medical information was searched more often in health situations that causes mild concern and a majority of patients did not change the communication style to their preferred styles. By attuning the communication style to patient’s experience and medical health situation according to the applied principles and acquired insights, SMSSs are expected to be better used. © 2019, IUPESM and Springer-Verlag GmbH Germany, part of Springer Nature.","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85065720963"
"Kızrak M.A.; Müftüoğlu Z.; Yıldırım T.","Kızrak, Merve Ayyuce (55842167800); Müftüoğlu, Zümrüt (57211657942); Yıldırım, Tülay (7005994652)","55842167800; 57211657942; 7005994652","Limitations and challenges on the diagnosis of COVID-19 using radiology images and deep learning","2021","Data Science for COVID-19 Volume 1: Computational Perspectives","","","","91","115","24","5","10.1016/B978-0-12-824536-1.00007-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127638449&doi=10.1016%2fB978-0-12-824536-1.00007-1&partnerID=40&md5=74a7e36d1edbf1382c27ace3332abfec","The world is facing a great threat nowadays. The COVID-19 virus outbreak that occurred in Wuhan in China in December 2019 continues to increase in the middle of 2020. Within the scope of this epidemic, different contents of data are published and products for improving the treatment process. One of the major symptoms of COVID-19 epidemic disease, which was revealed by the World Health Organization, is intense cough and breathing difficulties. Chest X-ray (CXR) and computing tomography (CT) images of patients infected with COVID-19 are also a type of data that allows data scientists to work with healthcare professionals during this struggle. Fast evaluation of these images by experts is important in the days when the epidemic has suffered. This chapter focuses on artificial intelligence (AI) for a successful and rapid diagnostic recommendation as part of these deadly epidemic prevention efforts that have emerged. As a study case, a dataset of 373 CXR images, 139 of which were COVID-19 infected, collected from open sources, was used for diagnosis with deep learning approaches of COVID-19. The use of EfficientNet, an up-to-date and robust deep learning model for education, offers the possibility to become infected with an accuracy of 94.7%. Nevertheless, some limitations must be considered when producing AI solutions by making use of medical data. Using these results, a perspective is provided on the limitations of deep learning models in the diagnosis of COVID-19 from radiology images for data quality, amount of data, data privacy, explainability, and robust solutions. © 2021 Elsevier Inc. All rights reserved.","Book chapter","Final","","Scopus","2-s2.0-85127638449"
"Hong C.W.; Lee C.; Lee K.; Ko M.-S.; Kim D.E.; Hur K.","Hong, Chang Woo (57216618761); Lee, Changmin (57216649036); Lee, Kwangsuk (57205595406); Ko, Min-Seung (57211030307); Kim, Dae Eun (58105882900); Hur, Kyeon (8880164600)","57216618761; 57216649036; 57205595406; 57211030307; 58105882900; 8880164600","Remaining useful life prognosis for turbofan engine using explainable deep neural networks with dimensionality reduction","2020","Sensors (Switzerland)","20","22","6626","1","19","18","28","10.3390/s20226626","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096625031&doi=10.3390%2fs20226626&partnerID=40&md5=6eb2d5f2093cab63d341c10ffac4799b","This study prognoses the remaining useful life of a turbofan engine using a deep learning model, which is essential for the health management of an engine. The proposed deep learning model affords a significantly improved accuracy by organizing networks with a one-dimensional convolutional neural network, long short-term memory, and bidirectional long short-term memory. In particular, this paper investigates two practical and crucial issues in applying the deep learning model for system prognosis. The first is the requirement of numerous sensors for different components, i.e., the curse of dimensionality. Second, the deep neural network cannot identify the problematic component of the turbofan engine due to its “black box” property. This study thus employs dimensionality reduction and Shapley additive explanation (SHAP) techniques. Dimensionality reduction in the model reduces the complexity and prevents overfitting, while maintaining high accuracy. SHAP analyzes and visualizes the black box to identify the sensors. The experimental results demonstrate the high accuracy and efficiency of the proposed model with dimensionality reduction and show that SHAP enhances the explainability in a conventional deep learning model for system prognosis. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85096625031"
"Byeon H.","Byeon, Haewon (36452769600)","36452769600","Exploring factors for predicting anxiety disorders of the elderly living alone in south korea using interpretable machine learning: A population-based study","2021","International Journal of Environmental Research and Public Health","18","14","7625","","","","21","10.3390/ijerph18147625","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110210285&doi=10.3390%2fijerph18147625&partnerID=40&md5=acfe5c207bdc1680823851b7649411da","This epidemiological study aimed to develop an X-AI that could explain groups with a high anxiety disorder risk in old age. To achieve this objective, (1) this study explored the predictors of senile anxiety using base models and meta models. (2) This study presented decision tree visualization that could help psychiatric consultants and primary physicians easily interpret the path of predicting high-risk groups based on major predictors derived from final machine learning models with the best performance. This study analyzed 1558 elderly (695 males and 863 females) who were 60 years or older and completed the Zung’s Self-Rating Anxiety Scale (SAS). We used support vector machine (SVM), random forest, LightGBM, and Adaboost for the base model, a single predictive model, while using XGBoost algorithm for the meta model. The analysis results confirmed that the predictive performance of the “SVM + Random forest + LightGBM + AdaBoost + XGBoost model (stacking ensemble: accuracy 87.4%, precision 85.1%, recall 87.4%, and F1-score 85.5%)” was the best. Also, the results of this study showed that the elderly who often (or mostly) felt subjective loneliness, had a Self Esteem Scale score of 26 or less, and had a subjective communication with their family of 4 or less (on a 10-point scale) were the group with the highest risk anxiety disorder. The results of this study imply that it is necessary to establish a community-based mental health policy that can identify elderly groups with high anxiety risks based on multiple risk factors and manage them constantly. © 2021 by the author. Licensee MDPI, Basel, Switzerland.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85110210285"
"Jha I.P.; Awasthi R.; Kumar A.; Kumar V.; Sethi T.","Jha, Indra Prakash (57218685737); Awasthi, Raghav (57221250275); Kumar, Ajit (57212559779); Kumar, Vibhor (57212900996); Sethi, Tavpritesh (36991690700)","57218685737; 57221250275; 57212559779; 57212900996; 36991690700","Learning the mental health impact of COVID-19 in the United States with explainable artificial intelligence: Observational study","2021","JMIR Mental Health","8","4","e25097","","","","29","10.2196/25097","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104823640&doi=10.2196%2f25097&partnerID=40&md5=6f663d9d8b39eca73ddcf24e1a69964f","Background: The COVID-19 pandemic has affected the health, economic, and social fabric of many nations worldwide. Identification of individual-level susceptibility factors may help people in identifying and managing their emotional, psychological, and social well-being. Objective: This study is focused on learning a ranked list of factors that could indicate a predisposition to a mental disorder during the COVID-19 pandemic. Methods: In this study, we have used a survey of 17,764 adults in the United States from different age groups, genders, and socioeconomic statuses. Through initial statistical analysis and Bayesian network inference, we have identified key factors affecting mental health during the COVID-19 pandemic. Integrating Bayesian networks with classical machine learning approaches led to effective modeling of the level of mental health prevalence. Results: Overall, females were more stressed than males, and people in the age group 18-29 years were more vulnerable to anxiety than other age groups. Using the Bayesian network model, we found that people with a chronic mental illness were more prone to mental disorders during the COVID-19 pandemic. The new realities of working from home; homeschooling; and lack of communication with family, friends, and neighbors induces mental pressure. Financial assistance from social security helps in reducing mental stress during the COVID-19–generated economic crises. Finally, using supervised machine learning models, we predicted the most mentally vulnerable people with ~80% accuracy. Conclusions: Multiple factors such as social isolation, digital communication, and working and schooling from home were identified as factors of mental illness during the COVID-19 pandemic. Regular in-person communication with friends and family, a healthy social life, and social security were key factors, and taking care of people with a history of mental disease appears to be even more important during this time. © Indra Prakash Jha, Raghav Awasthi, Ajit Kumar, Vibhor Kumar, Tavpritesh Sethi.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85104823640"
"Angulo C.; Gonzalez-Abril L.; Raya C.; Ortega J.A.","Angulo, Cecilio (23090235000); Gonzalez-Abril, Luis (23667004700); Raya, Cristóbal (19640653500); Ortega, Juan Antonio (33867735500)","23090235000; 23667004700; 19640653500; 33867735500","A Proposal to Evolving Towards Digital Twins in Healthcare","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12108 LNBI","","","418","426","8","29","10.1007/978-3-030-45385-5_37","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085198631&doi=10.1007%2f978-3-030-45385-5_37&partnerID=40&md5=a9c00cc3ebc7cfc96810e7aaf7a73da6","The main objective in this proposal is to orchestrate an ecosystem of manipulation of reliable and safe data, applied to the field of health, specifically lung cancer, by introducing the creation of digital twins for personalised healthcare about the behaviour of this disease on patients. Digital twins is a very popular and novel approach in digitisation units in industry which will be used by both kind of experts: (i) data analysts, who will design expert recommender systems and extract knowledge – explainable Artificial Intelligence (AI); and (ii) professionals in medicine, who will consume that knowledge generated with their research for better diagnosis. This knowledge generation/extraction process will work in the form of a lifelong learning system by iterative and continuous use. The produced software platform will be abstracted so it can be applied like a general purpose service tool in other domains of knowledge, specially health and industry. Furthermore, a rule extraction module will be made available for explainability issues. © Springer Nature Switzerland AG 2020.","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85085198631"
"Davagdorj K.; Bae J.-W.; Pham V.-H.; Theera-Umpon N.; Ryu K.H.","Davagdorj, Khishigsuren (57205729276); Bae, Jang-Whan (8699381000); Pham, Van-Huy (57191658592); Theera-Umpon, Nipon (57196217102); Ryu, Keun Ho (7202685903)","57205729276; 8699381000; 57191658592; 57196217102; 7202685903","Explainable Artificial Intelligence Based Framework for Non-Communicable Diseases Prediction","2021","IEEE Access","9","","","123672","123688","16","21","10.1109/ACCESS.2021.3110336","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114749089&doi=10.1109%2fACCESS.2021.3110336&partnerID=40&md5=a75c370fc1640ee8f901151339b495af","The rapid rise of non-communicable diseases (NCDs) becomes one of the serious health issues and the leading cause of death worldwide. In recent years, artificial intelligence-based systems have been developed to assist clinicians in decision-making to reduce morbidity and mortality. However, a common drawback of these modern studies is related to explanations of their output. In other words, understanding the inner logic behind the predictions is hidden to the end-user. Thus, clinicians struggle to interpret these models because of their black-box nature, and hence they are not acceptable in the medical practice. To address this problem, we have proposed a Deep Shapley Additive Explanations (DeepSHAP) based deep neural network framework equipped with a feature selection technique for NCDs prediction and explanation among the population in the United States. Our proposed framework comprises three components: First, representative features are done based on the elastic net-based embedded feature selection technique; second a deep neural network classifier is tuned with the hyper-parameters and used to train the model with the selected feature subset; third, two kinds of model explanation are provided by the DeepSHAP approach. Herein, (I) explaining the risk factors that affected the model's prediction from the population-based perspective; (II) aiming to explain a single instance from the human-centered perspective. The experimental results indicated that the proposed model outperforms various state-of-the-art models. In addition, the proposed model can improve the medical understanding of NCDs diagnosis by providing general insights into the changes in disease risk at the global and local levels. Consequently, DeepSHAP based explainable deep learning framework contributes not only to the medical decision support systems but also can provide to real-world needs in other domains.  © 2013 IEEE.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85114749089"
"Kaptein F.; Broekens J.; Hindriks K.; Neerincx M.","Kaptein, Frank (6504499204); Broekens, Joost (22940425000); Hindriks, Koen (55904124900); Neerincx, Mark (9133405200)","6504499204; 22940425000; 55904124900; 9133405200","Evaluating Cognitive and Affective Intelligent Agent Explanations in a Long-Term Health-Support Application for Children with Type 1 Diabetes","2019","2019 8th International Conference on Affective Computing and Intelligent Interaction, ACII 2019","","","8925526","304","310","6","5","10.1109/ACII.2019.8925526","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077798730&doi=10.1109%2fACII.2019.8925526&partnerID=40&md5=f2e43246d21bfd51aa733c68171e56d6","Explanation of actions is important for transparency of-, and trust in the decisions of smart systems. Literature suggests that emotions and emotion words-in addition to beliefs and goals-are used in human explanations of behaviour. Furthermore, research in e-health support systems and human-robot interaction stresses the need for studying long-term interaction with users. However, state of the art explainable artificial intelligence for intelligent agents focuses mainly on explaining an agent's behaviour based on the underlying beliefs and goals in short-term experiments. In this paper, we report on a long-term experiment in which we tested the effect of cognitive, affective and lack of explanations on children's motivation to use an e-health support system. Children (aged 6-14) suffering from type 1 diabetes mellitus interacted with a virtual robot as part of the e-health system over a period of 2.5-3 months. Children alternated between the three conditions. Agent behaviours that were explained to the children included why 1) the agent asks a certain quiz question; 2) the agent provides a specific tip (a short instruction) about diabetes; or, 3) the agent provides a task suggestion, e.g., play a quiz, or, watch a video about diabetes. Their motivation was measured by counting how often children would follow the agent's suggestion, how often they would continue to play the quiz or ask for an additional tip, and how often they would request an explanation from the system. Surprisingly, children proved to follow task suggestions more often when no explanation was given, while other explanation effects did not appear. This is to our knowledge the first longterm study to report empirical evidence for an agent explanation effect, challenging the next studies to uncover the underlying mechanism. © 2019 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85077798730"
"Chary M.; Boyer E.W.; Burns M.M.","Chary, Michael (55657142800); Boyer, Ed W. (35589263600); Burns, Michele M. (7201870589)","55657142800; 35589263600; 7201870589","Diagnosis of Acute Poisoning using explainable artificial intelligence","2021","Computers in Biology and Medicine","134","","104469","","","","8","10.1016/j.compbiomed.2021.104469","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107300043&doi=10.1016%2fj.compbiomed.2021.104469&partnerID=40&md5=5cb19376d1981f68c9797e13251c033d","Introduction: Medical toxicology is the clinical specialty that treats the toxic effects of substances, for example, an overdose, a medication error, or a scorpion sting. The volume of toxicological knowledge and research has, as with other medical specialties, outstripped the ability of the individual clinician to entirely master and stay current with it. The application of machine learning/artificial intelligence (ML/AI) techniques to medical toxicology is challenging because initial treatment decisions are often based on a few pieces of textual data and rely heavily on experience and prior knowledge. ML/AI techniques, moreover, often do not represent knowledge in a way that is transparent for the physician, raising barriers to usability. Logic-based systems are more transparent approaches, but often generalize poorly and require expert curation to implement and maintain. Methods: We constructed a probabilistic logic network to model how a toxicologist recognizes a toxidrome, using only physical exam findings. Our approach transparently mimics the knowledge representation and decision-making of practicing clinicians. We created a library of 300 synthetic cases of varying clinical complexity. Each case contained 5 physical exam findings drawn from a mixture of 1 or 2 toxidromes. We used this library to evaluate the performance of our probabilistic logic network, dubbed Tak, against 2 medical toxicologists, a decision tree model, as well as its ability to recover the actual diagnosis. Results: The inter-rater reliability between Tak and the consensus of human raters was κ = 0.8432 for straightforward cases, 0.4396 for moderately complex cases, and 0.3331 for challenging cases. The inter-rater reliability between the decision tree classifier and the consensus of human raters was, κ = 0.2522 for straightforward cases, 0.1963 for moderately complex cases and 0.0331 for challenging cases. Conclusions: The software, dubbed Tak, performs comparably to humans on straightforward cases and intermediate difficulty cases, but is outperformed by humans on challenging clinical cases. Tak outperforms a decision tree classifier at all levels of difficulty. Our results are a proof-of-concept that, in a restricted domain, probabilistic logic networks can perform medical reasoning comparably to humans. © 2021 Elsevier Ltd","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85107300043"
"Shad R.; Cunningham J.P.; Ashley E.A.; Langlotz C.P.; Hiesinger W.","Shad, Rohan (57195425028); Cunningham, John P. (35387912300); Ashley, Euan A. (7004338939); Langlotz, Curtis P. (20134955200); Hiesinger, William (16316302300)","57195425028; 35387912300; 7004338939; 20134955200; 16316302300","Designing clinically translatable artificial intelligence systems for high-dimensional medical imaging","2021","Nature Machine Intelligence","3","11","","929","935","6","22","10.1038/s42256-021-00399-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119094881&doi=10.1038%2fs42256-021-00399-8&partnerID=40&md5=4d798195821f08c65565e38210d4b51a","The National Institutes of Health in 2018 identified key focus areas for the future of artificial intelligence in medical imaging, creating a foundational roadmap for research in image acquisition, algorithms, data standardization and translatable clinical decision support systems. Among the key issues raised in the report, data availability, the need for novel computing architectures and explainable artificial intelligence algorithms are still relevant, despite the tremendous progress made over the past few years alone. Furthermore, translational goals of data sharing, validation of performance for regulatory approval, generalizability and mitigation of unintended bias must be accounted for early in the development process. In this Perspective, we explore challenges unique to high-dimensional clinical imaging data, in addition to highlighting some of the technical and ethical considerations involved in developing machine learning systems that better represent the high-dimensional nature of many imaging modalities. Furthermore, we argue that methods that attempt to address explainability, uncertainty and bias should be treated as core components of any clinical machine learning system. © 2021, Springer Nature Limited.","Review","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85119094881"
"Panigutti C.; Perotti A.; Pedreschi D.","Panigutti, Cecilia (57194345147); Perotti, Alan (58321306400); Pedreschi, Dino (6603935985)","57194345147; 58321306400; 6603935985","Doctor XAI An ontology-based approach to black-box sequential data classification explanations","2020","FAT* 2020 - Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency","","","","629","639","10","116","10.1145/3351095.3372855","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079663429&doi=10.1145%2f3351095.3372855&partnerID=40&md5=ae7732c13137b67be7c09e9541bd22b0","Several recent advancements in Machine Learning involve black-box models: algorithms that do not provide human-understandable explanations in support of their decisions. This limitation hampers the fairness, accountability and transparency of these models; the field of eXplainable Artificial Intelligence (XAI) tries to solve this problem providing human-understandable explanations for black-box models. However, healthcare datasets (and the related learning tasks) often present peculiar features, such as sequential data, multi-label predictions, and links to structured background knowledge. In this paper, we introduce Doctor XAI, a model-agnostic explainability technique able to deal with multi-labeled, sequential, ontology-linked data. We focus on explaining Doctor AI, a multi-label classifier which takes as input the clinical history of a patient in order to predict the next visit. Furthermore, we show how exploiting the temporal dimension in the data and the domain knowledge encoded in the medical ontology improves the quality of the mined explanations. © 2020 Copyright held by the owner/author(s).","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85079663429"
"Kanonirov A.; Balabaeva K.; Kovalchuk S.","Kanonirov, Alexander (57363059700); Balabaeva, Ksenia (57209270709); Kovalchuk, Sergey (55382199400)","57363059700; 57209270709; 55382199400","Statistical inference for clustering results interpretation in clinical practice","2021","Studies in Health Technology and Informatics","285","","","100","105","5","1","10.3233/SHTI210580","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120615816&doi=10.3233%2fSHTI210580&partnerID=40&md5=903269bd9c2d2cfcc9a00a06752893ef","The relevance of this study lies in improvement of machine learning models understanding. We present a method for interpreting clustering results and apply it to the case of clinical pathways modeling. This method is based on statistical inference and allows to get the description of the clusters, determining the influence of a particular feature on the difference between them. Based on the proposed approach, it is possible to determine the characteristic features for each cluster. Finally, we compare the method with the Bayesian inference explanation and with the interpretation of medical experts [1]. © 2021 The authors and IOS Press.","Conference paper","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85120615816"
"Santos D.A.; Baranauskas J.A.; Tinos R.","Santos, Daniel A. (57705584900); Baranauskas, Jose A. (7801468076); Tinos, Renato (6602473913)","57705584900; 7801468076; 6602473913","Use of Fitness Sharing in the Local Rule-Based Explanations Method","2021","2021 IEEE Latin American Conference on Computational Intelligence, LA-CCI 2021","","","","","","","0","10.1109/LA-CCI48322.2021.9769789","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130614548&doi=10.1109%2fLA-CCI48322.2021.9769789&partnerID=40&md5=1aa67c1ec69d905281ecf0dcf6dd44eb","Recent machine learning algorithms present remarkable results in many problems. However, the decisions made by some of these algorithms are very often difficult for human experts to interpret. Some recent works in the literature try to minimize this disadvantage, proposing algorithms that explain the decisions taken by any black-box model. One of these models is the Local Rule Based Explanations (LORE), that generates local explanations by using a Decision Tree (DT) that locally reproduces the decision boundaries of the black-box model. In LORE, the DT is trained by using an artificial dataset generated by a standard Genetic Algorithm (GA). In this paper, we show that the GA employed in LORE does not necessarily preserve the diversity of solutions in the final population. The diversity of the population is important to generate DTs that can more accurately reproduce the decision boundaries of the black-box model close to the instance to be explained. We then propose the use of fitness sharing in the GA in order to preserve the diversity of the population and generate local decision boundaries of the DT more similar to those of the black-box model. Experimental results show that LORE with fitness sharing generally produces better local explanations.  © 2021 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85130614548"
"Moreno-Sanchez P.A.","Moreno-Sanchez, Pedro A. (57219909354)","57219909354","An automated feature selection and classification pipeline to improve explainability of clinical prediction models","2021","Proceedings - 2021 IEEE 9th International Conference on Healthcare Informatics, ISCHI 2021","","","","527","534","7","9","10.1109/ICHI52183.2021.00100","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118125788&doi=10.1109%2fICHI52183.2021.00100&partnerID=40&md5=a1eeea6d305a59bf0649afa293f6c922","Artificial Intelligence is becoming recently a promising tool to achieve the deployment of personalized medicine in clinical practice. However, healthcare professionals are demanding clinical prediction models with better interpretability of the results in order to achieve an actual adoption and use of these solutions. The eXplainable Artificial Intelligence tackle this issue by offering feature relevance explanations of the model, among other techniques, where the selection of the important features and elimination of the redundant are cornerstones. This work presents a data management pipeline that allows automating the selection of those relevant features as well as the classifier technique that provides the best performance in terms of classification. The pipeline developed, named SCI-XAI (feature Selection and Classification for Improving eXplainable Artificial Intelligence) has been evaluated with 6 clinical datasets in a cross-validation approach as well as in a test set with unseen data. Next, an explainability evaluation has been carried out of the best models obtained by applying the SCI-XAI pipeline. Results obtained show that SCI-XAI achieves the best classification performance by applying different feature selection techniques depending on the variable type of the feature which reduces significantly the features processed by the model. Thus, feature reduction allows increasing the explainability of the models. © 2021 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85118125788"
"Alaff A.J.I.; Mukhairez H.H.A.; Kose U.","Alaff, Ahmed J. I. (57223083069); Mukhairez, Hosam H. A. (57223093230); Kose, Utku (36544118500)","57223083069; 57223093230; 36544118500","An Explainable Artificial Intelligence Model for Detecting COVID-19 with Twitter Text Classification: Turkey Case","2021","Lecture Notes in Networks and Systems","170 LNNS","","","87","97","10","1","10.1007/978-981-33-4084-8_9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104763429&doi=10.1007%2f978-981-33-4084-8_9&partnerID=40&md5=98824461b02f2511dd91a1680dc2cb2a","In December 2019, the first case of the coronavirus was reported, specifically in Wuhan, in China, and the virus began to spread very quickly until it reached more than 3 million cases around the world. But with the lack of technology and medical equipment and the existence of low health awareness in many countries, there is an intense research to combat with that massive problem. In this context, objective of this paper is to discover the spread of the virus according to the countries, by following what users around the world publish on social networking sites, especially on Twitter. In detail, we proposed an explainable artificial intelligence (XAI)-based text classification model that depends on three main steps to discover approximate numbers of infects, by checking the symptoms that published within Twitter posts. A dataset publishing cases of infected and the accompanying symptoms of more than 112 k cases was considered and a model with Naïve Bayes was trained through a comparative work with eight different classifiers. Naïve Bayes is a transparent machine learning technique so it is easier to use probabilistic relations between inputs and the outputs to explain results. Eventually, that XAI-based Naïve Bayes model reached to the highest accuracy rate with 93.6%. © 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Conference paper","Final","","Scopus","2-s2.0-85104763429"
"Cavaliere F.; Cioppa A.D.; Marcelli A.; Parziale A.; Senatore R.","Cavaliere, F. (57219601570); Cioppa, A. Della (57215948865); Marcelli, A. (7005267205); Parziale, A. (36983322000); Senatore, R. (36983155800)","57219601570; 57215948865; 7005267205; 36983322000; 36983155800","Parkinson's Disease Diagnosis: Towards Grammar-based Explainable Artificial Intelligence","2020","Proceedings - IEEE Symposium on Computers and Communications","2020-July","","9219616","","","","18","10.1109/ISCC50000.2020.9219616","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094146780&doi=10.1109%2fISCC50000.2020.9219616&partnerID=40&md5=b85d1f90b4663e17ae3b312618c4b402","Machine Learning (ML) approaches are vastly used for supporting humans in decision-making processes. However, the poor explainability associated to their behavior hampers their application in fields were the impact of the decision is critical, as it is the case for medical application, since physicians cannot simply use the predictions of the model but they must trust the results it provides. This work focuses on the automatic detection of Parkinson's disease (PD), whose impact on both the individual's quality of life and social well-being is constantly increasing with the aging of the population. To this end, we propose an explainable approach based on Genetic Programming, called Grammar Evolution (GE). This technique uses context-free grammar to describe the language of the programs to be generated and evolved. In this case, the generated programs are the explicit classification rules for the diagnosis of the subjects. The results of the experiments obtained on the publicly available HandPD data set show GE's high expressive power and performance comparable to those of several ML models that have been proposed in the literature. © 2020 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85094146780"
"","","","AVI 2020 Workshop on Road Mapping Infrastructures for Artificial Intelligence Supporting Advanced Visual Big Data Analysis, AVI-BDA 2020 and 2nd Italian Workshop on Visualization and Visual Analytics, ITAVIS 2020","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12585 LNCS","","","","","200","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102616006&partnerID=40&md5=178923ff0d1eb68306e7fc87e20d1e63","The proceedings contain 14 papers. The special focus in this conference is on Road Mapping Infrastructures for Artificial Intelligence Supporting Advanced Visual Big Data Analysis. The topics include: An Experience on Cooperative Development of Interactive Visualizations for the Analysis of Urban Data; Explaining AI Through Critical Reflection Artifacts: On the Role of Communication Design Within XAI; Information Visualization and Visual Analytics at IVU Lab; visual Analytics for Financial Crime Detection at the University of Perugia; a Visual Analytics Technique to Compare the Performance of Predictive Models; affective Analytics and Visualization for Ensemble Event-Driven Stock Market Forecasting; understanding the Role of (Advanced) Machine Learning in Metagenomic Workflows; intelligent Advanced User Interfaces for Monitoring Mental Health Wellbeing; Towards Explainable Artificial Intelligence and Explanation User Interfaces to Open the ‘Black Box’ of Automated ECG Interpretation; recognition and Visualization of Facial Expression and Emotion in Healthcare; machine Learning in Healthcare: Breast Cancer and Diabetes Cases; AI2VIS4BigData: Qualitative Evaluation of an AI-Based Big Data Analysis and Visualization Reference Model.","Conference review","Final","","Scopus","2-s2.0-85102616006"
"Babic B.B.; Gerke S.; Evgeniou T.; Glenn Cohen I.","Babic, By Boris (57212252217); Gerke, Sara (57199176578); Evgeniou, Theodoros (6603074119); Glenn Cohen, I. (25653634900)","57212252217; 57199176578; 6603074119; 25653634900","Beware explanations from AI in health care the benefits of explainable artificial intelligence are not what they appear","2021","Science","373","6552","","284","286","2","81","10.1126/science.abg1834","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110525466&doi=10.1126%2fscience.abg1834&partnerID=40&md5=9c4f4a466a114ee3eded6ee4d2cdcff2","[No abstract available]","Article","Final","","Scopus","2-s2.0-85110525466"
"Kavya R.; Christopher J.; Panda S.; Lazarus Y.B.","Kavya, Ramisetty (57216408556); Christopher, Jabez (56692361800); Panda, Subhrakanta (55786847500); Lazarus, Y. Bakthasingh (57225053731)","57216408556; 56692361800; 55786847500; 57225053731","Machine Learning and XAI approaches for Allergy Diagnosis","2021","Biomedical Signal Processing and Control","69","","102681","","","","30","10.1016/j.bspc.2021.102681","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109042148&doi=10.1016%2fj.bspc.2021.102681&partnerID=40&md5=25c218971ee66bffabdbfcc525bc23d1","This work presents a computer-aided framework for allergy diagnosis which is capable of handling comorbidities. The system was developed using datasets collected from allergy testing centers in South India. Intradermal skin test results of 878 patients were recorded and it was observed that the data contained very few samples for comorbid conditions. Modified data sampling techniques were applied to handle this data imbalance for improving the efficiency of the learning algorithms. The algorithms were cross-validated to choose the optimal trained model for multi-label classification. The transparency of the machine learning models was ensured using post-hoc explainable artificial intelligence approaches. The system was tested by verifying the performance of a trained random forest model on the test data. The training and validation accuracy rate of the decision tree, support vector machine and random forest are 81.62, 81.04 and 83.07 respectively. During evaluation, random forest achieved a rate of 86.39 accuracy overall, and 75% sensitivity for the comorbid Rhinitis-Urticaria class. The framework along with all the functionalities were deployed on mobile devices. The average performance of the clinicians before and after using the decision support system were 77.21% and 81.80% respectively. The diagnosis system integrated with mobile applications serves as a source of information whereby junior clinicians can use it to confirm their diagnostic predictions. © 2021 Elsevier Ltd","Article","Final","","Scopus","2-s2.0-85109042148"
"Marvin G.; Alam M.G.R.","Marvin, Ggaliwango (57302525500); Alam, Md. Golam Rabiul (57675622100)","57302525500; 57675622100","Cardiotocogram Biomedical Signal Classification and Interpretation for Fetal Health Evaluation","2021","2021 IEEE Asia-Pacific Conference on Computer Science and Data Engineering, CSDE 2021","","","","","","","5","10.1109/CSDE53843.2021.9718415","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127874965&doi=10.1109%2fCSDE53843.2021.9718415&partnerID=40&md5=ee536a898cb9a5fbf1323c433211f9ea","Maternal and Neonatal health has been greatly constrained by the in-access to essential maternal health care services due to the preventive measures implemented against the spread of covid-19 hence making maternal and fetal monitoring so hard for physicians. Besides maternal toxic stress caused by fear of catching covid-19, affordable mobility of pregnant mothers to skilled health practitioners in limited resource settings is another contributor to maternal and neonatal mortality and morbidity. In this work, we leveraged existing health data to build interpretable Machine Learning (ML) models that allow physicians to offer precision maternal and fetal medicine based on biomedical signal classification results of fetal cardiotocograms (CTGs).We obtained 99%, 100% and 97% accuracy, precision and recall respectively for the LightGBM classification model without any GPU Learning resources. Then we explainably evaluated all built models with ELI5 and comprehensive feature extraction.  © IEEE 2022.","Conference paper","Final","","Scopus","2-s2.0-85127874965"
"Wilhelm D.; Bouarfa L.; Navab N.; Meining A.; Müller-Stich B.; Jarc A.; Padoy N.","Wilhelm, Dirk (7006636007); Bouarfa, Loubna (35329201800); Navab, Nassir (7003458998); Meining, Alexander (7003303890); Müller-Stich, Beat (14322034300); Jarc, Anthony (26635229800); Padoy, Nicolas (24338962300)","7006636007; 35329201800; 7003458998; 7003303890; 14322034300; 26635229800; 24338962300","Artificial intelligence in visceral medicine","2020","Visceral Medicine","36","6","","471","475","4","5","10.1159/000512440","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095995724&doi=10.1159%2f000512440&partnerID=40&md5=b7a0c3a1bed9ada3fc5f36b8732c2b4e","[No abstract available]","Article","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85095995724"
"Song X.; Yu A.S.L.; Kellum J.A.; Waitman L.R.; Matheny M.E.; Simpson S.Q.; Hu Y.; Liu M.","Song, Xing (57201297291); Yu, Alan S. L. (13003044500); Kellum, John A. (7103325757); Waitman, Lemuel R. (6603246496); Matheny, Michael E. (57210774976); Simpson, Steven Q. (58863560200); Hu, Yong (55682757000); Liu, Mei (56337948100)","57201297291; 13003044500; 7103325757; 6603246496; 57210774976; 58863560200; 55682757000; 56337948100","Cross-site transportability of an explainable artificial intelligence model for acute kidney injury prediction","2020","Nature Communications","11","1","5668","","","","57","10.1038/s41467-020-19551-w","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095692331&doi=10.1038%2fs41467-020-19551-w&partnerID=40&md5=8aa90f760f35dd0dd8af52b44252e40d","Artificial intelligence (AI) has demonstrated promise in predicting acute kidney injury (AKI), however, clinical adoption of these models requires interpretability and transportability. Non-interoperable data across hospitals is a major barrier to model transportability. Here, we leverage the US PCORnet platform to develop an AKI prediction model and assess its transportability across six independent health systems. Our work demonstrates that cross-site performance deterioration is likely and reveals heterogeneity of risk factors across populations to be the cause. Therefore, no matter how accurate an AI model is trained at the source hospital, whether it can be adopted at target hospitals is an unanswered question. To fill the research gap, we derive a method to predict the transportability of AI models which can accelerate the adaptation process of external AI models in hospitals. © 2020, The Author(s).","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85095692331"
"Deramgozin M.; Jovanovic S.; Rabah H.; Ramzan N.","Deramgozin, M. (57190847391); Jovanovic, S. (24479758400); Rabah, H. (6508206988); Ramzan, N. (16069861100)","57190847391; 24479758400; 6508206988; 16069861100","A Hybrid Explainable AI Framework Applied to Global and Local Facial Expression Recognition","2021","IST 2021 - IEEE International Conference on Imaging Systems and Techniques, Proceedings","","","","","","","7","10.1109/IST50367.2021.9651357","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124339587&doi=10.1109%2fIST50367.2021.9651357&partnerID=40&md5=d92adcf6b13a759a10c695eb416d2ad3","Facial Expression Recognition (FER) systems have many applications such as human behavior understanding, human machine interface, video games and health monitoring. The main advantage of the traditional white box methods is their explainability. However, the accuracy of recognition of these methods is completely reliant on the extracted features. On the other hand, the use of deep neural networks has advantage regarding the overall precision compared to traditional methods. Indeed, they are considered as black box methods and thus suffer from lack of reliability and explainability. In this work, we introduce a hybrid AI explainable framework (HEF) composed of a main functional pipeline comprising a Convolutional Neural Network (CNN) to classify input images and an explainable pipeline using Facial Action Units and application agnostic models LIME providing more useful data allowing to explain the obtained results and reinforce the decision provided by the main functional pipeline. The proposed HEF has been validated on the CK+ dataset and shows very promising results in terms of explainability of the obtained results. © 2021 IEEE.","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85124339587"
"Ahn I.; Gwon H.; Kang H.; Kim Y.; Seo H.; Choi H.; Cho H.N.; Kim M.; Jun T.J.; Kim Y.-H.","Ahn, Imjin (57221780901); Gwon, Hansle (57221779785); Kang, Heejun (57204768650); Kim, Yunha (57221775986); Seo, Hyeram (57298277400); Choi, Heejung (57298685400); Cho, Ha Na (57297668700); Kim, Minkyoung (57356592700); Jun, Tae Joon (57191960185); Kim, Young-Hak (36067581900)","57221780901; 57221779785; 57204768650; 57221775986; 57298277400; 57298685400; 57297668700; 57356592700; 57191960185; 36067581900","Machine learning-based hospital discharge prediction for patients with cardiovascular diseases:development and usability study","2021","JMIR Medical Informatics","9","11","e32662","","","","4","10.2196/32662","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120166300&doi=10.2196%2f32662&partnerID=40&md5=eb9530ff1c173218c68d9f0984f2cd24","Background: Effective resource management in hospitals can improve the quality of medical services by reducing labor-intensive burdens on staff, decreasing inpatient waiting time, and securing the optimal treatment time. The use of hospital processes requires effective bed management; a stay in the hospital that is longer than the optimal treatment time hinders bed management. Therefore, predicting a patient's hospitalization period may support the making of judicious decisions regarding bed management. Objective: First, this study aims to develop a machine learning (ML)-based predictive model for predicting the discharge probability of inpatients with cardiovascular diseases (CVDs). Second, we aim to assess the outcome of the predictive model and explain the primary risk factors of inpatients for patient-specific care. Finally, we aim to evaluate whether our ML-based predictive model helps manage bed scheduling efficiently and detects long-term inpatients in advance to improve the use of hospital processes and enhance the quality of medical services. Methods: We set up the cohort criteria and extracted the data from CardioNet, a manually curated database that specializes in CVDs. We processed the data to create a suitable data set by reindexing the date-index, integrating the present features with past features from the previous 3 years, and imputing missing values. Subsequently, we trained the ML-based predictive models and evaluated them to find an elaborate model. Finally, we predicted the discharge probability within 3 days and explained the outcomes of the model by identifying, quantifying, and visualizing its features. Results: We experimented with 5 ML-based models using 5 cross-validations. Extreme gradient boosting, which was selected as the final model, accomplished an average area under the receiver operating characteristic curve score that was 0.865 higher than that of the other models (ie, logistic regression, random forest, support vector machine, and multilayer perceptron). Furthermore, we performed feature reduction, represented the feature importance, and assessed prediction outcomes. One of the outcomes, the individual explainer, provides a discharge score during hospitalization and a daily feature influence score to the medical team and patients. Finally, we visualized simulated bed management to use the outcomes. Conclusions: In this study, we propose an individual explainer based on an ML-based predictive model, which provides the discharge probability and relative contributions of individual features. Our model can assist medical teams and patients in identifying individual and common risk factors in CVDs and can support hospital administrators in improving the management of hospital beds and other resources. © 2021 Eesti Rakenduslingvistika Uhingu Aastaraamat. All rights reserved.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85120166300"
"Lauritsen S.M.; Kristensen M.; Olsen M.V.; Larsen M.S.; Lauritsen K.M.; Jørgensen M.J.; Lange J.; Thiesson B.","Lauritsen, Simon Meyer (57201018315); Kristensen, Mads (57218340122); Olsen, Mathias Vassard (57205752817); Larsen, Morten Skaarup (23974216100); Lauritsen, Katrine Meyer (57204714703); Jørgensen, Marianne Johansson (55577646300); Lange, Jeppe (25936350800); Thiesson, Bo (55892450200)","57201018315; 57218340122; 57205752817; 23974216100; 57204714703; 55577646300; 25936350800; 55892450200","Explainable artificial intelligence model to predict acute critical illness from electronic health records","2020","Nature Communications","11","1","3852","","","","194","10.1038/s41467-020-17431-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088868771&doi=10.1038%2fs41467-020-17431-x&partnerID=40&md5=353d3fa811d0215765b6d64305ecf734","Acute critical illness is often preceded by deterioration of routinely measured clinical parameters, e.g., blood pressure and heart rate. Early clinical prediction is typically based on manually calculated screening metrics that simply weigh these parameters, such as early warning scores (EWS). The predictive performance of EWSs yields a tradeoff between sensitivity and specificity that can lead to negative outcomes for the patient. Previous work on electronic health records (EHR) trained artificial intelligence (AI) systems offers promising results with high levels of predictive performance in relation to the early, real-time prediction of acute critical illness. However, without insight into the complex decisions by such system, clinical translation is hindered. Here, we present an explainable AI early warning score (xAI-EWS) system for early detection of acute critical illness. xAI-EWS potentiates clinical translation by accompanying a prediction with information on the EHR data explaining it. © 2020, The Author(s).","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85088868771"
"Ayobi A.; Stawarz K.; Katz D.; Marshall P.; Yamagata T.; Santos-Rodriguez R.; Flach P.; O'Kane A.A.","Ayobi, Amid (57193529390); Stawarz, Katarzyna (55258782400); Katz, Dmitri (57191503338); Marshall, Paul (57191485447); Yamagata, Taku (57219685215); Santos-Rodriguez, Raul (27968154800); Flach, Peter (7004057691); O'Kane, Aisling Ann (56157053200)","57193529390; 55258782400; 57191503338; 57191485447; 57219685215; 27968154800; 7004057691; 56157053200","Co-Designing Personal Health? Multidisciplinary Benefits and Challenges in Informing Diabetes Self-Care Technologies","2021","Proceedings of the ACM on Human-Computer Interaction","5","CSCW2","457","","","","7","10.1145/3479601","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117933041&doi=10.1145%2f3479601&partnerID=40&md5=cc0b32b1a43f0ef77bc8197900214881","Co-design is a widely applied design process with well-documented values, including mutual learning and collective creativity. However, the real-world challenges of conducting multidisciplinary co-design research to inform the design of self-care technologies are not well established. We provide a qualitative account of a multidisciplinary project that aimed to co-design machine learning applications for Type 1 Diabetes (T1D) self-management. Through interviews, we identify not only perceived social, technological and strategic benefits of co-design but also organisational, translational and pragmatic design challenges: participants with T1D experienced difficulties in co-designing systems that met their individual self-care needs as part of group activities; HCI and AI researchers described challenges resulting from applying co-design outcomes to data-driven ML work; and industry collaborators highlighted academic data sharing regulations as cross-organisational challenges that can impede co-design efforts. Based on this understanding, we discuss opportunities for supporting multidisciplinary collaborations and aligning individual health needs with collaborative co-design activities.  © 2021 ACM.","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85117933041"
"Holzinger A.","Holzinger, Andreas (23396282000)","23396282000","The Next Frontier: AI We Can Really Trust","2021","Communications in Computer and Information Science","1524 CCIS","","","427","440","13","77","10.1007/978-3-030-93736-2_33","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126192307&doi=10.1007%2f978-3-030-93736-2_33&partnerID=40&md5=0b72544816d6578ae936a20ea652b8b0","Enormous advances in the domain of statistical machine learning, the availability of large amounts of training data, and increasing computing power have made Artificial Intelligence (AI) very successful. For certain tasks, algorithms can even achieve performance beyond the human level. Unfortunately, the most powerful methods suffer from the fact that it is difficult to explain why a certain result was achieved on the one hand, and that they lack robustness on the other. Our most powerful machine learning models are very sensitive to even small changes. Perturbations in the input data can have a dramatic impact on the output and lead to entirely different results. This is of great importance in virtually all critical domains where we suffer from low data quality, i.e. we do not have the expected i.i.d. data. Therefore, the use of AI in domains that impact human life (agriculture, climate, health,..) has led to an increased demand for trustworthy AI. Explainability is now even mandatory due to regulatory requirements in sensitive domains such as medicine, which requires traceability, transparency and interpretability capabilities. One possible step to make AI more robust is to combine statistical learning with knowledge representations. For certain tasks, it can be advantageous to use a human in the loop. A human expert can - sometimes, of course not always - bring experience, domain knowledge and conceptual understanding to the AI pipeline. Such approaches are not only a solution from a legal point of view, but in many application areas the “why” is often more important than a pure classification result. Consequently, both explainability and robustness can promote reliability and trust and ensure that humans remain in control, thus complementing human intelligence with artificial intelligence. © 2021, Springer Nature Switzerland AG.","Conference paper","Final","","Scopus","2-s2.0-85126192307"
"Knapič S.; Malhi A.; Saluja R.; Främling K.","Knapič, Samanta (57218279054); Malhi, Avleen (56663163300); Saluja, Rohit (57223830730); Främling, Kary (6506103412)","57218279054; 56663163300; 57223830730; 6506103412","Explainable Artificial Intelligence for Human Decision Support System in the Medical Domain","2021","Machine Learning and Knowledge Extraction","3","3","","740","770","30","60","10.3390/make3030037","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142788243&doi=10.3390%2fmake3030037&partnerID=40&md5=6deb5297b93cc307fc213c563d902a38","In this paper, we present the potential of Explainable Artificial Intelligence methods for decision support in medical image analysis scenarios. Using three types of explainable methods applied to the same medical image data set, we aimed to improve the comprehensibility of the decisions provided by the Convolutional Neural Network (CNN). In vivo gastral images obtained by a video capsule endoscopy (VCE) were the subject of visual explanations, with the goal of increasing health professionals’ trust in black-box predictions. We implemented two post hoc interpretable machine learning methods, called Local Interpretable Model-Agnostic Explanations (LIME) and SHapley Additive exPlanations (SHAP), and an alternative explanation approach, the Contextual Importance and Utility (CIU) method. The produced explanations were assessed by human evaluation. We conducted three user studies based on explanations provided by LIME, SHAP and CIU. Users from different non-medical backgrounds carried out a series of tests in a web-based survey setting and stated their experience and understanding of the given explanations. Three user groups (n = 20, 20, 20) with three distinct forms of explanations were quantitatively analyzed. We found that, as hypothesized, the CIU-explainable method performed better than both LIME and SHAP methods in terms of improving support for human decision-making and being more transparent and thus understandable to users. Additionally, CIU outperformed LIME and SHAP by generating explanations more rapidly. Our findings suggest that there are notable differences in human decision-making between various explanation support settings. In line with that, we present three potential explainable methods that, with future improvements in implementation, can be generalized to different medical data sets and can provide effective decision support to medical experts. © 2021 by the authors.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85142788243"
"Tao J.; Xiong Y.; Zhao S.; Xu Y.; Lin J.; Wu R.; Fan C.","Tao, Jianrong (57203392720); Xiong, Yu (57220887562); Zhao, Shiwei (57220101188); Xu, Yuhong (57211940567); Lin, Jianshi (57210646014); Wu, Runze (57001915300); Fan, Changjie (57203398908)","57203392720; 57220887562; 57220101188; 57211940567; 57210646014; 57001915300; 57203398908","XAI-Driven Explainable Multi-view Game Cheating Detection","2020","IEEE Conference on Computatonal Intelligence and Games, CIG","2020-August","","9231843","144","151","7","11","10.1109/CoG47356.2020.9231843","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096923372&doi=10.1109%2fCoG47356.2020.9231843&partnerID=40&md5=1eb789679c4733e2ac8c8eaad5748919","Online gaming is one of the most successful applications having a large number of players interacting in an online persistent virtual world through the Internet. However, some cheating players gain improper advantages over normal players by using illegal automated plugins which has brought huge harm to game health and player enjoyment. Game industries have been devoting much efforts on cheating detection with multiview data sources and achieved great accuracy improvements by applying artificial intelligence (AI) techniques. However, generating explanations for cheating detection from multiple views still remains a challenging task. To respond to the different purposes of explainability in AI models from different audience profiles, we propose the EMGCD, the first explainable multi-view game cheating detection framework driven by explainable AI (XAI). It combines cheating explainers to cheating classifiers from different views to generate individual, local and global explanations which contributes to the evidence generation, reason generation, model debugging and model compression. The EMGCD has been implemented and deployed in multiple game productions in NetEase Games, achieving remarkable and trustworthy performance. Our framework can also easily generalize to other types of related tasks in online games, such as explainable recommender systems, explainable churn prediction, etc.  © 2020 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85096923372"
"Clementino J.M., Jr.; Faical B.S.; Bones C.C.; Traina C., Jr.; Gutierrez M.A.; Traina A.J.M.","Clementino, Jose M. (57219055612); Faical, Bruno S. (51461140100); Bones, Christian C. (57190227919); Traina, Caetano (6701924244); Gutierrez, Marco A. (35581214400); Traina, Agma J. M. (7003628271)","57219055612; 51461140100; 57190227919; 6701924244; 35581214400; 7003628271","Multilevel clustering explainer: An explainable approach to electronic health records","2021","Proceedings - IEEE Symposium on Computer-Based Medical Systems","2021-June","","9474753","253","258","5","1","10.1109/CBMS52027.2021.00073","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110799705&doi=10.1109%2fCBMS52027.2021.00073&partnerID=40&md5=f2d9ad3b6a619e5b7e6bc19b14447863","Machine learning (ML) algorithms have been used in many areas of activity, and their results can often be applied without further human intervention. The ML algorithms have also been widely used in medical contexts, but in this area, the result needs to be thoroughly confirmed by a specialist, who needs explanatory information on how the results were obtained. Aimed at such scenarios, we propose the Multilevel Clustering Explainer (MCE), a method capable of providing explanatory information to health professionals about the knowledge discovery process. The MCE was developed for the analysis of medical data, providing a synthesis of explanatory information for the specialist to quickly and clearly understand how the results were obtained.  © 2021 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85110799705"
"Naiseh M.; Jiang N.; Ma J.; Ali R.","Naiseh, Mohammad (57217108046); Jiang, Nan (56038315600); Ma, Jianbing (23397645400); Ali, Raian (56038311800)","57217108046; 56038315600; 23397645400; 56038311800","Explainable Recommendations in Intelligent Systems: Delivery Methods, Modalities and Risks","2020","Lecture Notes in Business Information Processing","385 LNBIP","","","212","228","16","16","10.1007/978-3-030-50316-1_13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087765799&doi=10.1007%2f978-3-030-50316-1_13&partnerID=40&md5=777419a1e8c2f9de57cc64b10ea4840e","With the increase in data volume, velocity and types, intelligent human-agent systems have become popular and adopted in different application domains, including critical and sensitive areas such as health and security. Humans’ trust, their consent and receptiveness to recommendations are the main requirement for the success of such services. Recently, the demand on explaining the recommendations to humans has increased both from humans interacting with these systems so that they make an informed decision and, also, owners and systems managers to increase transparency and consequently trust and users’ retention. Existing systematic reviews in the area of explainable recommendations focused on the goal of providing explanations, their presentation and informational content. In this paper, we review the literature with a focus on two user experience facets of explanations; delivery methods and modalities. We then focus on the risks of explanation both on user experience and their decision making. Our review revealed that explanations delivery to end-users is mostly designed to be along with the recommendation in a push and pull styles while archiving explanations for later accountability and traceability is still limited. We also found that the emphasis was mainly on the benefits of recommendations while risks and potential concerns, such as over-reliance on machines, is still a new area to explore. © 2020, Springer Nature Switzerland AG.","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85087765799"
"Sisman A.R.; Basok B.I.","Sisman, Ali Riza (6701635293); Basok, Banu Isbilen (56241221800)","6701635293; 56241221800","Digitalization and artificial intelligence in laboratory medicine","2020","International Journal of Medical Biochemistry","3","2","","106","110","4","2","10.14744/ijmb.2020.81994","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123717969&doi=10.14744%2fijmb.2020.81994&partnerID=40&md5=3d271a5337830b9170454d4ec84e200c","In parallel with the increasing number and variety of medical tests and the widespread use of electronic health records combined with the growing capabilities and capacity of computers have attention to “big data.” Processing and extract-ing meaningful interpretations from such vast and complex data require artificial intelligence (AI) that refers to complex software systems that enable computers to augment and even imitate human intelligence and decision-making. Machine learning (ML) is a subfield of AI that uses algorithms to parse and learn data and then apply this new learning to make predictions and informed recommendations. In recent years, the effects that the digitalization of healthcare services will have on medicine, especially laboratory medicine as seen in the industry, the economy, and social life. The abundance of health data will lead to a shift from analytical competence in diagnostic tests to the ability to integrate data and simultaneously interpret them within the clinical context. Therefore, “computational laboratory medicine” units should be established and integrated into resident and undergraduate education curricula. Using the computational approach, the promise of improved medical interpretation will further increase the effectiveness of laboratory diagnostics in the process of intensive dialogue/ consultation and clinical decision-making. Medical laboratories may play an active role in the future as a ""nerve center of diagnostics"" and joining the patient and physician to form a ""Diagnostics 4.0"" triangle. As the big data continue to grow in healthcare, the need for implementing AI and ML techniques into laboratory medicine is inevitable. In this new AI-supported era, clinical laboratories will move towards a more specialized role in translational medicine, advanced technology, management of clinical information, and quality control of results generated outside the laboratory. The field of laboratory medicine should consider such a development sooner rather than later. © 2020 by International Journal of Medical Biochemistry.","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85123717969"
"Senatore R.; Cioppa A.D.; Marcelli A.","Senatore, Rosa (36983155800); Cioppa, Antonio Della (57215948865); Marcelli, Angelo (7005267205)","36983155800; 57215948865; 7005267205","Automatic diagnosis of Parkinson disease through handwriting analysis: A cartesian genetic programming approach","2019","Proceedings - IEEE Symposium on Computer-Based Medical Systems","2019-June","","8787399","312","317","5","14","10.1109/CBMS.2019.00071","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070960719&doi=10.1109%2fCBMS.2019.00071&partnerID=40&md5=9a9b7b38125d6a0ea5597441c9d39df0","Early disease identification through non-invasive and automatic techniques has gathered increasing interest by the scientific community in the last decades. In this context, Parkinsons Disease (PD) has received particular attention in that it is a severe and progressive neurodegenerative disease and, therefore, early diagnosis would provide more prompt and effective intervention strategies. This, in turn, would successfully influence the life expectancy of the patients. However, the acceptance of computer-based diagnosis by doctors is hampered by the black-box approach implemented by the most performing systems, such as Artificial Neural Networks and Support Vector Machines, which do not explicit the rules adopted by the system. In this context, we propose a Cartesian Genetic Programming, aimed at automatically identify PD through the analysis of handwriting performed by PD patients and healthy controls. The use of such approach is particularly interesting in that it allows to infer explicit models of classification and, at same time, to automatically identify a suitable subset of features relevant for a correct diagnosis. The approach has been evaluated on the features extracted from the handwriting samples contained in the publicly available PaHaW dataset. Experimental results show that our approach compares favorably with state-of-the-art methods and, more importantly, provides an explicit model of the classification criteria. © 2019 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85070960719"
"Rickett C.D.; Maschhoff K.J.; Sukumar S.R.","Rickett, Christopher D. (57221551395); Maschhoff, Kristyn J. (6602546558); Sukumar, Sreenivas R. (8988886000)","57221551395; 6602546558; 8988886000","Does tetanus vaccination contribute to reduced severity of the COVID-19 infection?","2021","Medical Hypotheses","146","","110395","","","","6","10.1016/j.mehy.2020.110395","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099431905&doi=10.1016%2fj.mehy.2020.110395&partnerID=40&md5=9ad450d4e0f8d6417bee512e160e48f0","We present the hypothesis to the scientific community actively designing clinical trials and recommending public health guidelines to control the pandemic that – “Tetanus vaccination may be contributing to reduced severity of the COVID-19 infection” – and urge further research to validate or invalidate the effectiveness of the tetanus toxoid vaccine against COVID-19. This hypothesis was revealed by an explainable artificial intelligence system unleashed on open public biomedical datasets. As a foundation for scientific rigor, we describe the data and the artificial intelligence system, document the provenance and methodology used to derive the hypothesis and also gather potentially relevant data/evidence from recent studies. We conclude that while correlations may not be reason for causation, correlations from multiple sources is more than a serendipitous coincidence that is worthy of further and deeper investigation. © 2020 Elsevier Ltd","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85099431905"
"Kim B.; Srinivasan K.; Ram S.","Kim, Buomsoo (57215896795); Srinivasan, Karthik (57189235644); Ram, Sudha (35608028600)","57215896795; 57189235644; 35608028600","Robust local explanations for healthcare predictive analytics: An application to fragility fracture risk modeling","2019","40th International Conference on Information Systems, ICIS 2019","","","","","","","1","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114902619&partnerID=40&md5=3b9a57d53e8064ac45f642d812ad9bc4","With recent advancements in data analytics, healthcare predictive analytics (HPA) is garnering growing interest among practitioners and researchers. However, it is risky to blindly accept the results and users will not accept the HPA model if transparency is not guaranteed. To address this challenge, we propose the RObust Local EXplanations (ROLEX) method, which provides robust, instance-level explanations for any HPA model. The applicability of the ROLEX method is demonstrated using the fragility fracture prediction problem. Analysis with a large real-world dataset demonstrates that our method outperforms state-of-the-art methods in terms of local fidelity. The ROLEX method is applicable to various types of HPA problems beyond the fragility fracture problem. It is applicable to any type of supervised learning model and provides fine-grained explanations that can improve understanding of the phenomenon of interest. Finally, we discuss theoretical implications of our study in light of healthcare IS, big data, and design science. © 40th International Conference on Information Systems, ICIS 2019. All rights reserved.","Conference paper","Final","","Scopus","2-s2.0-85114902619"
"Smucny J.; Davidson I.; Carter C.S.","Smucny, Jason (25951785700); Davidson, Ian (12344826300); Carter, Cameron S. (55186906500)","25951785700; 12344826300; 55186906500","Comparing machine and deep learning-based algorithms for prediction of clinical improvement in psychosis with functional magnetic resonance imaging","2021","Human Brain Mapping","42","4","","1197","1205","8","21","10.1002/hbm.25286","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096697583&doi=10.1002%2fhbm.25286&partnerID=40&md5=6ea4d79d2d9e1ffe23b56a5485268fbe","Previous work using logistic regression suggests that cognitive control-related frontoparietal activation in early psychosis can predict symptomatic improvement after 1 year of coordinated specialty care with 66% accuracy. Here, we evaluated the ability of six machine learning (ML) algorithms and deep learning (DL) to predict “Improver” status (>20% improvement on Brief Psychiatric Rating Scale [BPRS] total score at 1-year follow-up vs. baseline) and continuous change in BPRS score using the same functional magnetic resonance imaging-based features (frontoparietal activations during the AX-continuous performance task) in the same sample (individuals with either schizophrenia (n = 65, 49M/16F, mean age 20.8 years) or Type I bipolar disorder (n = 17, 9M/8F, mean age 21.6 years)). 138 healthy controls were included as a reference group. “Shallow” ML methods included Naive Bayes, support vector machine, K Star, AdaBoost, J48 decision tree, and random forest. DL included an explainable artificial intelligence (XAI) procedure for understanding results. The best overall performances (70% accuracy for the binary outcome and root mean square error = 9.47 for the continuous outcome) were achieved using DL. XAI revealed left DLPFC activation was the strongest feature used to make binary classification decisions, with a classification activation threshold (adjusted beta =.017) intermediate to the healthy control mean (adjusted beta =.15, 95% CI = −0.02 to 0.31) and patient mean (adjusted beta = −.13, 95% CI = −0.37 to 0.11). Our results suggest DL is more powerful than shallow ML methods for predicting symptomatic improvement. The left DLPFC may be a functional target for future biomarker development as its activation was particularly important for predicting improvement. © 2020 The Authors. Human Brain Mapping published by Wiley Periodicals LLC.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85096697583"
"Kobylińska K.; Mikołajczyk T.; Adamek M.; Orłowski T.; Biecek P.","Kobylińska, Katarzyna (57214219923); Mikołajczyk, Tomasz (7005656341); Adamek, Mariusz (7003377712); Orłowski, Tadeusz (7103127686); Biecek, Przemysław (23003457600)","57214219923; 7005656341; 7003377712; 7103127686; 23003457600","Explainable Machine Learning for Modeling of Early Postoperative Mortality in Lung Cancer","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11979 LNAI","","","161","174","13","7","10.1007/978-3-030-37446-4_13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078401371&doi=10.1007%2f978-3-030-37446-4_13&partnerID=40&md5=80a0f7a54ad6499d6f52682587955d10","In recent years we see an increasing interest in applications of complex machine learning methods to medical problems. Black box models based on deep neural networks or ensembles are more and more popular in diagnostic, personalized medicine (Hamet and Tremblay 2017) or screening studies (Scheeder et al. 2018). Partially because they are accurate and easy to train. Nevertheless such models may be hard to understand and interpret. In high stake decisions, especially in medicine, the understanding of factors that drive model decisions is crucial. Lack of model understanding creates a serious risk in applications. In our study we propose and validate new approaches to exploration and explanation of predictive models for early postoperative mortality in lung cancer patients. Models are created on the Domestic Lung Cancer Database run by the National Institute of Tuberculosis and Lung Diseases. We show how explainable machine learning techniques can be used to combine data driven signals with domain knowledge. Additionally we explore whether the insight provided by model explainers give valuable information for physicians. © Springer Nature Switzerland AG 2019.","Conference paper","Final","","Scopus","2-s2.0-85078401371"
"Payrovnaziri S.N.; Chen Z.; Rengifo-Moreno P.; Miller T.; Bian J.; Chen J.H.; Liu X.; He Z.","Payrovnaziri, Seyedeh Neelufar (57210788535); Chen, Zhaoyi (57193076426); Rengifo-Moreno, Pablo (24169504600); Miller, Tim (7403948057); Bian, Jiang (7103200005); Chen, Jonathan H. (57112911000); Liu, Xiuwen (57202583383); He, Zhe (55320918000)","57210788535; 57193076426; 24169504600; 7403948057; 7103200005; 57112911000; 57202583383; 55320918000","Explainable artificial intelligence models using real-world electronic health record data: A systematic scoping review","2020","Journal of the American Medical Informatics Association","27","7","","1173","1185","12","158","10.1093/jamia/ocaa053","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088611016&doi=10.1093%2fjamia%2focaa053&partnerID=40&md5=2e5be4326cef6d64de8c00965257e3fe","Objective: To conduct a systematic scoping review of explainable artificial intelligence (XAI) models that use real-world electronic health record data, categorize these techniques according to different biomedical applications, identify gaps of current studies, and suggest future research directions. Materials and Methods: We searched MEDLINE, IEEE Xplore, and the Association for Computing Machinery (ACM) Digital Library to identify relevant papers published between January 1, 2009 and May 1, 2019. We summarized these studies based on the year of publication, prediction tasks, machine learning algorithm, dataset(s) used to build the models, the scope, category, and evaluation of the XAI methods. We further assessed the reproducibility of the studies in terms of the availability of data and code and discussed open issues and challenges. Results: Forty-two articles were included in this review. We reported the research trend and most-studied diseases. We grouped XAI methods into 5 categories: knowledge distillation and rule extraction (N = 13), intrinsically interpretable models (N = 9), data dimensionality reduction (N = 8), attention mechanism (N = 7), and feature interaction and importance (N = 5). Discussion: XAI evaluation is an open issue that requires a deeper focus in the case of medical applications. We also discuss the importance of reproducibility of research work in this field, as well as the challenges and opportunities of XAI from 2 medical professionals' point of view. Conclusion: Based on our review, we found that XAI evaluation in medicine has not been adequately and formally practiced. Reproducibility remains a critical concern. Ample opportunities exist to advance XAI research in medicine.  © 2020 The Author(s) 2020. Published by Oxford University Press on behalf of the American Medical Informatics Association. All rights reserved.","Review","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85088611016"
"Pawar U.; Culbert C.T.; O'Reilly R.","Pawar, Urja (57202719602); Culbert, Christopher T (57226194932); O'Reilly, Ruairi (57204975215)","57202719602; 57226194932; 57204975215","Evaluating hierarchical medical workflows using feature importance","2021","Proceedings - IEEE Symposium on Computer-Based Medical Systems","2021-June","","9474678","265","270","5","3","10.1109/CBMS52027.2021.00075","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110870289&doi=10.1109%2fCBMS52027.2021.00075&partnerID=40&md5=4904fc77c58cf0e3e9ab3681c9ff457b","The applicability and utility of Artificial Intelligence (AI) based solutions has been demonstrated widely in the healthcare domain via the automated analysis of medical information. However, the adoption rate of AI-based healthcare systems is inhibited due to their complicated nature. Also, the hierarchical nature of the medical settings adds a layer of complexity in understanding how a Machine Learning (ML) model functions when subjected to varying workflows. Variation in models' performance, and individual features' contribution, needs to be effectively quantified such that medical practitioners can understand the models, and validate their operation if widespread adoption is to be enabled. In this paper, a hierarchical medical workflow for understanding the operation of ML in a healthcare-based setting is proposed. Its utility is demonstrated in the context of heart disease classification. Explainable Artificial Intelligence (XAI) is incorporated in the form of Feature Importance (FI) scores and correlated with an ML model's performance metrics (Accuracy, F1-score). This provides a multi-stakeholder perspective aligned with the hierarchy as experienced in a real-world medical setting. The paper contributes a methodology for accommodating an enhanced understanding of diverse hierarchical healthcare settings that would benefit from the adoption of AI-based systems.  © 2021 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85110870289"
"Bourdon P.; Ahmed O.B.; Urruty T.; Djemal K.; Fernandez-Maloigne C.","Bourdon, Pascal (7005784278); Ahmed, Olfa Ben (35770340300); Urruty, Thierry (23092149400); Djemal, Khalifa (14821991200); Fernandez-Maloigne, Christine (55900445500)","7005784278; 35770340300; 23092149400; 14821991200; 55900445500","Explainable ai for medical imaging: Knowledge matters","2021","Multi-faceted Deep Learning: Models and Data","","","","267","292","25","5","10.1007/978-3-030-74478-6_11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130092980&doi=10.1007%2f978-3-030-74478-6_11&partnerID=40&md5=badfb6bfa01180c854672c9b90ddca92","Cooperation between medical experts and virtual assistance depends on trust. Over recent years, machine learning algorithms have been able to construct models of high accuracy and predictive power. Yet as opposed to their earlier, hypothesis-driven counterparts, current data-driven models are increasingly criticized for their opaque decision-making process. Safety-critical applications such as self-driving cars or health status estimation cannot rely on benchmark-winning black-box models. They need prediction models which rationale and logic can be explained in an understandable, human-readable format, not just out of curiosity but also to highlight and deter potential biases. In this chapter we discuss how Explainable Artificial Intelligence (XAI) assesses such issues in medical imaging. We will also put focus on machine learning approaches developed for breast cancer diagnosis, and discuss the advent of deep learning in this particular domain. Indeed, despite promising results achieved over the last few years, advanced state of the art analysis identifies several important challenges faced by deep learning approaches. We will present the emerging trends and proposals to overcome these challenges. © Springer Nature Switzerland AG 2021.","Book chapter","Final","","Scopus","2-s2.0-85130092980"
"Qu Z.; Lau C.W.; Catchpoole D.R.; Simoff S.; Nguyen Q.V.","Qu, Zhonglin (57205712829); Lau, Chng Wei (57205716908); Catchpoole, Daniel R. (6701637669); Simoff, Simeon (6601912069); Nguyen, Quang Vinh (7202088881)","57205712829; 57205716908; 6701637669; 6601912069; 7202088881","Intelligent and immersive visual analytics of health data","2020","Studies in Computational Intelligence","891","","","29","44","15","5","10.1007/978-3-662-61114-2_3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083958640&doi=10.1007%2f978-3-662-61114-2_3&partnerID=40&md5=e0696977eb925f99c0d7f3e107d04d69","Massive amounts of health data have been created together with the advent of computer technologies and next generation sequencing technologies. Analytical techniques can significantly aid in the processing, integration and interpretation of the complex data. Visual analytics field has been rapidly evolving together with the advancement in automated analysis methods such as data mining, machine learning and statistics, visualization, and immersive technologies. Although automated analysis processes greatly support the decision making, conservative domains such as medicine, banking, and insurance need trusts on machine learning models. Explainable artificial intelligence could open the black boxes of the machine learning models to improve the trusts for decision makers. Immersive technologies allow the users to engage naturally with the blended reality in where they can look the information in different angles in addition to traditional screens. This chapter reviews and discusses the intelligent visualization, artificial intelligence and immersive technologies in health domain. We also illustrate the ideas with various case studies in genomic data visual analytics. © Springer-Verlag GmbH Germany, part of Springer Nature 2020.","Book chapter","Final","","Scopus","2-s2.0-85083958640"
"Duell J.; Fan X.; Burnett B.; Aarts G.; Zhou S.-M.","Duell, Jamie (57222478271); Fan, Xiuyi (35788022100); Burnett, Bruce (57471321400); Aarts, Gert (6603959726); Zhou, Shang-Ming (7404166600)","57222478271; 35788022100; 57471321400; 6603959726; 7404166600","A comparison of explanations given by explainable artificial intelligence methods on analysing electronic health records","2021","BHI 2021 - 2021 IEEE EMBS International Conference on Biomedical and Health Informatics, Proceedings","","","","","","","28","10.1109/BHI50953.2021.9508618","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121483145&doi=10.1109%2fBHI50953.2021.9508618&partnerID=40&md5=0b4f9cd34139bd5416b20cd2b3dc27f6","eXplainable Artificial Intelligence (XAI) aims to provide intelligible explanations to users. XAI algorithms such as SHAP, LIME and Scoped Rules compute feature importance for machine learning predictions. Although XAI has attracted much research attention, applying XAI techniques in healthcare to inform clinical decision making is challenging. In this paper, we provide a comparison of explanations given by XAI methods as a tertiary extension in analysing complex Electronic Health Records (EHRs). With a large-scale EHR dataset, we compare features of EHRs in terms of their prediction importance estimated by XAI models. Our experimental results show that the studied XAI methods circumstantially generate different top features; their aberrations in shared feature importance merit further exploration from domain-experts to evaluate human trust towards XAI. © 2021 IEEE","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85121483145"
"Ortega A.; Fierrez J.; Morales A.; Wang Z.; de la Cruz M.; Alonso C.L.; Ribeiro T.","Ortega, Alfonso (8935048300); Fierrez, Julian (55664171900); Morales, Aythami (24476050500); Wang, Zilong (57221634216); de la Cruz, Marina (7006142730); Alonso, César Luis (8899087500); Ribeiro, Tony (55661856600)","8935048300; 55664171900; 24476050500; 57221634216; 7006142730; 8899087500; 55661856600","Symbolic AI for XAI: Evaluating LFIT inductive programming for explaining biases in machine learning","2021","Computers","10","11","154","","","","6","10.3390/computers10110154","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119840594&doi=10.3390%2fcomputers10110154&partnerID=40&md5=cdee498df05ca4c70fb27e78f11b5da9","Machine learning methods are growing in relevance for biometrics and personal information processing in domains such as forensics, e-health, recruitment, and e-learning. In these domains, white-box (human-readable) explanations of systems built on machine learning methods become crucial. Inductive logic programming (ILP) is a subfield of symbolic AI aimed to automatically learn declarative theories about the processing of data. Learning from interpretation transition (LFIT) is an ILP technique that can learn a propositional logic theory equivalent to a given black-box system (under certain conditions). The present work takes a first step to a general methodology to incorporate accurate declarative explanations to classic machine learning by checking the viability of LFIT in a specific AI application scenario: fair recruitment based on an automatic tool generated with machine learning methods for ranking Curricula Vitae that incorporates soft biometric information (gender and ethnicity). We show the expressiveness of LFIT for this specific problem and propose a scheme that can be applicable to other domains. In order to check the ability to cope with other domains no matter the machine learning paradigm used, we have done a preliminary test of the expressiveness of LFIT, feeding it with a real dataset about adult incomes taken from the US census, in which we consider the income level as a function of the rest of attributes to verify if LFIT can provide logical theory to support and explain to what extent higher incomes are biased by gender and ethnicity. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85119840594"
"Giudici P.; Raffinetti E.","Giudici, Paolo (23491813000); Raffinetti, Emanuela (36601260100)","23491813000; 36601260100","Shapley-Lorenz eXplainable Artificial Intelligence","2021","Expert Systems with Applications","167","","114104","","","","97","10.1016/j.eswa.2020.114104","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094149359&doi=10.1016%2fj.eswa.2020.114104&partnerID=40&md5=2ad0976a448c20ccbe52307ac4cf2877","Explainability of artificial intelligence methods has become a crucial issue, especially in the most regulated fields, such as health and finance. In this paper, we provide a global explainable AI method which is based on Lorenz decompositions, thus extending previous contributions based on variance decompositions. This allows the resulting Shapley-Lorenz decomposition to be more generally applicable, and provides a unifying variable importance criterion that combines predictive accuracy with explainability, using a normalised and easy to interpret metric. The proposed decomposition is illustrated within the context of a real financial problem: the prediction of bitcoin prices. © 2020 The Authors","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85094149359"
"Markus A.F.; Kors J.A.; Rijnbeek P.R.","Markus, Aniek F. (57219787723); Kors, Jan A. (7005293297); Rijnbeek, Peter R. (6603033335)","57219787723; 7005293297; 6603033335","The role of explainability in creating trustworthy artificial intelligence for health care: A comprehensive survey of the terminology, design choices, and evaluation strategies","2021","Journal of Biomedical Informatics","113","","103655","","","","268","10.1016/j.jbi.2020.103655","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099215997&doi=10.1016%2fj.jbi.2020.103655&partnerID=40&md5=4f62193ff8b3d59a60c74153699e6e8d","Artificial intelligence (AI) has huge potential to improve the health and well-being of people, but adoption in clinical practice is still limited. Lack of transparency is identified as one of the main barriers to implementation, as clinicians should be confident the AI system can be trusted. Explainable AI has the potential to overcome this issue and can be a step towards trustworthy AI. In this paper we review the recent literature to provide guidance to researchers and practitioners on the design of explainable AI systems for the health-care domain and contribute to formalization of the field of explainable AI. We argue the reason to demand explainability determines what should be explained as this determines the relative importance of the properties of explainability (i.e. interpretability and fidelity). Based on this, we propose a framework to guide the choice between classes of explainable AI methods (explainable modelling versus post-hoc explanation; model-based, attribution-based, or example-based explanations; global and local explanations). Furthermore, we find that quantitative evaluation metrics, which are important for objective standardized evaluation, are still lacking for some properties (e.g. clarity) and types of explanations (e.g. example-based methods). We conclude that explainable modelling can contribute to trustworthy AI, but the benefits of explainability still need to be proven in practice and complementary measures might be needed to create trustworthy AI in health care (e.g. reporting data quality, performing extensive (external) validation, and regulation). © 2020 The Authors","Review","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85099215997"
"Pintelas E.; Liaskos M.; Livieris I.E.; Kotsiantis S.; Pintelas P.","Pintelas, Emmanuel (57205740463); Liaskos, Meletis (57219288982); Livieris, Ioannis E. (35190672300); Kotsiantis, Sotiris (35584345800); Pintelas, Panagiotis (6701867219)","57205740463; 57219288982; 35190672300; 35584345800; 6701867219","A novel explainable image classification framework: case study on skin cancer and plant disease prediction","2021","Neural Computing and Applications","33","22","","15171","15189","18","18","10.1007/s00521-021-06141-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107463085&doi=10.1007%2fs00521-021-06141-0&partnerID=40&md5=2c79a2dfbdf9e0434c3433b1d4f6f209","An explainable/interpretable machine learning model is able to make reasoning about its predictions in understandable terms to humans. These properties are essential in order to trust model’s predictions, especially when these decisions affect critical aspects such as health, rights, security, and educational issues. Image classification is an area in machine learning and computer vision in which convolutional neural networks have flourished since they have shown remarkable performance in such problems. However, these models suffer in terms of transparency, interpretability and explainability, considered as black box models. This work proposes a novel explainable image classification framework applying it on skin cancer and plant diseases prediction problems. This framework combines segmentation and clustering techniques aiming to extract texture features from various subregions of the input image. Then, a feature filtering and cleaning procedure is applied on these extracted features in order to ensure that the proposed model will be also reliable and trustful, while these final extracted features are utilized for training an intrinsic linear white box prediction model. Finally, a hierarchy-based tree approach was created, in order to provide a meaningful interpretation of the model’s decision behavior. The experimental results have shown that the model’s explanations are clearly understandable, reliable, and trustful. Furthermore, regarding the prediction accuracy, the proposed model manages to achieve almost equal performance score (1–2% difference on average) comparing to the state-of-the-art black box convolutional image classification models. Such performance is considered noticeably good since the proposed classifier is an explainable intrinsic white box model. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.","Article","Final","","Scopus","2-s2.0-85107463085"
"Sallam I.E.; Abdelwareth A.; Attia H.; Aziz R.K.; Homsi M.N.; von Bergen M.; Farag M.A.","Sallam, Ibrahim E. (57214800803); Abdelwareth, Amr (57214796994); Attia, Heba (57206724117); Aziz, Ramy K. (7003399135); Homsi, Masun Nabhan (24528430800); von Bergen, Martin (6603254363); Farag, Mohamed A. (7006035865)","57214800803; 57214796994; 57206724117; 7003399135; 24528430800; 6603254363; 7006035865","Effect of gut microbiota biotransformation on dietary tannins and human health implications","2021","Microorganisms","9","5","965","","","","41","10.3390/microorganisms9050965","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107174408&doi=10.3390%2fmicroorganisms9050965&partnerID=40&md5=982c2e0087e8fa03d4bdbb237eca0720","Tannins represent a heterogeneous group of high‐molecular‐weight polyphenols that are ubiquitous among plant families, especially in cereals, as well as in many fruits and vegetables. Hydrolysable and condensed tannins, in addition to phlorotannins from marine algae, are the main classes of these bioactive compounds. Despite their low bioavailability, tannins have many beneficial pharmacological effects, such as anti‐inflammatory, antioxidant, antidiabetic, anticancer, and cardioprotective effects. Microbiota‐mediated hydrolysis of tannins produces highly bioaccessible metabolites, which have been extensively studied and account for most of the health effects attributed to tannins. This review article summarises the effect of the human microbiota on the metabolism of different tannin groups and the expected health benefits that may be induced by such mutual interactions. Microbial metabolism of tannins yields highly bioaccessible microbial metabolites that account for most of the systemic effects of tannins. This article also uses explainable artificial intelligence to define the molecular signatures of gut‐biotransformed tannin metabolites that are correlated with chemical and biological activity. An understanding of microbiota–tannin interactions, tannin metabolism‐related phenotypes (metabotypes) and chemical tannin‐metabolites motifs is of great importance for harnessing the biological effects of tannins for drug discovery and other health benefits. © 2021 by the author. Licensee MDPI, Basel, Switzerland.","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85107174408"
"Pawar U.; O'Shea D.; Rea S.; O'Reilly R.","Pawar, Urja (57202719602); O'Shea, Donna (56530263700); Rea, Susan (8329742500); O'Reilly, Ruairi (57204975215)","57202719602; 56530263700; 8329742500; 57204975215","Incorporating explainable artificial intelligence (XAI) to aid the understanding of machine learning in the healthcare domain","2020","CEUR Workshop Proceedings","2771","","","169","180","11","25","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099374575&partnerID=40&md5=ac79fb81eb29c9a44ec73d5896659dee","In the healthcare domain, Artificial Intelligence (AI) based systems are being increasingly adopted with applications ranging from surgical robots to automated medical diagnostics. While a Machine Learning (ML) engineer might be interested in the parameters related to the performance and accuracy of these AI-based systems, it is postulated that a medical practitioner would be more concerned with the applicability, and utility of these systems in the medical setting. However, medical practitioners are unlikely to have the prerequisite skills to enable reasonable interpretation of an AI-based system. This is a concern for two reasons. Firstly, it inhibits the adoption of systems capable of automating routine analysis work and prevents the associated productivity gains. Secondly, and perhaps more importantly, it reduces the scope of expertise available to assist in the validation, iteration, and improvement of AI-based systems in providing healthcare solutions. Explainable Artificial Intelligence (XAI) is a domain focused on techniques and approaches that facilitate the understanding and interpretation of the operation of ML models. Research interest in the domain of XAI is becoming more widespread due to the increasing adoption of AI-based solutions and the associated regulatory requirements [1]. Providing an understanding of ML models is typically approached from a Computer Science (CS) perspective [2] with a limited research emphasis being placed on supporting alternate domains [3]. In this paper, a simple, yet powerful solution for increasing the explainability of AI-based solutions to individuals from non-CS domains (such as medical practitioners), is presented. The proposed solution enables the explainability of ML models and the underlying workflows to be readily integrated into a standard ML workflow. Central to this solution are feature importance techniques that measure the impact of individual features on the outcomes of AI-based systems. It is envisaged that feature importance can enable a high-level understanding of a ML model and the workflow used to train the model. This could aid medical practitioners in comprehending AI-based systems and enhance their understanding of ML models' applicability and utility. © 2020 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).","Conference paper","Final","","Scopus","2-s2.0-85099374575"
"Weitz K.; Hassan T.; Schmid U.; Garbas J.-U.","Weitz, Katharina (57191854355); Hassan, Teena (57193091146); Schmid, Ute (56032455700); Garbas, Jens-Uwe (8367980700)","57191854355; 57193091146; 56032455700; 8367980700","Deep-learned faces of pain and emotions: Elucidating the differences of facial expressions with the help of explainable AI methods","2019","Technisches Messen","86","7-8","","404","412","8","36","10.1515/teme-2019-0024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067963538&doi=10.1515%2fteme-2019-0024&partnerID=40&md5=bdee852b50129190fff42b9c1ddb9629","Deep neural networks are successfully used for object and face recognition in images and videos. In order to be able to apply such networks in practice, for example in hospitals as a pain recognition tool, the current procedures are only suitable to a limited extent. The advantage of deep neural methods is that they can learn complex non-linear relationships between raw data and target classes without limiting themselves to a set of hand-crafted features provided by humans. However, the disadvantage is that due to the complexity of these networks, it is not possible to interpret the knowledge that is stored inside the network. It is a black-box learning procedure. Explainable Artificial Intelligence (AI) approaches mitigate this problem by extracting explanations for decisions and representing them in a human-interpretable form. The aim of this paper is to investigate the explainable AI methods Layer-wise Relevance Propagation (LRP) and Local Interpretable Model-agnostic Explanations (LIME). These approaches are applied to explain how a deep neural network distinguishes facial expressions of pain from facial expressions of emotions such as happiness and disgust. © 2019 Walter de Gruyter GmbH, Berlin/Boston 2019.","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85067963538"
"Ornek A.H.; Ceylan M.","Ornek, Ahmet H. (57210593918); Ceylan, Murat (56276648900)","57210593918; 56276648900","Explainable artificial intelligence (XAI): Classification of medical thermal images of neonates using class activation maps","2021","Traitement du Signal","38","5","","1271","1279","8","6","10.18280/ts.380502","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120499617&doi=10.18280%2fts.380502&partnerID=40&md5=a4cd0467fe05e143c2c0073c21148563","In order to determine the health status of the neonates, studies focus on either statistical behavior of the thermograms' temperature distributions, or just correct classifications of the thermograms. However, there exists always a lack of explain-ability for classification processes. Especially in the medical studies, doctors need explanations to assess the possible results of the decisions. Presenting our new study, how Convolutional Neural Networks (CNNs) decide the health status of neonates has been shown for the first time by using Class Activation Maps (CAMs). VGG16 which is one of the pre-trained models has been selected as a CNN model and the last layers of the VGG16 have been tuned according to CAMs. When the model was trained for 50 epochs, train-validation accuracies reached over 95% and test sensitivity-specificity were obtained as 80.701%-96.842% respectively. According to our findings, the CNN learns the temperature distribution of the body by mainly looking at the neck, armpit, and abdomen regions. The focused regions of the healthy babies are armpit and abdomen whereas of the unhealthy babies are neck and abdomen regions. Thus, we can say that the CNN focuses on dedicated regions to monitor the neonates and decides the health status of the neonates. © 2021 Lavoisier. All rights reserved.","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85120499617"
"Ming Y.; Xu P.; Cheng F.; Qu H.; Ren L.","Ming, Yao (57196148798); Xu, Panpan (36505244500); Cheng, Furui (57211997821); Qu, Huamin (7101947159); Ren, Liu (57193058523)","57196148798; 36505244500; 57211997821; 7101947159; 57193058523","ProtoSteer: Steering Deep Sequence Model with Prototypes","2020","IEEE Transactions on Visualization and Computer Graphics","26","1","8827944","238","248","10","39","10.1109/TVCG.2019.2934267","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075630282&doi=10.1109%2fTVCG.2019.2934267&partnerID=40&md5=7eab1cda9540bfbde4a816ebf424cd64","Recently we have witnessed growing adoption of deep sequence models (e.g. LSTMs) in many application domains, including predictive health care, natural language processing, and log analysis. However, the intricate working mechanism of these models confines their accessibility to the domain experts. Their black-box nature also makes it a challenging task to incorporate domain-specific knowledge of the experts into the model. In ProtoSteer (Prototype Steering), we tackle the challenge of directly involving the domain experts to steer a deep sequence model without relying on model developers as intermediaries. Our approach originates in case-based reasoning, which imitates the common human problem-solving process of consulting past experiences to solve new problems. We utilize ProSeNet (Prototype Sequence Network), which learns a small set of exemplar cases (i.e., prototypes) from historical data. In ProtoSteer they serve both as an efficient visual summary of the original data and explanations of model decisions. With ProtoSteer the domain experts can inspect, critique, and revise the prototypes interactively. The system then incorporates user-specified prototypes and incrementally updates the model. We conduct extensive case studies and expert interviews in application domains including sentiment analysis on texts and predictive diagnostics based on vehicle fault logs. The results demonstrate that involvements of domain users can help obtain more interpretable models with concise prototypes while retaining similar accuracy. © 1995-2012 IEEE.","Article","Final","","Scopus","2-s2.0-85075630282"
"Emmert-Streib F.; Yli-Harja O.; Dehmer M.","Emmert-Streib, Frank (15057742200); Yli-Harja, Olli (55880982100); Dehmer, Matthias (13404645900)","15057742200; 55880982100; 13404645900","Explainable artificial intelligence and machine learning: A reality rooted perspective","2020","Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery","10","6","e1368","","","","53","10.1002/widm.1368","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087183138&doi=10.1002%2fwidm.1368&partnerID=40&md5=0e45305a6f312a061c2b8e9ee033c652","As a consequence of technological progress, nowadays, one is used to the availability of big data generated in nearly all fields of science. However, the analysis of such data possesses vast challenges. One of these challenges relates to the explainability of methods from artificial intelligence (AI) or machine learning. Currently, many of such methods are nontransparent with respect to their working mechanism and for this reason are called black box models, most notably deep learning methods. However, it has been realized that this constitutes severe problems for a number of fields including the health sciences and criminal justice and arguments have been brought forward in favor of an explainable AI (XAI). In this paper, we do not assume the usual perspective presenting XAI as it should be, but rather provide a discussion what XAI can be. The difference is that we do not present wishful thinking but reality grounded properties in relation to a scientific theory beyond physics. This article is categorized under: Fundamental Concepts of Data and Knowledge > Explainable AI Algorithmic Development > Statistics Technologies > Machine Learning. © 2020 Wiley Periodicals LLC.","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85087183138"
"Kwon B.C.; Choi M.-J.; Kim J.T.; Choi E.; Kim Y.B.; Kwon S.; Sun J.; Choo J.","Kwon, Bum Chul (34881881400); Choi, Min-Je (57191033467); Kim, Joanne Taery (57203691124); Choi, Edward (57188811144); Kim, Young Bin (55653511500); Kwon, Soonwook (54585425600); Sun, Jimeng (9737233900); Choo, Jaegul (24512223400)","34881881400; 57191033467; 57203691124; 57188811144; 55653511500; 54585425600; 9737233900; 24512223400","RetainVis: Visual Analytics with Interpretable and Interactive Recurrent Neural Networks on Electronic Medical Records","2019","IEEE Transactions on Visualization and Computer Graphics","25","1","8440842","299","309","10","193","10.1109/TVCG.2018.2865027","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052616093&doi=10.1109%2fTVCG.2018.2865027&partnerID=40&md5=53aa10f57ad3d41f2147ef7d9845dbf6","We have recently seen many successful applications of recurrent neural networks (RNNs) on electronic medical records (EMRs), which contain histories of patients' diagnoses, medications, and other various events, in order to predict the current and future states of patients. Despite the strong performance of RNNs, it is often challenging for users to understand why the model makes a particular prediction. Such black-box nature of RNNs can impede its wide adoption in clinical practice. Furthermore, we have no established methods to interactively leverage users' domain expertise and prior knowledge as inputs for steering the model. Therefore, our design study aims to provide a visual analytics solution to increase interpretability and interactivity of RNNs via a joint effort of medical experts, artificial intelligence scientists, and visual analytics researchers. Following the iterative design process between the experts, we design, implement, and evaluate a visual analytics tool called RetainVis, which couples a newly improved, interpretable, and interactive RNN-based model called RetainEX and visualizations for users' exploration of EMR data in the context of prediction tasks. Our study shows the effective use of RetainVis for gaining insights into how individual medical codes contribute to making risk predictions, using EMRs of patients with heart failure and cataract symptoms. Our study also demonstrates how we made substantial changes to the state-of-the-art RNN model called RETAIN in order to make use of temporal information and increase interactivity. This study will provide a useful guideline for researchers that aim to design an interpretable and interactive visual analytics tool for RNNs. © 2018 IEEE.","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85052616093"
"Langlotz C.P.; Allen B.; Erickson B.J.; Kalpathy-Cramer J.; Bigelow K.; Cook T.S.; Flanders A.E.; Lungren M.P.; Mendelson D.S.; Rudie J.D.; Wang G.; Kandarpa K.","Langlotz, Curtis P. (20134955200); Allen, Bibb (15834227200); Erickson, Bradley J. (7201472755); Kalpathy-Cramer, Jayashree (6504761279); Bigelow, Keith (57518679900); Cook, Tessa S. (36144992200); Flanders, Adam E. (7006675016); Lungren, Matthew P. (36729660500); Mendelson, David S. (7005934757); Rudie, Jeffrey D. (36911792800); Wang, Ge (7407148134); Kandarpa, Krishna (7003309019)","20134955200; 15834227200; 7201472755; 6504761279; 57518679900; 36144992200; 7006675016; 36729660500; 7005934757; 36911792800; 7407148134; 7003309019","A roadmap for foundational research on artificial intelligence in medical imaging: From the 2018 NIH/RSNA/ACR/The Academy workshop","2019","Radiology","291","3","","781","791","10","237","10.1148/radiol.2019190613","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066513823&doi=10.1148%2fradiol.2019190613&partnerID=40&md5=85c2db8e3f654650ba7d12adcd1eddc9","Imaging research laboratories are rapidly creating machine learning systems that achieve expert human performance using open-source methods and tools. These artificial intelligence systems are being developed to improve medical image reconstruction, noise reduction, quality assurance, triage, segmentation, computer-aided detection, computer-aided classification, and radiogenomics. In August 2018, a meeting was held in Bethesda, Maryland, at the National Institutes of Health to discuss the current state of the art and knowledge gaps and to develop a roadmap for future research initiatives. Key research priorities include: 1, new image reconstruction methods that efficiently produce images suitable for human interpretation from source data; 2, automated image labeling and annotation methods, including information extraction from the imaging report, electronic phenotyping, and prospective structured image reporting; 3, new machine learning methods for clinical imaging data, such as tailored, pretrained model architectures, and federated machine learning methods; 4, machine learning methods that can explain the advice they provide to human users (so-called explainable artificial intelligence); and 5, validated methods for image de-identification and data sharing to facilitate wide availability of clinical imaging data sets. This research roadmap is intended to identify and prioritize these needs for academic research laboratories, funding agencies, professional societies, and industry. © RSNA, 2019","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85066513823"
"Senatore R.; Cioppa A.D.; Marcelli A.","Senatore, Rosa (36983155800); Cioppa, Antonio Della (57215948865); Marcelli, Angelo (7005267205)","36983155800; 57215948865; 7005267205","Automatic Diagnosis of Neurodegenerative Diseases: An evolutionary approach for facing the interpretability problem","2019","Information (Switzerland)","10","1","30","","","","21","10.3390/info10010030","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063767849&doi=10.3390%2finfo10010030&partnerID=40&md5=7cfd53375cf1df053f325aee39aba8be","Background: The use of Artificial Intelligence (AI) systems for automatic diagnoses is increasingly in the clinical field, being a useful support for the identification of several diseases. Nonetheless, the acceptance of AI-based diagnoses by the physicians is hampered by the black-box approach implemented by most performing systems, which do not clearly state the classification rules adopted. Methods: In this framework we propose a classification method based on a Cartesian Genetic Programming (CGP) approach, which allows for the automatic identification of the presence of the disease, and concurrently, provides the explicit classification model used by the system. Results: The proposed approach has been evaluated on the publicly available HandPD dataset, which contains handwriting samples drawn by Parkinson's disease patients and healthy controls. We show that our approach compares favorably with state-of-the-art methods, and more importantly, allows the physician to identify an explicit model relevant for the diagnosis based on the most informative subset of features. Conclusion: The obtained results suggest that the proposed approach is particularly appealing in that, starting from the explicit model, it allows the physicians to derive a set of guidelines for defining novel testing protocols and intervention strategies. © 2019 by the authors.","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85063767849"
"Ahmad M.A.; Teredesai A.; Eckert C.","Ahmad, Muhammad Aurangzeb (57670469700); Teredesai, Ankur (6602892213); Eckert, Carly (56803339200)","57670469700; 6602892213; 56803339200","Interpretable machine learning in healthcare","2018","Proceedings - 2018 IEEE International Conference on Healthcare Informatics, ICHI 2018","","","8419428","447","","","128","10.1109/ICHI.2018.00095","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051141812&doi=10.1109%2fICHI.2018.00095&partnerID=40&md5=53f85ac5b64c300a83388c1d947f2273","This tutorial extensively covers the definitions, nuances, challenges, and requirements for the design of interpretable and explainable machine learning models and systems in healthcare. We discuss many uses in which interpretable machine learning models are needed in healthcare and how they should be deployed. Additionally, we explore the landscape of recent advances to address the challenges model interpretability in healthcare and also describe how one would go about choosing the right interpretable machine learnig algorithm for a given problem in healthcare. © 2018 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85051141812"
